{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac359a2b",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-10-28T17:36:48.189075Z",
     "iopub.status.busy": "2024-10-28T17:36:48.188653Z",
     "iopub.status.idle": "2024-10-28T17:36:48.962069Z",
     "shell.execute_reply": "2024-10-28T17:36:48.961088Z"
    },
    "papermill": {
     "duration": 0.781201,
     "end_time": "2024-10-28T17:36:48.964257",
     "exception": false,
     "start_time": "2024-10-28T17:36:48.183056",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/gnngnn/trian.py\n",
      "/kaggle/input/gnngnn/layers.py\n",
      "/kaggle/input/gnngnn/README.md\n",
      "/kaggle/input/gnngnn/data_processing.py\n",
      "/kaggle/input/gnngnn/models.py\n",
      "/kaggle/input/gnngnn/__init__.py\n",
      "/kaggle/input/gnngnn/Networks/8anchor_1000agent_0PercentNLOS_smallLOS.mat\n",
      "/kaggle/input/gnngnn/Networks/8anchor_1000agent_10PercentNLOS_largeLOS.mat\n",
      "/kaggle/input/gnngnn/Networks/8anchor_1000agent_30PercentNLOS_largeLOS.mat\n",
      "/kaggle/input/gnngnn/Networks/8anchor_1000agent_50PercentNLOS_0.5LOS.mat\n",
      "/kaggle/input/gnngnn/Networks/readme\n",
      "/kaggle/input/gnngnn/Networks/8anchor_1000agent_10PercentNLOS_mediumLOS.mat\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2225237e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-28T17:36:48.973856Z",
     "iopub.status.busy": "2024-10-28T17:36:48.973429Z",
     "iopub.status.idle": "2024-10-28T17:37:04.249763Z",
     "shell.execute_reply": "2024-10-28T17:37:04.248834Z"
    },
    "papermill": {
     "duration": 15.28362,
     "end_time": "2024-10-28T17:37:04.252260",
     "exception": false,
     "start_time": "2024-10-28T17:36:48.968640",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'GNN-For-localization'...\r\n",
      "remote: Enumerating objects: 64, done.\u001b[K\r\n",
      "remote: Counting objects: 100% (64/64), done.\u001b[K\r\n",
      "remote: Compressing objects: 100% (62/62), done.\u001b[K\r\n",
      "remote: Total 64 (delta 27), reused 0 (delta 0), pack-reused 0 (from 0)\u001b[K\r\n",
      "Receiving objects: 100% (64/64), 74.35 MiB | 22.07 MiB/s, done.\r\n",
      "Resolving deltas: 100% (27/27), done.\r\n"
     ]
    }
   ],
   "source": [
    "# 克隆 GitHub 仓库\n",
    "!git clone https://github.com/dedaow/GNN-For-localization.git\n",
    "\n",
    "# 添加路径\n",
    "import sys\n",
    "sys.path.append('/kaggle/working/GNN-For-localization')  # 修改为您克隆的实际路径\n",
    "\n",
    "# 导入自定义模块\n",
    "from layers import *  # 确保这些模块存在于克隆的目录中\n",
    "from models import *\n",
    "from data_processing import *\n",
    "from models import *\n",
    "\n",
    "# 现在可以使用这些导入的模块\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95a7a233",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-28T17:37:04.265811Z",
     "iopub.status.busy": "2024-10-28T17:37:04.264871Z",
     "iopub.status.idle": "2024-10-28T17:37:04.270693Z",
     "shell.execute_reply": "2024-10-28T17:37:04.269663Z"
    },
    "papermill": {
     "duration": 0.014331,
     "end_time": "2024-10-28T17:37:04.272835",
     "exception": false,
     "start_time": "2024-10-28T17:37:04.258504",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__init__.py', 'README.md', '__pycache__', 'trian.py', 'layers.py', 'notebook4bdbff0441.ipynb', 'models.py', 'Networks', '.git', 'data_processing.py']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.listdir('/kaggle/working/GNN-For-localization'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a36b5de5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-28T17:37:04.284890Z",
     "iopub.status.busy": "2024-10-28T17:37:04.284554Z",
     "iopub.status.idle": "2024-10-28T17:37:04.289014Z",
     "shell.execute_reply": "2024-10-28T17:37:04.288072Z"
    },
    "papermill": {
     "duration": 0.012728,
     "end_time": "2024-10-28T17:37:04.291011",
     "exception": false,
     "start_time": "2024-10-28T17:37:04.278283",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/kaggle/working/GNN-For-localization')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9eb66993",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-28T17:37:04.302943Z",
     "iopub.status.busy": "2024-10-28T17:37:04.302585Z",
     "iopub.status.idle": "2024-10-28T17:37:04.312636Z",
     "shell.execute_reply": "2024-10-28T17:37:04.311866Z"
    },
    "papermill": {
     "duration": 0.018415,
     "end_time": "2024-10-28T17:37:04.314530",
     "exception": false,
     "start_time": "2024-10-28T17:37:04.296115",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from __future__ import division\n",
    "# from __future__ import print_function\n",
    "\n",
    "# import time\n",
    "# import numpy as np\n",
    "# from scipy import io\n",
    "# import os\n",
    "\n",
    "# import torch\n",
    "# import torch.nn.functional as F\n",
    "# import torch.optim as optim\n",
    "\n",
    "# from data_processing import load_data\n",
    "# from models import GCN\n",
    "# import datetime\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # 训练设置\n",
    "# # 移除 argparse 部分\n",
    "# args = {\n",
    "#     'no_cuda': False,\n",
    "#     'fastmode': False,\n",
    "#     'seed': 42,\n",
    "#     'epochs': 200,\n",
    "#     'lr': 0.01,\n",
    "#     'weight_decay': 0,\n",
    "#     'hidden': 2000,\n",
    "#     'dropout': 0.5\n",
    "# }\n",
    "\n",
    "# args['cuda'] = not args['no_cuda'] and torch.cuda.is_available()\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# threshold = 1\n",
    "# seed = 42\n",
    "# anchor = 50\n",
    "# repeat = 20  # 确保定义了 repeat 变量\n",
    "# loss_tem = np.zeros(repeat)\n",
    "\n",
    "# # 更新 load_data 函数，确保路径指向正确的文件\n",
    "# def load_data(threshold, num_anchor):\n",
    "#     # 更改为新的 .mat 文件路径\n",
    "#     mat_file_path = \"/kaggle/working/GNN-For-localization/Networks/8anchor_1000agent_10PercentNLOS_largeLOS.mat\"\n",
    "#     m = io.loadmat(mat_file_path)\n",
    "\n",
    "#     Range_Mat = m[\"Range_Mat\"]  # Range = Distance + noise\n",
    "#     Dist_Mat = m[\"Dist_Mat\"]\n",
    "    \n",
    "#     # 假设以下变量是从 .mat 文件中提取的\n",
    "#     mode_fea = 1  # 例如\n",
    "#     mode_adj = 1  # 例如\n",
    "#     num_anchor = num_anchor\n",
    "#     adj = np.zeros((num_anchor, num_anchor))  # 示例，需替换为实际数据\n",
    "#     features = np.random.rand(num_anchor, 10)  # 示例特征数据\n",
    "#     labels = np.random.rand(num_anchor, 1)  # 示例标签数据\n",
    "#     delta = None\n",
    "#     degree = None\n",
    "#     fea_original = None\n",
    "#     fea_true = None\n",
    "#     Range = None\n",
    "#     Dist = None\n",
    "#     truncated_noise = None\n",
    "#     idx_train = np.arange(num_anchor // 2)\n",
    "#     idx_val = np.arange(num_anchor // 2, num_anchor * 3 // 4)\n",
    "#     idx_test = np.arange(num_anchor * 3 // 4, num_anchor)\n",
    "\n",
    "#     return mode_fea, mode_adj, num_anchor, adj, features, labels, delta, degree, fea_original, fea_true, Range_Mat, Range, Dist_Mat, Dist, truncated_noise, idx_train, idx_val, idx_test\n",
    "\n",
    "# for axis in range(repeat):\n",
    "#     np.random.seed(seed)\n",
    "#     torch.manual_seed(seed)\n",
    "#     if args['cuda']:\n",
    "#         torch.cuda.manual_seed(seed)\n",
    "\n",
    "#     # 加载数据\n",
    "#     mode_fea, mode_adj, num_anchor, adj, features, labels, delta, degree, fea_original, fea_true, Range_Mat, Range, Dist_Mat, Dist, truncated_noise, idx_train, idx_val, idx_test = load_data(threshold, anchor)\n",
    "\n",
    "#     # 将 NumPy 数组转换为 PyTorch 张量并移动到 GPU（如果可用）\n",
    "#     features = torch.from_numpy(features).float().to(device)\n",
    "#     adj = torch.from_numpy(adj).float().to(device)\n",
    "#     labels = torch.from_numpy(labels).float().to(device)\n",
    "#     # 确保其他 NumPy 数组（如 delta, degree 等）也转换为张量\n",
    "#     if delta is not None:\n",
    "#         delta = torch.from_numpy(delta).float().to(device)\n",
    "#     if degree is not None:\n",
    "#         degree = torch.from_numpy(degree).float().to(device)\n",
    "\n",
    "#     # 将 idx_train、idx_val、idx_test 转换为张量\n",
    "#     idx_train = torch.from_numpy(idx_train).long().to(device)\n",
    "#     idx_val = torch.from_numpy(idx_val).long().to(device)\n",
    "#     idx_test = torch.from_numpy(idx_test).long().to(device)\n",
    "\n",
    "#     # 模型和优化器\n",
    "#     model = GCN(nfeat=features.shape[1],\n",
    "#                 nhid1=args['hidden'],\n",
    "#                 nhid2=2000,\n",
    "#                 #nout=labels.shape[1],\n",
    "#                 nout=1,\n",
    "#                 dropout=args['dropout'])\n",
    "\n",
    "#     optimizer = optim.Adam(model.parameters(),\n",
    "#                            lr=args['lr'], weight_decay=args['weight_decay'])\n",
    "\n",
    "#     print(model)\n",
    "\n",
    "#     loss_fun = torch.nn.MSELoss()\n",
    "\n",
    "#     if args['cuda']:\n",
    "#         model.cuda()\n",
    "\n",
    "#     def train(epoch):\n",
    "#         t = time.time()\n",
    "#         model.train()\n",
    "#         optimizer.zero_grad()\n",
    "#         output = model(features, adj)\n",
    "#         loss_train = loss_fun(output[idx_train], labels[idx_train])\n",
    "\n",
    "#         loss_train.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         if not args['fastmode']:\n",
    "#             model.eval()\n",
    "#             output = model(features, adj)\n",
    "\n",
    "#         loss_val = loss_fun(output[idx_val], labels[idx_val])\n",
    "#         loss_val = torch.sqrt(loss_val)\n",
    "#         print('Epoch: {:04d}'.format(epoch + 1),\n",
    "#               'loss_train (RMSE): {:.4f}'.format(loss_train.item()),\n",
    "#               'loss_val (RMSE): {:.4f}'.format(loss_val.item()))\n",
    "#         return loss_train\n",
    "\n",
    "#     def test():\n",
    "#         model.eval()\n",
    "#         output = model(features, adj)\n",
    "#         loss_test = loss_fun(output[idx_test], labels[idx_test])\n",
    "#         loss_test = torch.sqrt(loss_test)\n",
    "#         print(\"Test set results:\",\n",
    "#               \"loss= {:.4f} (RMSE)\".format(loss_test.item()))\n",
    "#         return output, loss_test\n",
    "\n",
    "#     # 训练模型\n",
    "#     t_total = time.time()\n",
    "#     for epoch in range(args['epochs']):\n",
    "#         loss_train = train(epoch)\n",
    "\n",
    "#     print(\"Optimization Finished!\")\n",
    "#     print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
    "\n",
    "#     # 测试\n",
    "#     predict, loss_test = test()\n",
    "#     loss_tem[axis] = loss_test.item()\n",
    "#     predict = predict.data.cpu().numpy()\n",
    "\n",
    "#     seed += 1\n",
    "\n",
    "# loss = sum(loss_tem) / repeat\n",
    "# print(\"=====================================\\n\")\n",
    "# print(\"Averaged Test results:\", \"loss= {:.4f} (RMSE)\".format(loss))\n",
    "\n",
    "# nowTime = datetime.datetime.now().strftime('%Y-%m-%d-%H_%M_%S')  # 获取当前时间\n",
    "# file_handle = open('result.txt', mode='a')\n",
    "# file_handle.write('=====================================\\n')\n",
    "# file_handle.write(nowTime + '\\n')\n",
    "# file_handle.write('loss_train (RMSE):' + format(loss_train.item()) + '\\n')\n",
    "# file_handle.write('loss_test (RMSE):' + format(loss) + '\\n')\n",
    "# file_handle.close()\n",
    "\n",
    "# labels = labels.data.cpu().numpy()\n",
    "\n",
    "# plt.figure(1)\n",
    "# plt.scatter(predict[:, 0], predict[:, 1], color='b')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40b3a264",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-28T17:37:04.327286Z",
     "iopub.status.busy": "2024-10-28T17:37:04.326797Z",
     "iopub.status.idle": "2024-10-28T17:37:04.330889Z",
     "shell.execute_reply": "2024-10-28T17:37:04.329834Z"
    },
    "papermill": {
     "duration": 0.013264,
     "end_time": "2024-10-28T17:37:04.333034",
     "exception": false,
     "start_time": "2024-10-28T17:37:04.319770",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # 检查预测结果的形状\n",
    "# print(f\"Predict shape: {predict.shape}\")\n",
    "# print(f\"Labels shape: {labels.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5f8c96",
   "metadata": {
    "papermill": {
     "duration": 0.005051,
     "end_time": "2024-10-28T17:37:04.343122",
     "exception": false,
     "start_time": "2024-10-28T17:37:04.338071",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ec7a72e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-28T17:37:04.354834Z",
     "iopub.status.busy": "2024-10-28T17:37:04.354540Z",
     "iopub.status.idle": "2024-10-28T17:37:17.002044Z",
     "shell.execute_reply": "2024-10-28T17:37:17.000999Z"
    },
    "papermill": {
     "duration": 12.656282,
     "end_time": "2024-10-28T17:37:17.004467",
     "exception": false,
     "start_time": "2024-10-28T17:37:04.348185",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCN(\n",
      "  (gc1): GraphConvolution (10 -> 2000)\n",
      "  (gc5): GraphConvolution (2000 -> 2)\n",
      ")\n",
      "Epoch: 0001 loss_train (RMSE): 0.4057 loss_val (RMSE): 0.5228\n",
      "Epoch: 0002 loss_train (RMSE): 0.3953 loss_val (RMSE): 0.5150\n",
      "Epoch: 0003 loss_train (RMSE): 0.3850 loss_val (RMSE): 0.5073\n",
      "Epoch: 0004 loss_train (RMSE): 0.3750 loss_val (RMSE): 0.4997\n",
      "Epoch: 0005 loss_train (RMSE): 0.3652 loss_val (RMSE): 0.4922\n",
      "Epoch: 0006 loss_train (RMSE): 0.3555 loss_val (RMSE): 0.4848\n",
      "Epoch: 0007 loss_train (RMSE): 0.3462 loss_val (RMSE): 0.4775\n",
      "Epoch: 0008 loss_train (RMSE): 0.3370 loss_val (RMSE): 0.4703\n",
      "Epoch: 0009 loss_train (RMSE): 0.3280 loss_val (RMSE): 0.4632\n",
      "Epoch: 0010 loss_train (RMSE): 0.3193 loss_val (RMSE): 0.4562\n",
      "Epoch: 0011 loss_train (RMSE): 0.3108 loss_val (RMSE): 0.4494\n",
      "Epoch: 0012 loss_train (RMSE): 0.3025 loss_val (RMSE): 0.4427\n",
      "Epoch: 0013 loss_train (RMSE): 0.2945 loss_val (RMSE): 0.4362\n",
      "Epoch: 0014 loss_train (RMSE): 0.2866 loss_val (RMSE): 0.4298\n",
      "Epoch: 0015 loss_train (RMSE): 0.2790 loss_val (RMSE): 0.4235\n",
      "Epoch: 0016 loss_train (RMSE): 0.2716 loss_val (RMSE): 0.4174\n",
      "Epoch: 0017 loss_train (RMSE): 0.2645 loss_val (RMSE): 0.4115\n",
      "Epoch: 0018 loss_train (RMSE): 0.2575 loss_val (RMSE): 0.4057\n",
      "Epoch: 0019 loss_train (RMSE): 0.2508 loss_val (RMSE): 0.4001\n",
      "Epoch: 0020 loss_train (RMSE): 0.2443 loss_val (RMSE): 0.3946\n",
      "Epoch: 0021 loss_train (RMSE): 0.2380 loss_val (RMSE): 0.3893\n",
      "Epoch: 0022 loss_train (RMSE): 0.2320 loss_val (RMSE): 0.3841\n",
      "Epoch: 0023 loss_train (RMSE): 0.2261 loss_val (RMSE): 0.3792\n",
      "Epoch: 0024 loss_train (RMSE): 0.2205 loss_val (RMSE): 0.3743\n",
      "Epoch: 0025 loss_train (RMSE): 0.2150 loss_val (RMSE): 0.3697\n",
      "Epoch: 0026 loss_train (RMSE): 0.2097 loss_val (RMSE): 0.3652\n",
      "Epoch: 0027 loss_train (RMSE): 0.2046 loss_val (RMSE): 0.3608\n",
      "Epoch: 0028 loss_train (RMSE): 0.1998 loss_val (RMSE): 0.3566\n",
      "Epoch: 0029 loss_train (RMSE): 0.1950 loss_val (RMSE): 0.3526\n",
      "Epoch: 0030 loss_train (RMSE): 0.1905 loss_val (RMSE): 0.3487\n",
      "Epoch: 0031 loss_train (RMSE): 0.1861 loss_val (RMSE): 0.3450\n",
      "Epoch: 0032 loss_train (RMSE): 0.1819 loss_val (RMSE): 0.3413\n",
      "Epoch: 0033 loss_train (RMSE): 0.1779 loss_val (RMSE): 0.3379\n",
      "Epoch: 0034 loss_train (RMSE): 0.1740 loss_val (RMSE): 0.3345\n",
      "Epoch: 0035 loss_train (RMSE): 0.1703 loss_val (RMSE): 0.3313\n",
      "Epoch: 0036 loss_train (RMSE): 0.1667 loss_val (RMSE): 0.3282\n",
      "Epoch: 0037 loss_train (RMSE): 0.1632 loss_val (RMSE): 0.3252\n",
      "Epoch: 0038 loss_train (RMSE): 0.1599 loss_val (RMSE): 0.3223\n",
      "Epoch: 0039 loss_train (RMSE): 0.1567 loss_val (RMSE): 0.3195\n",
      "Epoch: 0040 loss_train (RMSE): 0.1536 loss_val (RMSE): 0.3168\n",
      "Epoch: 0041 loss_train (RMSE): 0.1506 loss_val (RMSE): 0.3142\n",
      "Epoch: 0042 loss_train (RMSE): 0.1477 loss_val (RMSE): 0.3117\n",
      "Epoch: 0043 loss_train (RMSE): 0.1450 loss_val (RMSE): 0.3093\n",
      "Epoch: 0044 loss_train (RMSE): 0.1423 loss_val (RMSE): 0.3069\n",
      "Epoch: 0045 loss_train (RMSE): 0.1398 loss_val (RMSE): 0.3046\n",
      "Epoch: 0046 loss_train (RMSE): 0.1373 loss_val (RMSE): 0.3025\n",
      "Epoch: 0047 loss_train (RMSE): 0.1350 loss_val (RMSE): 0.3003\n",
      "Epoch: 0048 loss_train (RMSE): 0.1327 loss_val (RMSE): 0.2983\n",
      "Epoch: 0049 loss_train (RMSE): 0.1305 loss_val (RMSE): 0.2963\n",
      "Epoch: 0050 loss_train (RMSE): 0.1284 loss_val (RMSE): 0.2944\n",
      "Epoch: 0051 loss_train (RMSE): 0.1263 loss_val (RMSE): 0.2925\n",
      "Epoch: 0052 loss_train (RMSE): 0.1244 loss_val (RMSE): 0.2907\n",
      "Epoch: 0053 loss_train (RMSE): 0.1225 loss_val (RMSE): 0.2890\n",
      "Epoch: 0054 loss_train (RMSE): 0.1207 loss_val (RMSE): 0.2873\n",
      "Epoch: 0055 loss_train (RMSE): 0.1189 loss_val (RMSE): 0.2857\n",
      "Epoch: 0056 loss_train (RMSE): 0.1173 loss_val (RMSE): 0.2842\n",
      "Epoch: 0057 loss_train (RMSE): 0.1157 loss_val (RMSE): 0.2827\n",
      "Epoch: 0058 loss_train (RMSE): 0.1141 loss_val (RMSE): 0.2813\n",
      "Epoch: 0059 loss_train (RMSE): 0.1126 loss_val (RMSE): 0.2799\n",
      "Epoch: 0060 loss_train (RMSE): 0.1112 loss_val (RMSE): 0.2786\n",
      "Epoch: 0061 loss_train (RMSE): 0.1098 loss_val (RMSE): 0.2773\n",
      "Epoch: 0062 loss_train (RMSE): 0.1085 loss_val (RMSE): 0.2761\n",
      "Epoch: 0063 loss_train (RMSE): 0.1073 loss_val (RMSE): 0.2750\n",
      "Epoch: 0064 loss_train (RMSE): 0.1061 loss_val (RMSE): 0.2739\n",
      "Epoch: 0065 loss_train (RMSE): 0.1050 loss_val (RMSE): 0.2729\n",
      "Epoch: 0066 loss_train (RMSE): 0.1039 loss_val (RMSE): 0.2719\n",
      "Epoch: 0067 loss_train (RMSE): 0.1028 loss_val (RMSE): 0.2710\n",
      "Epoch: 0068 loss_train (RMSE): 0.1018 loss_val (RMSE): 0.2702\n",
      "Epoch: 0069 loss_train (RMSE): 0.1009 loss_val (RMSE): 0.2694\n",
      "Epoch: 0070 loss_train (RMSE): 0.1000 loss_val (RMSE): 0.2686\n",
      "Epoch: 0071 loss_train (RMSE): 0.0991 loss_val (RMSE): 0.2680\n",
      "Epoch: 0072 loss_train (RMSE): 0.0983 loss_val (RMSE): 0.2673\n",
      "Epoch: 0073 loss_train (RMSE): 0.0975 loss_val (RMSE): 0.2667\n",
      "Epoch: 0074 loss_train (RMSE): 0.0967 loss_val (RMSE): 0.2662\n",
      "Epoch: 0075 loss_train (RMSE): 0.0960 loss_val (RMSE): 0.2657\n",
      "Epoch: 0076 loss_train (RMSE): 0.0954 loss_val (RMSE): 0.2653\n",
      "Epoch: 0077 loss_train (RMSE): 0.0947 loss_val (RMSE): 0.2649\n",
      "Epoch: 0078 loss_train (RMSE): 0.0941 loss_val (RMSE): 0.2646\n",
      "Epoch: 0079 loss_train (RMSE): 0.0936 loss_val (RMSE): 0.2643\n",
      "Epoch: 0080 loss_train (RMSE): 0.0930 loss_val (RMSE): 0.2640\n",
      "Epoch: 0081 loss_train (RMSE): 0.0925 loss_val (RMSE): 0.2638\n",
      "Epoch: 0082 loss_train (RMSE): 0.0920 loss_val (RMSE): 0.2636\n",
      "Epoch: 0083 loss_train (RMSE): 0.0916 loss_val (RMSE): 0.2635\n",
      "Epoch: 0084 loss_train (RMSE): 0.0912 loss_val (RMSE): 0.2634\n",
      "Epoch: 0085 loss_train (RMSE): 0.0907 loss_val (RMSE): 0.2633\n",
      "Epoch: 0086 loss_train (RMSE): 0.0904 loss_val (RMSE): 0.2632\n",
      "Epoch: 0087 loss_train (RMSE): 0.0900 loss_val (RMSE): 0.2632\n",
      "Epoch: 0088 loss_train (RMSE): 0.0897 loss_val (RMSE): 0.2632\n",
      "Epoch: 0089 loss_train (RMSE): 0.0893 loss_val (RMSE): 0.2633\n",
      "Epoch: 0090 loss_train (RMSE): 0.0890 loss_val (RMSE): 0.2633\n",
      "Epoch: 0091 loss_train (RMSE): 0.0888 loss_val (RMSE): 0.2634\n",
      "Epoch: 0092 loss_train (RMSE): 0.0885 loss_val (RMSE): 0.2635\n",
      "Epoch: 0093 loss_train (RMSE): 0.0882 loss_val (RMSE): 0.2636\n",
      "Epoch: 0094 loss_train (RMSE): 0.0880 loss_val (RMSE): 0.2638\n",
      "Epoch: 0095 loss_train (RMSE): 0.0878 loss_val (RMSE): 0.2639\n",
      "Epoch: 0096 loss_train (RMSE): 0.0876 loss_val (RMSE): 0.2641\n",
      "Epoch: 0097 loss_train (RMSE): 0.0874 loss_val (RMSE): 0.2643\n",
      "Epoch: 0098 loss_train (RMSE): 0.0872 loss_val (RMSE): 0.2645\n",
      "Epoch: 0099 loss_train (RMSE): 0.0870 loss_val (RMSE): 0.2647\n",
      "Epoch: 0100 loss_train (RMSE): 0.0869 loss_val (RMSE): 0.2649\n",
      "Epoch: 0101 loss_train (RMSE): 0.0867 loss_val (RMSE): 0.2651\n",
      "Epoch: 0102 loss_train (RMSE): 0.0866 loss_val (RMSE): 0.2654\n",
      "Epoch: 0103 loss_train (RMSE): 0.0865 loss_val (RMSE): 0.2656\n",
      "Epoch: 0104 loss_train (RMSE): 0.0863 loss_val (RMSE): 0.2658\n",
      "Epoch: 0105 loss_train (RMSE): 0.0862 loss_val (RMSE): 0.2661\n",
      "Epoch: 0106 loss_train (RMSE): 0.0861 loss_val (RMSE): 0.2663\n",
      "Epoch: 0107 loss_train (RMSE): 0.0860 loss_val (RMSE): 0.2666\n",
      "Epoch: 0108 loss_train (RMSE): 0.0859 loss_val (RMSE): 0.2668\n",
      "Epoch: 0109 loss_train (RMSE): 0.0859 loss_val (RMSE): 0.2670\n",
      "Epoch: 0110 loss_train (RMSE): 0.0858 loss_val (RMSE): 0.2673\n",
      "Epoch: 0111 loss_train (RMSE): 0.0857 loss_val (RMSE): 0.2675\n",
      "Epoch: 0112 loss_train (RMSE): 0.0856 loss_val (RMSE): 0.2678\n",
      "Epoch: 0113 loss_train (RMSE): 0.0856 loss_val (RMSE): 0.2680\n",
      "Epoch: 0114 loss_train (RMSE): 0.0855 loss_val (RMSE): 0.2682\n",
      "Epoch: 0115 loss_train (RMSE): 0.0855 loss_val (RMSE): 0.2685\n",
      "Epoch: 0116 loss_train (RMSE): 0.0854 loss_val (RMSE): 0.2687\n",
      "Epoch: 0117 loss_train (RMSE): 0.0854 loss_val (RMSE): 0.2689\n",
      "Epoch: 0118 loss_train (RMSE): 0.0853 loss_val (RMSE): 0.2691\n",
      "Epoch: 0119 loss_train (RMSE): 0.0853 loss_val (RMSE): 0.2693\n",
      "Epoch: 0120 loss_train (RMSE): 0.0853 loss_val (RMSE): 0.2695\n",
      "Epoch: 0121 loss_train (RMSE): 0.0852 loss_val (RMSE): 0.2697\n",
      "Epoch: 0122 loss_train (RMSE): 0.0852 loss_val (RMSE): 0.2699\n",
      "Epoch: 0123 loss_train (RMSE): 0.0852 loss_val (RMSE): 0.2701\n",
      "Epoch: 0124 loss_train (RMSE): 0.0852 loss_val (RMSE): 0.2703\n",
      "Epoch: 0125 loss_train (RMSE): 0.0851 loss_val (RMSE): 0.2705\n",
      "Epoch: 0126 loss_train (RMSE): 0.0851 loss_val (RMSE): 0.2707\n",
      "Epoch: 0127 loss_train (RMSE): 0.0851 loss_val (RMSE): 0.2708\n",
      "Epoch: 0128 loss_train (RMSE): 0.0851 loss_val (RMSE): 0.2710\n",
      "Epoch: 0129 loss_train (RMSE): 0.0851 loss_val (RMSE): 0.2711\n",
      "Epoch: 0130 loss_train (RMSE): 0.0850 loss_val (RMSE): 0.2713\n",
      "Epoch: 0131 loss_train (RMSE): 0.0850 loss_val (RMSE): 0.2714\n",
      "Epoch: 0132 loss_train (RMSE): 0.0850 loss_val (RMSE): 0.2716\n",
      "Epoch: 0133 loss_train (RMSE): 0.0850 loss_val (RMSE): 0.2717\n",
      "Epoch: 0134 loss_train (RMSE): 0.0850 loss_val (RMSE): 0.2718\n",
      "Epoch: 0135 loss_train (RMSE): 0.0850 loss_val (RMSE): 0.2719\n",
      "Epoch: 0136 loss_train (RMSE): 0.0850 loss_val (RMSE): 0.2720\n",
      "Epoch: 0137 loss_train (RMSE): 0.0850 loss_val (RMSE): 0.2722\n",
      "Epoch: 0138 loss_train (RMSE): 0.0850 loss_val (RMSE): 0.2723\n",
      "Epoch: 0139 loss_train (RMSE): 0.0850 loss_val (RMSE): 0.2724\n",
      "Epoch: 0140 loss_train (RMSE): 0.0850 loss_val (RMSE): 0.2725\n",
      "Epoch: 0141 loss_train (RMSE): 0.0850 loss_val (RMSE): 0.2725\n",
      "Epoch: 0142 loss_train (RMSE): 0.0850 loss_val (RMSE): 0.2726\n",
      "Epoch: 0143 loss_train (RMSE): 0.0849 loss_val (RMSE): 0.2727\n",
      "Epoch: 0144 loss_train (RMSE): 0.0849 loss_val (RMSE): 0.2728\n",
      "Epoch: 0145 loss_train (RMSE): 0.0849 loss_val (RMSE): 0.2729\n",
      "Epoch: 0146 loss_train (RMSE): 0.0849 loss_val (RMSE): 0.2729\n",
      "Epoch: 0147 loss_train (RMSE): 0.0849 loss_val (RMSE): 0.2730\n",
      "Epoch: 0148 loss_train (RMSE): 0.0849 loss_val (RMSE): 0.2731\n",
      "Epoch: 0149 loss_train (RMSE): 0.0849 loss_val (RMSE): 0.2731\n",
      "Epoch: 0150 loss_train (RMSE): 0.0849 loss_val (RMSE): 0.2732\n",
      "Epoch: 0151 loss_train (RMSE): 0.0849 loss_val (RMSE): 0.2732\n",
      "Epoch: 0152 loss_train (RMSE): 0.0849 loss_val (RMSE): 0.2733\n",
      "Epoch: 0153 loss_train (RMSE): 0.0849 loss_val (RMSE): 0.2733\n",
      "Epoch: 0154 loss_train (RMSE): 0.0849 loss_val (RMSE): 0.2734\n",
      "Epoch: 0155 loss_train (RMSE): 0.0849 loss_val (RMSE): 0.2734\n",
      "Epoch: 0156 loss_train (RMSE): 0.0849 loss_val (RMSE): 0.2735\n",
      "Epoch: 0157 loss_train (RMSE): 0.0849 loss_val (RMSE): 0.2735\n",
      "Epoch: 0158 loss_train (RMSE): 0.0849 loss_val (RMSE): 0.2736\n",
      "Epoch: 0159 loss_train (RMSE): 0.0849 loss_val (RMSE): 0.2736\n",
      "Epoch: 0160 loss_train (RMSE): 0.0849 loss_val (RMSE): 0.2736\n",
      "Epoch: 0161 loss_train (RMSE): 0.0849 loss_val (RMSE): 0.2737\n",
      "Epoch: 0162 loss_train (RMSE): 0.0849 loss_val (RMSE): 0.2737\n",
      "Epoch: 0163 loss_train (RMSE): 0.0849 loss_val (RMSE): 0.2737\n",
      "Epoch: 0164 loss_train (RMSE): 0.0849 loss_val (RMSE): 0.2737\n",
      "Epoch: 0165 loss_train (RMSE): 0.0849 loss_val (RMSE): 0.2738\n",
      "Epoch: 0166 loss_train (RMSE): 0.0849 loss_val (RMSE): 0.2738\n",
      "Epoch: 0167 loss_train (RMSE): 0.0849 loss_val (RMSE): 0.2738\n",
      "Epoch: 0168 loss_train (RMSE): 0.0849 loss_val (RMSE): 0.2738\n",
      "Epoch: 0169 loss_train (RMSE): 0.0849 loss_val (RMSE): 0.2739\n",
      "Epoch: 0170 loss_train (RMSE): 0.0849 loss_val (RMSE): 0.2739\n",
      "Epoch: 0171 loss_train (RMSE): 0.0849 loss_val (RMSE): 0.2739\n",
      "Epoch: 0172 loss_train (RMSE): 0.0849 loss_val (RMSE): 0.2739\n",
      "Epoch: 0173 loss_train (RMSE): 0.0849 loss_val (RMSE): 0.2739\n",
      "Epoch: 0174 loss_train (RMSE): 0.0849 loss_val (RMSE): 0.2739\n",
      "Epoch: 0175 loss_train (RMSE): 0.0849 loss_val (RMSE): 0.2740\n",
      "Epoch: 0176 loss_train (RMSE): 0.0849 loss_val (RMSE): 0.2740\n",
      "Epoch: 0177 loss_train (RMSE): 0.0849 loss_val (RMSE): 0.2740\n",
      "Epoch: 0178 loss_train (RMSE): 0.0849 loss_val (RMSE): 0.2740\n",
      "Epoch: 0179 loss_train (RMSE): 0.0849 loss_val (RMSE): 0.2740\n",
      "Epoch: 0180 loss_train (RMSE): 0.0849 loss_val (RMSE): 0.2740\n",
      "Epoch: 0181 loss_train (RMSE): 0.0849 loss_val (RMSE): 0.2740\n",
      "Epoch: 0182 loss_train (RMSE): 0.0849 loss_val (RMSE): 0.2740\n",
      "Epoch: 0183 loss_train (RMSE): 0.0849 loss_val (RMSE): 0.2740\n",
      "Epoch: 0184 loss_train (RMSE): 0.0849 loss_val (RMSE): 0.2740\n",
      "Epoch: 0185 loss_train (RMSE): 0.0849 loss_val (RMSE): 0.2740\n",
      "Epoch: 0186 loss_train (RMSE): 0.0849 loss_val (RMSE): 0.2740\n",
      "Epoch: 0187 loss_train (RMSE): 0.0849 loss_val (RMSE): 0.2740\n",
      "Epoch: 0188 loss_train (RMSE): 0.0849 loss_val (RMSE): 0.2741\n",
      "Epoch: 0189 loss_train (RMSE): 0.0849 loss_val (RMSE): 0.2741\n",
      "Epoch: 0190 loss_train (RMSE): 0.0849 loss_val (RMSE): 0.2741\n",
      "Epoch: 0191 loss_train (RMSE): 0.0849 loss_val (RMSE): 0.2741\n",
      "Epoch: 0192 loss_train (RMSE): 0.0849 loss_val (RMSE): 0.2741\n",
      "Epoch: 0193 loss_train (RMSE): 0.0849 loss_val (RMSE): 0.2741\n",
      "Epoch: 0194 loss_train (RMSE): 0.0849 loss_val (RMSE): 0.2741\n",
      "Epoch: 0195 loss_train (RMSE): 0.0849 loss_val (RMSE): 0.2741\n",
      "Epoch: 0196 loss_train (RMSE): 0.0849 loss_val (RMSE): 0.2741\n",
      "Epoch: 0197 loss_train (RMSE): 0.0849 loss_val (RMSE): 0.2741\n",
      "Epoch: 0198 loss_train (RMSE): 0.0849 loss_val (RMSE): 0.2741\n",
      "Epoch: 0199 loss_train (RMSE): 0.0849 loss_val (RMSE): 0.2741\n",
      "Epoch: 0200 loss_train (RMSE): 0.0849 loss_val (RMSE): 0.2741\n",
      "Optimization Finished!\n",
      "Total time elapsed: 1.0682s\n",
      "Test set results: loss= 0.3489 (RMSE)\n",
      "GCN(\n",
      "  (gc1): GraphConvolution (10 -> 2000)\n",
      "  (gc5): GraphConvolution (2000 -> 2)\n",
      ")\n",
      "Epoch: 0001 loss_train (RMSE): 0.5020 loss_val (RMSE): 0.8119\n",
      "Epoch: 0002 loss_train (RMSE): 0.4925 loss_val (RMSE): 0.8043\n",
      "Epoch: 0003 loss_train (RMSE): 0.4832 loss_val (RMSE): 0.7968\n",
      "Epoch: 0004 loss_train (RMSE): 0.4740 loss_val (RMSE): 0.7895\n",
      "Epoch: 0005 loss_train (RMSE): 0.4651 loss_val (RMSE): 0.7823\n",
      "Epoch: 0006 loss_train (RMSE): 0.4564 loss_val (RMSE): 0.7753\n",
      "Epoch: 0007 loss_train (RMSE): 0.4477 loss_val (RMSE): 0.7685\n",
      "Epoch: 0008 loss_train (RMSE): 0.4392 loss_val (RMSE): 0.7619\n",
      "Epoch: 0009 loss_train (RMSE): 0.4307 loss_val (RMSE): 0.7554\n",
      "Epoch: 0010 loss_train (RMSE): 0.4223 loss_val (RMSE): 0.7490\n",
      "Epoch: 0011 loss_train (RMSE): 0.4141 loss_val (RMSE): 0.7427\n",
      "Epoch: 0012 loss_train (RMSE): 0.4059 loss_val (RMSE): 0.7365\n",
      "Epoch: 0013 loss_train (RMSE): 0.3979 loss_val (RMSE): 0.7303\n",
      "Epoch: 0014 loss_train (RMSE): 0.3900 loss_val (RMSE): 0.7242\n",
      "Epoch: 0015 loss_train (RMSE): 0.3822 loss_val (RMSE): 0.7181\n",
      "Epoch: 0016 loss_train (RMSE): 0.3745 loss_val (RMSE): 0.7119\n",
      "Epoch: 0017 loss_train (RMSE): 0.3670 loss_val (RMSE): 0.7057\n",
      "Epoch: 0018 loss_train (RMSE): 0.3596 loss_val (RMSE): 0.6995\n",
      "Epoch: 0019 loss_train (RMSE): 0.3523 loss_val (RMSE): 0.6932\n",
      "Epoch: 0020 loss_train (RMSE): 0.3452 loss_val (RMSE): 0.6869\n",
      "Epoch: 0021 loss_train (RMSE): 0.3381 loss_val (RMSE): 0.6806\n",
      "Epoch: 0022 loss_train (RMSE): 0.3312 loss_val (RMSE): 0.6742\n",
      "Epoch: 0023 loss_train (RMSE): 0.3243 loss_val (RMSE): 0.6678\n",
      "Epoch: 0024 loss_train (RMSE): 0.3176 loss_val (RMSE): 0.6615\n",
      "Epoch: 0025 loss_train (RMSE): 0.3111 loss_val (RMSE): 0.6552\n",
      "Epoch: 0026 loss_train (RMSE): 0.3046 loss_val (RMSE): 0.6490\n",
      "Epoch: 0027 loss_train (RMSE): 0.2983 loss_val (RMSE): 0.6429\n",
      "Epoch: 0028 loss_train (RMSE): 0.2921 loss_val (RMSE): 0.6369\n",
      "Epoch: 0029 loss_train (RMSE): 0.2860 loss_val (RMSE): 0.6309\n",
      "Epoch: 0030 loss_train (RMSE): 0.2801 loss_val (RMSE): 0.6251\n",
      "Epoch: 0031 loss_train (RMSE): 0.2742 loss_val (RMSE): 0.6194\n",
      "Epoch: 0032 loss_train (RMSE): 0.2685 loss_val (RMSE): 0.6137\n",
      "Epoch: 0033 loss_train (RMSE): 0.2629 loss_val (RMSE): 0.6082\n",
      "Epoch: 0034 loss_train (RMSE): 0.2574 loss_val (RMSE): 0.6027\n",
      "Epoch: 0035 loss_train (RMSE): 0.2520 loss_val (RMSE): 0.5972\n",
      "Epoch: 0036 loss_train (RMSE): 0.2468 loss_val (RMSE): 0.5918\n",
      "Epoch: 0037 loss_train (RMSE): 0.2416 loss_val (RMSE): 0.5864\n",
      "Epoch: 0038 loss_train (RMSE): 0.2366 loss_val (RMSE): 0.5811\n",
      "Epoch: 0039 loss_train (RMSE): 0.2317 loss_val (RMSE): 0.5757\n",
      "Epoch: 0040 loss_train (RMSE): 0.2269 loss_val (RMSE): 0.5704\n",
      "Epoch: 0041 loss_train (RMSE): 0.2221 loss_val (RMSE): 0.5651\n",
      "Epoch: 0042 loss_train (RMSE): 0.2176 loss_val (RMSE): 0.5598\n",
      "Epoch: 0043 loss_train (RMSE): 0.2131 loss_val (RMSE): 0.5545\n",
      "Epoch: 0044 loss_train (RMSE): 0.2087 loss_val (RMSE): 0.5493\n",
      "Epoch: 0045 loss_train (RMSE): 0.2044 loss_val (RMSE): 0.5442\n",
      "Epoch: 0046 loss_train (RMSE): 0.2002 loss_val (RMSE): 0.5391\n",
      "Epoch: 0047 loss_train (RMSE): 0.1962 loss_val (RMSE): 0.5341\n",
      "Epoch: 0048 loss_train (RMSE): 0.1922 loss_val (RMSE): 0.5292\n",
      "Epoch: 0049 loss_train (RMSE): 0.1883 loss_val (RMSE): 0.5244\n",
      "Epoch: 0050 loss_train (RMSE): 0.1845 loss_val (RMSE): 0.5196\n",
      "Epoch: 0051 loss_train (RMSE): 0.1809 loss_val (RMSE): 0.5150\n",
      "Epoch: 0052 loss_train (RMSE): 0.1773 loss_val (RMSE): 0.5104\n",
      "Epoch: 0053 loss_train (RMSE): 0.1738 loss_val (RMSE): 0.5059\n",
      "Epoch: 0054 loss_train (RMSE): 0.1704 loss_val (RMSE): 0.5015\n",
      "Epoch: 0055 loss_train (RMSE): 0.1671 loss_val (RMSE): 0.4971\n",
      "Epoch: 0056 loss_train (RMSE): 0.1638 loss_val (RMSE): 0.4927\n",
      "Epoch: 0057 loss_train (RMSE): 0.1607 loss_val (RMSE): 0.4884\n",
      "Epoch: 0058 loss_train (RMSE): 0.1577 loss_val (RMSE): 0.4842\n",
      "Epoch: 0059 loss_train (RMSE): 0.1547 loss_val (RMSE): 0.4799\n",
      "Epoch: 0060 loss_train (RMSE): 0.1518 loss_val (RMSE): 0.4758\n",
      "Epoch: 0061 loss_train (RMSE): 0.1490 loss_val (RMSE): 0.4716\n",
      "Epoch: 0062 loss_train (RMSE): 0.1463 loss_val (RMSE): 0.4675\n",
      "Epoch: 0063 loss_train (RMSE): 0.1436 loss_val (RMSE): 0.4635\n",
      "Epoch: 0064 loss_train (RMSE): 0.1411 loss_val (RMSE): 0.4596\n",
      "Epoch: 0065 loss_train (RMSE): 0.1386 loss_val (RMSE): 0.4557\n",
      "Epoch: 0066 loss_train (RMSE): 0.1361 loss_val (RMSE): 0.4519\n",
      "Epoch: 0067 loss_train (RMSE): 0.1338 loss_val (RMSE): 0.4481\n",
      "Epoch: 0068 loss_train (RMSE): 0.1315 loss_val (RMSE): 0.4445\n",
      "Epoch: 0069 loss_train (RMSE): 0.1293 loss_val (RMSE): 0.4409\n",
      "Epoch: 0070 loss_train (RMSE): 0.1272 loss_val (RMSE): 0.4374\n",
      "Epoch: 0071 loss_train (RMSE): 0.1251 loss_val (RMSE): 0.4339\n",
      "Epoch: 0072 loss_train (RMSE): 0.1231 loss_val (RMSE): 0.4306\n",
      "Epoch: 0073 loss_train (RMSE): 0.1211 loss_val (RMSE): 0.4272\n",
      "Epoch: 0074 loss_train (RMSE): 0.1192 loss_val (RMSE): 0.4239\n",
      "Epoch: 0075 loss_train (RMSE): 0.1174 loss_val (RMSE): 0.4207\n",
      "Epoch: 0076 loss_train (RMSE): 0.1156 loss_val (RMSE): 0.4175\n",
      "Epoch: 0077 loss_train (RMSE): 0.1139 loss_val (RMSE): 0.4144\n",
      "Epoch: 0078 loss_train (RMSE): 0.1122 loss_val (RMSE): 0.4113\n",
      "Epoch: 0079 loss_train (RMSE): 0.1106 loss_val (RMSE): 0.4082\n",
      "Epoch: 0080 loss_train (RMSE): 0.1091 loss_val (RMSE): 0.4052\n",
      "Epoch: 0081 loss_train (RMSE): 0.1076 loss_val (RMSE): 0.4023\n",
      "Epoch: 0082 loss_train (RMSE): 0.1061 loss_val (RMSE): 0.3994\n",
      "Epoch: 0083 loss_train (RMSE): 0.1047 loss_val (RMSE): 0.3966\n",
      "Epoch: 0084 loss_train (RMSE): 0.1034 loss_val (RMSE): 0.3939\n",
      "Epoch: 0085 loss_train (RMSE): 0.1021 loss_val (RMSE): 0.3912\n",
      "Epoch: 0086 loss_train (RMSE): 0.1008 loss_val (RMSE): 0.3886\n",
      "Epoch: 0087 loss_train (RMSE): 0.0996 loss_val (RMSE): 0.3860\n",
      "Epoch: 0088 loss_train (RMSE): 0.0984 loss_val (RMSE): 0.3835\n",
      "Epoch: 0089 loss_train (RMSE): 0.0973 loss_val (RMSE): 0.3811\n",
      "Epoch: 0090 loss_train (RMSE): 0.0962 loss_val (RMSE): 0.3787\n",
      "Epoch: 0091 loss_train (RMSE): 0.0952 loss_val (RMSE): 0.3763\n",
      "Epoch: 0092 loss_train (RMSE): 0.0941 loss_val (RMSE): 0.3740\n",
      "Epoch: 0093 loss_train (RMSE): 0.0932 loss_val (RMSE): 0.3717\n",
      "Epoch: 0094 loss_train (RMSE): 0.0922 loss_val (RMSE): 0.3695\n",
      "Epoch: 0095 loss_train (RMSE): 0.0913 loss_val (RMSE): 0.3673\n",
      "Epoch: 0096 loss_train (RMSE): 0.0905 loss_val (RMSE): 0.3652\n",
      "Epoch: 0097 loss_train (RMSE): 0.0896 loss_val (RMSE): 0.3631\n",
      "Epoch: 0098 loss_train (RMSE): 0.0888 loss_val (RMSE): 0.3610\n",
      "Epoch: 0099 loss_train (RMSE): 0.0880 loss_val (RMSE): 0.3591\n",
      "Epoch: 0100 loss_train (RMSE): 0.0873 loss_val (RMSE): 0.3571\n",
      "Epoch: 0101 loss_train (RMSE): 0.0866 loss_val (RMSE): 0.3552\n",
      "Epoch: 0102 loss_train (RMSE): 0.0859 loss_val (RMSE): 0.3534\n",
      "Epoch: 0103 loss_train (RMSE): 0.0852 loss_val (RMSE): 0.3516\n",
      "Epoch: 0104 loss_train (RMSE): 0.0846 loss_val (RMSE): 0.3499\n",
      "Epoch: 0105 loss_train (RMSE): 0.0840 loss_val (RMSE): 0.3482\n",
      "Epoch: 0106 loss_train (RMSE): 0.0834 loss_val (RMSE): 0.3465\n",
      "Epoch: 0107 loss_train (RMSE): 0.0829 loss_val (RMSE): 0.3449\n",
      "Epoch: 0108 loss_train (RMSE): 0.0823 loss_val (RMSE): 0.3433\n",
      "Epoch: 0109 loss_train (RMSE): 0.0818 loss_val (RMSE): 0.3417\n",
      "Epoch: 0110 loss_train (RMSE): 0.0813 loss_val (RMSE): 0.3402\n",
      "Epoch: 0111 loss_train (RMSE): 0.0808 loss_val (RMSE): 0.3387\n",
      "Epoch: 0112 loss_train (RMSE): 0.0804 loss_val (RMSE): 0.3372\n",
      "Epoch: 0113 loss_train (RMSE): 0.0800 loss_val (RMSE): 0.3358\n",
      "Epoch: 0114 loss_train (RMSE): 0.0796 loss_val (RMSE): 0.3345\n",
      "Epoch: 0115 loss_train (RMSE): 0.0792 loss_val (RMSE): 0.3331\n",
      "Epoch: 0116 loss_train (RMSE): 0.0788 loss_val (RMSE): 0.3318\n",
      "Epoch: 0117 loss_train (RMSE): 0.0784 loss_val (RMSE): 0.3306\n",
      "Epoch: 0118 loss_train (RMSE): 0.0781 loss_val (RMSE): 0.3294\n",
      "Epoch: 0119 loss_train (RMSE): 0.0777 loss_val (RMSE): 0.3282\n",
      "Epoch: 0120 loss_train (RMSE): 0.0774 loss_val (RMSE): 0.3270\n",
      "Epoch: 0121 loss_train (RMSE): 0.0771 loss_val (RMSE): 0.3259\n",
      "Epoch: 0122 loss_train (RMSE): 0.0768 loss_val (RMSE): 0.3248\n",
      "Epoch: 0123 loss_train (RMSE): 0.0766 loss_val (RMSE): 0.3237\n",
      "Epoch: 0124 loss_train (RMSE): 0.0763 loss_val (RMSE): 0.3227\n",
      "Epoch: 0125 loss_train (RMSE): 0.0761 loss_val (RMSE): 0.3216\n",
      "Epoch: 0126 loss_train (RMSE): 0.0758 loss_val (RMSE): 0.3206\n",
      "Epoch: 0127 loss_train (RMSE): 0.0756 loss_val (RMSE): 0.3197\n",
      "Epoch: 0128 loss_train (RMSE): 0.0754 loss_val (RMSE): 0.3187\n",
      "Epoch: 0129 loss_train (RMSE): 0.0752 loss_val (RMSE): 0.3178\n",
      "Epoch: 0130 loss_train (RMSE): 0.0750 loss_val (RMSE): 0.3170\n",
      "Epoch: 0131 loss_train (RMSE): 0.0748 loss_val (RMSE): 0.3161\n",
      "Epoch: 0132 loss_train (RMSE): 0.0746 loss_val (RMSE): 0.3153\n",
      "Epoch: 0133 loss_train (RMSE): 0.0745 loss_val (RMSE): 0.3145\n",
      "Epoch: 0134 loss_train (RMSE): 0.0743 loss_val (RMSE): 0.3137\n",
      "Epoch: 0135 loss_train (RMSE): 0.0741 loss_val (RMSE): 0.3129\n",
      "Epoch: 0136 loss_train (RMSE): 0.0740 loss_val (RMSE): 0.3122\n",
      "Epoch: 0137 loss_train (RMSE): 0.0739 loss_val (RMSE): 0.3114\n",
      "Epoch: 0138 loss_train (RMSE): 0.0737 loss_val (RMSE): 0.3107\n",
      "Epoch: 0139 loss_train (RMSE): 0.0736 loss_val (RMSE): 0.3101\n",
      "Epoch: 0140 loss_train (RMSE): 0.0735 loss_val (RMSE): 0.3094\n",
      "Epoch: 0141 loss_train (RMSE): 0.0734 loss_val (RMSE): 0.3088\n",
      "Epoch: 0142 loss_train (RMSE): 0.0733 loss_val (RMSE): 0.3081\n",
      "Epoch: 0143 loss_train (RMSE): 0.0732 loss_val (RMSE): 0.3075\n",
      "Epoch: 0144 loss_train (RMSE): 0.0731 loss_val (RMSE): 0.3069\n",
      "Epoch: 0145 loss_train (RMSE): 0.0730 loss_val (RMSE): 0.3064\n",
      "Epoch: 0146 loss_train (RMSE): 0.0729 loss_val (RMSE): 0.3058\n",
      "Epoch: 0147 loss_train (RMSE): 0.0728 loss_val (RMSE): 0.3053\n",
      "Epoch: 0148 loss_train (RMSE): 0.0728 loss_val (RMSE): 0.3048\n",
      "Epoch: 0149 loss_train (RMSE): 0.0727 loss_val (RMSE): 0.3043\n",
      "Epoch: 0150 loss_train (RMSE): 0.0726 loss_val (RMSE): 0.3038\n",
      "Epoch: 0151 loss_train (RMSE): 0.0725 loss_val (RMSE): 0.3033\n",
      "Epoch: 0152 loss_train (RMSE): 0.0725 loss_val (RMSE): 0.3029\n",
      "Epoch: 0153 loss_train (RMSE): 0.0724 loss_val (RMSE): 0.3024\n",
      "Epoch: 0154 loss_train (RMSE): 0.0724 loss_val (RMSE): 0.3020\n",
      "Epoch: 0155 loss_train (RMSE): 0.0723 loss_val (RMSE): 0.3016\n",
      "Epoch: 0156 loss_train (RMSE): 0.0723 loss_val (RMSE): 0.3012\n",
      "Epoch: 0157 loss_train (RMSE): 0.0722 loss_val (RMSE): 0.3008\n",
      "Epoch: 0158 loss_train (RMSE): 0.0722 loss_val (RMSE): 0.3004\n",
      "Epoch: 0159 loss_train (RMSE): 0.0722 loss_val (RMSE): 0.3000\n",
      "Epoch: 0160 loss_train (RMSE): 0.0721 loss_val (RMSE): 0.2997\n",
      "Epoch: 0161 loss_train (RMSE): 0.0721 loss_val (RMSE): 0.2993\n",
      "Epoch: 0162 loss_train (RMSE): 0.0720 loss_val (RMSE): 0.2990\n",
      "Epoch: 0163 loss_train (RMSE): 0.0720 loss_val (RMSE): 0.2987\n",
      "Epoch: 0164 loss_train (RMSE): 0.0720 loss_val (RMSE): 0.2983\n",
      "Epoch: 0165 loss_train (RMSE): 0.0720 loss_val (RMSE): 0.2980\n",
      "Epoch: 0166 loss_train (RMSE): 0.0719 loss_val (RMSE): 0.2977\n",
      "Epoch: 0167 loss_train (RMSE): 0.0719 loss_val (RMSE): 0.2975\n",
      "Epoch: 0168 loss_train (RMSE): 0.0719 loss_val (RMSE): 0.2972\n",
      "Epoch: 0169 loss_train (RMSE): 0.0719 loss_val (RMSE): 0.2969\n",
      "Epoch: 0170 loss_train (RMSE): 0.0718 loss_val (RMSE): 0.2967\n",
      "Epoch: 0171 loss_train (RMSE): 0.0718 loss_val (RMSE): 0.2964\n",
      "Epoch: 0172 loss_train (RMSE): 0.0718 loss_val (RMSE): 0.2962\n",
      "Epoch: 0173 loss_train (RMSE): 0.0718 loss_val (RMSE): 0.2959\n",
      "Epoch: 0174 loss_train (RMSE): 0.0718 loss_val (RMSE): 0.2957\n",
      "Epoch: 0175 loss_train (RMSE): 0.0718 loss_val (RMSE): 0.2955\n",
      "Epoch: 0176 loss_train (RMSE): 0.0717 loss_val (RMSE): 0.2953\n",
      "Epoch: 0177 loss_train (RMSE): 0.0717 loss_val (RMSE): 0.2951\n",
      "Epoch: 0178 loss_train (RMSE): 0.0717 loss_val (RMSE): 0.2949\n",
      "Epoch: 0179 loss_train (RMSE): 0.0717 loss_val (RMSE): 0.2947\n",
      "Epoch: 0180 loss_train (RMSE): 0.0717 loss_val (RMSE): 0.2945\n",
      "Epoch: 0181 loss_train (RMSE): 0.0717 loss_val (RMSE): 0.2943\n",
      "Epoch: 0182 loss_train (RMSE): 0.0717 loss_val (RMSE): 0.2942\n",
      "Epoch: 0183 loss_train (RMSE): 0.0717 loss_val (RMSE): 0.2940\n",
      "Epoch: 0184 loss_train (RMSE): 0.0717 loss_val (RMSE): 0.2938\n",
      "Epoch: 0185 loss_train (RMSE): 0.0717 loss_val (RMSE): 0.2937\n",
      "Epoch: 0186 loss_train (RMSE): 0.0717 loss_val (RMSE): 0.2935\n",
      "Epoch: 0187 loss_train (RMSE): 0.0716 loss_val (RMSE): 0.2934\n",
      "Epoch: 0188 loss_train (RMSE): 0.0716 loss_val (RMSE): 0.2932\n",
      "Epoch: 0189 loss_train (RMSE): 0.0716 loss_val (RMSE): 0.2931\n",
      "Epoch: 0190 loss_train (RMSE): 0.0716 loss_val (RMSE): 0.2930\n",
      "Epoch: 0191 loss_train (RMSE): 0.0716 loss_val (RMSE): 0.2929\n",
      "Epoch: 0192 loss_train (RMSE): 0.0716 loss_val (RMSE): 0.2927\n",
      "Epoch: 0193 loss_train (RMSE): 0.0716 loss_val (RMSE): 0.2926\n",
      "Epoch: 0194 loss_train (RMSE): 0.0716 loss_val (RMSE): 0.2925\n",
      "Epoch: 0195 loss_train (RMSE): 0.0716 loss_val (RMSE): 0.2924\n",
      "Epoch: 0196 loss_train (RMSE): 0.0716 loss_val (RMSE): 0.2923\n",
      "Epoch: 0197 loss_train (RMSE): 0.0716 loss_val (RMSE): 0.2922\n",
      "Epoch: 0198 loss_train (RMSE): 0.0716 loss_val (RMSE): 0.2921\n",
      "Epoch: 0199 loss_train (RMSE): 0.0716 loss_val (RMSE): 0.2920\n",
      "Epoch: 0200 loss_train (RMSE): 0.0716 loss_val (RMSE): 0.2919\n",
      "Optimization Finished!\n",
      "Total time elapsed: 0.3999s\n",
      "Test set results: loss= 0.3033 (RMSE)\n",
      "GCN(\n",
      "  (gc1): GraphConvolution (10 -> 2000)\n",
      "  (gc5): GraphConvolution (2000 -> 2)\n",
      ")\n",
      "Epoch: 0001 loss_train (RMSE): 0.5866 loss_val (RMSE): 0.8473\n",
      "Epoch: 0002 loss_train (RMSE): 0.5724 loss_val (RMSE): 0.8379\n",
      "Epoch: 0003 loss_train (RMSE): 0.5585 loss_val (RMSE): 0.8285\n",
      "Epoch: 0004 loss_train (RMSE): 0.5447 loss_val (RMSE): 0.8192\n",
      "Epoch: 0005 loss_train (RMSE): 0.5312 loss_val (RMSE): 0.8098\n",
      "Epoch: 0006 loss_train (RMSE): 0.5179 loss_val (RMSE): 0.8005\n",
      "Epoch: 0007 loss_train (RMSE): 0.5048 loss_val (RMSE): 0.7912\n",
      "Epoch: 0008 loss_train (RMSE): 0.4919 loss_val (RMSE): 0.7820\n",
      "Epoch: 0009 loss_train (RMSE): 0.4793 loss_val (RMSE): 0.7727\n",
      "Epoch: 0010 loss_train (RMSE): 0.4668 loss_val (RMSE): 0.7636\n",
      "Epoch: 0011 loss_train (RMSE): 0.4546 loss_val (RMSE): 0.7545\n",
      "Epoch: 0012 loss_train (RMSE): 0.4427 loss_val (RMSE): 0.7454\n",
      "Epoch: 0013 loss_train (RMSE): 0.4309 loss_val (RMSE): 0.7364\n",
      "Epoch: 0014 loss_train (RMSE): 0.4194 loss_val (RMSE): 0.7274\n",
      "Epoch: 0015 loss_train (RMSE): 0.4082 loss_val (RMSE): 0.7185\n",
      "Epoch: 0016 loss_train (RMSE): 0.3971 loss_val (RMSE): 0.7096\n",
      "Epoch: 0017 loss_train (RMSE): 0.3863 loss_val (RMSE): 0.7008\n",
      "Epoch: 0018 loss_train (RMSE): 0.3758 loss_val (RMSE): 0.6921\n",
      "Epoch: 0019 loss_train (RMSE): 0.3654 loss_val (RMSE): 0.6834\n",
      "Epoch: 0020 loss_train (RMSE): 0.3554 loss_val (RMSE): 0.6748\n",
      "Epoch: 0021 loss_train (RMSE): 0.3455 loss_val (RMSE): 0.6663\n",
      "Epoch: 0022 loss_train (RMSE): 0.3359 loss_val (RMSE): 0.6579\n",
      "Epoch: 0023 loss_train (RMSE): 0.3265 loss_val (RMSE): 0.6496\n",
      "Epoch: 0024 loss_train (RMSE): 0.3174 loss_val (RMSE): 0.6413\n",
      "Epoch: 0025 loss_train (RMSE): 0.3085 loss_val (RMSE): 0.6331\n",
      "Epoch: 0026 loss_train (RMSE): 0.2998 loss_val (RMSE): 0.6250\n",
      "Epoch: 0027 loss_train (RMSE): 0.2913 loss_val (RMSE): 0.6170\n",
      "Epoch: 0028 loss_train (RMSE): 0.2831 loss_val (RMSE): 0.6091\n",
      "Epoch: 0029 loss_train (RMSE): 0.2751 loss_val (RMSE): 0.6013\n",
      "Epoch: 0030 loss_train (RMSE): 0.2673 loss_val (RMSE): 0.5935\n",
      "Epoch: 0031 loss_train (RMSE): 0.2598 loss_val (RMSE): 0.5859\n",
      "Epoch: 0032 loss_train (RMSE): 0.2525 loss_val (RMSE): 0.5784\n",
      "Epoch: 0033 loss_train (RMSE): 0.2454 loss_val (RMSE): 0.5710\n",
      "Epoch: 0034 loss_train (RMSE): 0.2385 loss_val (RMSE): 0.5636\n",
      "Epoch: 0035 loss_train (RMSE): 0.2318 loss_val (RMSE): 0.5564\n",
      "Epoch: 0036 loss_train (RMSE): 0.2253 loss_val (RMSE): 0.5493\n",
      "Epoch: 0037 loss_train (RMSE): 0.2190 loss_val (RMSE): 0.5423\n",
      "Epoch: 0038 loss_train (RMSE): 0.2130 loss_val (RMSE): 0.5354\n",
      "Epoch: 0039 loss_train (RMSE): 0.2071 loss_val (RMSE): 0.5287\n",
      "Epoch: 0040 loss_train (RMSE): 0.2014 loss_val (RMSE): 0.5220\n",
      "Epoch: 0041 loss_train (RMSE): 0.1959 loss_val (RMSE): 0.5155\n",
      "Epoch: 0042 loss_train (RMSE): 0.1906 loss_val (RMSE): 0.5090\n",
      "Epoch: 0043 loss_train (RMSE): 0.1855 loss_val (RMSE): 0.5027\n",
      "Epoch: 0044 loss_train (RMSE): 0.1806 loss_val (RMSE): 0.4965\n",
      "Epoch: 0045 loss_train (RMSE): 0.1759 loss_val (RMSE): 0.4905\n",
      "Epoch: 0046 loss_train (RMSE): 0.1713 loss_val (RMSE): 0.4845\n",
      "Epoch: 0047 loss_train (RMSE): 0.1669 loss_val (RMSE): 0.4787\n",
      "Epoch: 0048 loss_train (RMSE): 0.1626 loss_val (RMSE): 0.4729\n",
      "Epoch: 0049 loss_train (RMSE): 0.1586 loss_val (RMSE): 0.4673\n",
      "Epoch: 0050 loss_train (RMSE): 0.1546 loss_val (RMSE): 0.4618\n",
      "Epoch: 0051 loss_train (RMSE): 0.1509 loss_val (RMSE): 0.4565\n",
      "Epoch: 0052 loss_train (RMSE): 0.1472 loss_val (RMSE): 0.4512\n",
      "Epoch: 0053 loss_train (RMSE): 0.1438 loss_val (RMSE): 0.4461\n",
      "Epoch: 0054 loss_train (RMSE): 0.1404 loss_val (RMSE): 0.4411\n",
      "Epoch: 0055 loss_train (RMSE): 0.1372 loss_val (RMSE): 0.4362\n",
      "Epoch: 0056 loss_train (RMSE): 0.1342 loss_val (RMSE): 0.4315\n",
      "Epoch: 0057 loss_train (RMSE): 0.1313 loss_val (RMSE): 0.4268\n",
      "Epoch: 0058 loss_train (RMSE): 0.1285 loss_val (RMSE): 0.4223\n",
      "Epoch: 0059 loss_train (RMSE): 0.1258 loss_val (RMSE): 0.4179\n",
      "Epoch: 0060 loss_train (RMSE): 0.1232 loss_val (RMSE): 0.4136\n",
      "Epoch: 0061 loss_train (RMSE): 0.1208 loss_val (RMSE): 0.4094\n",
      "Epoch: 0062 loss_train (RMSE): 0.1184 loss_val (RMSE): 0.4053\n",
      "Epoch: 0063 loss_train (RMSE): 0.1162 loss_val (RMSE): 0.4013\n",
      "Epoch: 0064 loss_train (RMSE): 0.1141 loss_val (RMSE): 0.3975\n",
      "Epoch: 0065 loss_train (RMSE): 0.1120 loss_val (RMSE): 0.3938\n",
      "Epoch: 0066 loss_train (RMSE): 0.1101 loss_val (RMSE): 0.3901\n",
      "Epoch: 0067 loss_train (RMSE): 0.1083 loss_val (RMSE): 0.3866\n",
      "Epoch: 0068 loss_train (RMSE): 0.1065 loss_val (RMSE): 0.3832\n",
      "Epoch: 0069 loss_train (RMSE): 0.1049 loss_val (RMSE): 0.3799\n",
      "Epoch: 0070 loss_train (RMSE): 0.1033 loss_val (RMSE): 0.3767\n",
      "Epoch: 0071 loss_train (RMSE): 0.1018 loss_val (RMSE): 0.3736\n",
      "Epoch: 0072 loss_train (RMSE): 0.1004 loss_val (RMSE): 0.3706\n",
      "Epoch: 0073 loss_train (RMSE): 0.0990 loss_val (RMSE): 0.3677\n",
      "Epoch: 0074 loss_train (RMSE): 0.0977 loss_val (RMSE): 0.3649\n",
      "Epoch: 0075 loss_train (RMSE): 0.0965 loss_val (RMSE): 0.3622\n",
      "Epoch: 0076 loss_train (RMSE): 0.0954 loss_val (RMSE): 0.3595\n",
      "Epoch: 0077 loss_train (RMSE): 0.0943 loss_val (RMSE): 0.3570\n",
      "Epoch: 0078 loss_train (RMSE): 0.0933 loss_val (RMSE): 0.3545\n",
      "Epoch: 0079 loss_train (RMSE): 0.0923 loss_val (RMSE): 0.3522\n",
      "Epoch: 0080 loss_train (RMSE): 0.0914 loss_val (RMSE): 0.3499\n",
      "Epoch: 0081 loss_train (RMSE): 0.0906 loss_val (RMSE): 0.3477\n",
      "Epoch: 0082 loss_train (RMSE): 0.0897 loss_val (RMSE): 0.3456\n",
      "Epoch: 0083 loss_train (RMSE): 0.0890 loss_val (RMSE): 0.3436\n",
      "Epoch: 0084 loss_train (RMSE): 0.0883 loss_val (RMSE): 0.3416\n",
      "Epoch: 0085 loss_train (RMSE): 0.0876 loss_val (RMSE): 0.3397\n",
      "Epoch: 0086 loss_train (RMSE): 0.0870 loss_val (RMSE): 0.3379\n",
      "Epoch: 0087 loss_train (RMSE): 0.0864 loss_val (RMSE): 0.3362\n",
      "Epoch: 0088 loss_train (RMSE): 0.0858 loss_val (RMSE): 0.3345\n",
      "Epoch: 0089 loss_train (RMSE): 0.0853 loss_val (RMSE): 0.3329\n",
      "Epoch: 0090 loss_train (RMSE): 0.0848 loss_val (RMSE): 0.3313\n",
      "Epoch: 0091 loss_train (RMSE): 0.0843 loss_val (RMSE): 0.3298\n",
      "Epoch: 0092 loss_train (RMSE): 0.0839 loss_val (RMSE): 0.3284\n",
      "Epoch: 0093 loss_train (RMSE): 0.0835 loss_val (RMSE): 0.3271\n",
      "Epoch: 0094 loss_train (RMSE): 0.0831 loss_val (RMSE): 0.3257\n",
      "Epoch: 0095 loss_train (RMSE): 0.0828 loss_val (RMSE): 0.3245\n",
      "Epoch: 0096 loss_train (RMSE): 0.0824 loss_val (RMSE): 0.3233\n",
      "Epoch: 0097 loss_train (RMSE): 0.0821 loss_val (RMSE): 0.3221\n",
      "Epoch: 0098 loss_train (RMSE): 0.0819 loss_val (RMSE): 0.3210\n",
      "Epoch: 0099 loss_train (RMSE): 0.0816 loss_val (RMSE): 0.3200\n",
      "Epoch: 0100 loss_train (RMSE): 0.0814 loss_val (RMSE): 0.3189\n",
      "Epoch: 0101 loss_train (RMSE): 0.0811 loss_val (RMSE): 0.3180\n",
      "Epoch: 0102 loss_train (RMSE): 0.0809 loss_val (RMSE): 0.3170\n",
      "Epoch: 0103 loss_train (RMSE): 0.0807 loss_val (RMSE): 0.3161\n",
      "Epoch: 0104 loss_train (RMSE): 0.0805 loss_val (RMSE): 0.3153\n",
      "Epoch: 0105 loss_train (RMSE): 0.0804 loss_val (RMSE): 0.3145\n",
      "Epoch: 0106 loss_train (RMSE): 0.0802 loss_val (RMSE): 0.3137\n",
      "Epoch: 0107 loss_train (RMSE): 0.0801 loss_val (RMSE): 0.3130\n",
      "Epoch: 0108 loss_train (RMSE): 0.0799 loss_val (RMSE): 0.3122\n",
      "Epoch: 0109 loss_train (RMSE): 0.0798 loss_val (RMSE): 0.3116\n",
      "Epoch: 0110 loss_train (RMSE): 0.0797 loss_val (RMSE): 0.3109\n",
      "Epoch: 0111 loss_train (RMSE): 0.0796 loss_val (RMSE): 0.3103\n",
      "Epoch: 0112 loss_train (RMSE): 0.0795 loss_val (RMSE): 0.3097\n",
      "Epoch: 0113 loss_train (RMSE): 0.0794 loss_val (RMSE): 0.3091\n",
      "Epoch: 0114 loss_train (RMSE): 0.0793 loss_val (RMSE): 0.3086\n",
      "Epoch: 0115 loss_train (RMSE): 0.0793 loss_val (RMSE): 0.3081\n",
      "Epoch: 0116 loss_train (RMSE): 0.0792 loss_val (RMSE): 0.3076\n",
      "Epoch: 0117 loss_train (RMSE): 0.0791 loss_val (RMSE): 0.3071\n",
      "Epoch: 0118 loss_train (RMSE): 0.0791 loss_val (RMSE): 0.3067\n",
      "Epoch: 0119 loss_train (RMSE): 0.0790 loss_val (RMSE): 0.3063\n",
      "Epoch: 0120 loss_train (RMSE): 0.0790 loss_val (RMSE): 0.3059\n",
      "Epoch: 0121 loss_train (RMSE): 0.0789 loss_val (RMSE): 0.3055\n",
      "Epoch: 0122 loss_train (RMSE): 0.0789 loss_val (RMSE): 0.3051\n",
      "Epoch: 0123 loss_train (RMSE): 0.0789 loss_val (RMSE): 0.3048\n",
      "Epoch: 0124 loss_train (RMSE): 0.0788 loss_val (RMSE): 0.3044\n",
      "Epoch: 0125 loss_train (RMSE): 0.0788 loss_val (RMSE): 0.3041\n",
      "Epoch: 0126 loss_train (RMSE): 0.0788 loss_val (RMSE): 0.3038\n",
      "Epoch: 0127 loss_train (RMSE): 0.0787 loss_val (RMSE): 0.3035\n",
      "Epoch: 0128 loss_train (RMSE): 0.0787 loss_val (RMSE): 0.3032\n",
      "Epoch: 0129 loss_train (RMSE): 0.0787 loss_val (RMSE): 0.3030\n",
      "Epoch: 0130 loss_train (RMSE): 0.0787 loss_val (RMSE): 0.3027\n",
      "Epoch: 0131 loss_train (RMSE): 0.0787 loss_val (RMSE): 0.3025\n",
      "Epoch: 0132 loss_train (RMSE): 0.0787 loss_val (RMSE): 0.3023\n",
      "Epoch: 0133 loss_train (RMSE): 0.0786 loss_val (RMSE): 0.3021\n",
      "Epoch: 0134 loss_train (RMSE): 0.0786 loss_val (RMSE): 0.3019\n",
      "Epoch: 0135 loss_train (RMSE): 0.0786 loss_val (RMSE): 0.3017\n",
      "Epoch: 0136 loss_train (RMSE): 0.0786 loss_val (RMSE): 0.3015\n",
      "Epoch: 0137 loss_train (RMSE): 0.0786 loss_val (RMSE): 0.3013\n",
      "Epoch: 0138 loss_train (RMSE): 0.0786 loss_val (RMSE): 0.3012\n",
      "Epoch: 0139 loss_train (RMSE): 0.0786 loss_val (RMSE): 0.3010\n",
      "Epoch: 0140 loss_train (RMSE): 0.0786 loss_val (RMSE): 0.3009\n",
      "Epoch: 0141 loss_train (RMSE): 0.0786 loss_val (RMSE): 0.3008\n",
      "Epoch: 0142 loss_train (RMSE): 0.0786 loss_val (RMSE): 0.3006\n",
      "Epoch: 0143 loss_train (RMSE): 0.0786 loss_val (RMSE): 0.3005\n",
      "Epoch: 0144 loss_train (RMSE): 0.0786 loss_val (RMSE): 0.3004\n",
      "Epoch: 0145 loss_train (RMSE): 0.0786 loss_val (RMSE): 0.3003\n",
      "Epoch: 0146 loss_train (RMSE): 0.0786 loss_val (RMSE): 0.3002\n",
      "Epoch: 0147 loss_train (RMSE): 0.0786 loss_val (RMSE): 0.3001\n",
      "Epoch: 0148 loss_train (RMSE): 0.0786 loss_val (RMSE): 0.3000\n",
      "Epoch: 0149 loss_train (RMSE): 0.0786 loss_val (RMSE): 0.2999\n",
      "Epoch: 0150 loss_train (RMSE): 0.0786 loss_val (RMSE): 0.2998\n",
      "Epoch: 0151 loss_train (RMSE): 0.0786 loss_val (RMSE): 0.2998\n",
      "Epoch: 0152 loss_train (RMSE): 0.0786 loss_val (RMSE): 0.2997\n",
      "Epoch: 0153 loss_train (RMSE): 0.0786 loss_val (RMSE): 0.2996\n",
      "Epoch: 0154 loss_train (RMSE): 0.0786 loss_val (RMSE): 0.2996\n",
      "Epoch: 0155 loss_train (RMSE): 0.0785 loss_val (RMSE): 0.2995\n",
      "Epoch: 0156 loss_train (RMSE): 0.0785 loss_val (RMSE): 0.2995\n",
      "Epoch: 0157 loss_train (RMSE): 0.0785 loss_val (RMSE): 0.2994\n",
      "Epoch: 0158 loss_train (RMSE): 0.0785 loss_val (RMSE): 0.2994\n",
      "Epoch: 0159 loss_train (RMSE): 0.0785 loss_val (RMSE): 0.2993\n",
      "Epoch: 0160 loss_train (RMSE): 0.0785 loss_val (RMSE): 0.2993\n",
      "Epoch: 0161 loss_train (RMSE): 0.0785 loss_val (RMSE): 0.2992\n",
      "Epoch: 0162 loss_train (RMSE): 0.0785 loss_val (RMSE): 0.2992\n",
      "Epoch: 0163 loss_train (RMSE): 0.0785 loss_val (RMSE): 0.2992\n",
      "Epoch: 0164 loss_train (RMSE): 0.0785 loss_val (RMSE): 0.2991\n",
      "Epoch: 0165 loss_train (RMSE): 0.0785 loss_val (RMSE): 0.2991\n",
      "Epoch: 0166 loss_train (RMSE): 0.0785 loss_val (RMSE): 0.2991\n",
      "Epoch: 0167 loss_train (RMSE): 0.0785 loss_val (RMSE): 0.2991\n",
      "Epoch: 0168 loss_train (RMSE): 0.0785 loss_val (RMSE): 0.2990\n",
      "Epoch: 0169 loss_train (RMSE): 0.0785 loss_val (RMSE): 0.2990\n",
      "Epoch: 0170 loss_train (RMSE): 0.0785 loss_val (RMSE): 0.2990\n",
      "Epoch: 0171 loss_train (RMSE): 0.0785 loss_val (RMSE): 0.2990\n",
      "Epoch: 0172 loss_train (RMSE): 0.0785 loss_val (RMSE): 0.2990\n",
      "Epoch: 0173 loss_train (RMSE): 0.0785 loss_val (RMSE): 0.2990\n",
      "Epoch: 0174 loss_train (RMSE): 0.0785 loss_val (RMSE): 0.2990\n",
      "Epoch: 0175 loss_train (RMSE): 0.0785 loss_val (RMSE): 0.2989\n",
      "Epoch: 0176 loss_train (RMSE): 0.0785 loss_val (RMSE): 0.2989\n",
      "Epoch: 0177 loss_train (RMSE): 0.0785 loss_val (RMSE): 0.2989\n",
      "Epoch: 0178 loss_train (RMSE): 0.0785 loss_val (RMSE): 0.2989\n",
      "Epoch: 0179 loss_train (RMSE): 0.0785 loss_val (RMSE): 0.2989\n",
      "Epoch: 0180 loss_train (RMSE): 0.0785 loss_val (RMSE): 0.2989\n",
      "Epoch: 0181 loss_train (RMSE): 0.0785 loss_val (RMSE): 0.2989\n",
      "Epoch: 0182 loss_train (RMSE): 0.0785 loss_val (RMSE): 0.2989\n",
      "Epoch: 0183 loss_train (RMSE): 0.0785 loss_val (RMSE): 0.2989\n",
      "Epoch: 0184 loss_train (RMSE): 0.0785 loss_val (RMSE): 0.2989\n",
      "Epoch: 0185 loss_train (RMSE): 0.0785 loss_val (RMSE): 0.2989\n",
      "Epoch: 0186 loss_train (RMSE): 0.0785 loss_val (RMSE): 0.2989\n",
      "Epoch: 0187 loss_train (RMSE): 0.0785 loss_val (RMSE): 0.2989\n",
      "Epoch: 0188 loss_train (RMSE): 0.0785 loss_val (RMSE): 0.2989\n",
      "Epoch: 0189 loss_train (RMSE): 0.0785 loss_val (RMSE): 0.2989\n",
      "Epoch: 0190 loss_train (RMSE): 0.0785 loss_val (RMSE): 0.2989\n",
      "Epoch: 0191 loss_train (RMSE): 0.0785 loss_val (RMSE): 0.2989\n",
      "Epoch: 0192 loss_train (RMSE): 0.0785 loss_val (RMSE): 0.2989\n",
      "Epoch: 0193 loss_train (RMSE): 0.0785 loss_val (RMSE): 0.2989\n",
      "Epoch: 0194 loss_train (RMSE): 0.0785 loss_val (RMSE): 0.2989\n",
      "Epoch: 0195 loss_train (RMSE): 0.0785 loss_val (RMSE): 0.2989\n",
      "Epoch: 0196 loss_train (RMSE): 0.0785 loss_val (RMSE): 0.2989\n",
      "Epoch: 0197 loss_train (RMSE): 0.0785 loss_val (RMSE): 0.2989\n",
      "Epoch: 0198 loss_train (RMSE): 0.0785 loss_val (RMSE): 0.2989\n",
      "Epoch: 0199 loss_train (RMSE): 0.0785 loss_val (RMSE): 0.2989\n",
      "Epoch: 0200 loss_train (RMSE): 0.0785 loss_val (RMSE): 0.2989\n",
      "Optimization Finished!\n",
      "Total time elapsed: 0.3935s\n",
      "Test set results: loss= 0.2915 (RMSE)\n",
      "GCN(\n",
      "  (gc1): GraphConvolution (10 -> 2000)\n",
      "  (gc5): GraphConvolution (2000 -> 2)\n",
      ")\n",
      "Epoch: 0001 loss_train (RMSE): 0.1943 loss_val (RMSE): 0.4134\n",
      "Epoch: 0002 loss_train (RMSE): 0.1881 loss_val (RMSE): 0.4070\n",
      "Epoch: 0003 loss_train (RMSE): 0.1822 loss_val (RMSE): 0.4007\n",
      "Epoch: 0004 loss_train (RMSE): 0.1765 loss_val (RMSE): 0.3946\n",
      "Epoch: 0005 loss_train (RMSE): 0.1710 loss_val (RMSE): 0.3887\n",
      "Epoch: 0006 loss_train (RMSE): 0.1657 loss_val (RMSE): 0.3830\n",
      "Epoch: 0007 loss_train (RMSE): 0.1606 loss_val (RMSE): 0.3774\n",
      "Epoch: 0008 loss_train (RMSE): 0.1557 loss_val (RMSE): 0.3721\n",
      "Epoch: 0009 loss_train (RMSE): 0.1510 loss_val (RMSE): 0.3669\n",
      "Epoch: 0010 loss_train (RMSE): 0.1466 loss_val (RMSE): 0.3619\n",
      "Epoch: 0011 loss_train (RMSE): 0.1424 loss_val (RMSE): 0.3571\n",
      "Epoch: 0012 loss_train (RMSE): 0.1384 loss_val (RMSE): 0.3526\n",
      "Epoch: 0013 loss_train (RMSE): 0.1345 loss_val (RMSE): 0.3482\n",
      "Epoch: 0014 loss_train (RMSE): 0.1309 loss_val (RMSE): 0.3440\n",
      "Epoch: 0015 loss_train (RMSE): 0.1275 loss_val (RMSE): 0.3400\n",
      "Epoch: 0016 loss_train (RMSE): 0.1243 loss_val (RMSE): 0.3362\n",
      "Epoch: 0017 loss_train (RMSE): 0.1213 loss_val (RMSE): 0.3325\n",
      "Epoch: 0018 loss_train (RMSE): 0.1184 loss_val (RMSE): 0.3290\n",
      "Epoch: 0019 loss_train (RMSE): 0.1157 loss_val (RMSE): 0.3257\n",
      "Epoch: 0020 loss_train (RMSE): 0.1132 loss_val (RMSE): 0.3224\n",
      "Epoch: 0021 loss_train (RMSE): 0.1108 loss_val (RMSE): 0.3193\n",
      "Epoch: 0022 loss_train (RMSE): 0.1085 loss_val (RMSE): 0.3163\n",
      "Epoch: 0023 loss_train (RMSE): 0.1064 loss_val (RMSE): 0.3133\n",
      "Epoch: 0024 loss_train (RMSE): 0.1044 loss_val (RMSE): 0.3105\n",
      "Epoch: 0025 loss_train (RMSE): 0.1025 loss_val (RMSE): 0.3077\n",
      "Epoch: 0026 loss_train (RMSE): 0.1007 loss_val (RMSE): 0.3050\n",
      "Epoch: 0027 loss_train (RMSE): 0.0990 loss_val (RMSE): 0.3023\n",
      "Epoch: 0028 loss_train (RMSE): 0.0974 loss_val (RMSE): 0.2997\n",
      "Epoch: 0029 loss_train (RMSE): 0.0959 loss_val (RMSE): 0.2971\n",
      "Epoch: 0030 loss_train (RMSE): 0.0945 loss_val (RMSE): 0.2945\n",
      "Epoch: 0031 loss_train (RMSE): 0.0931 loss_val (RMSE): 0.2920\n",
      "Epoch: 0032 loss_train (RMSE): 0.0918 loss_val (RMSE): 0.2896\n",
      "Epoch: 0033 loss_train (RMSE): 0.0906 loss_val (RMSE): 0.2872\n",
      "Epoch: 0034 loss_train (RMSE): 0.0895 loss_val (RMSE): 0.2848\n",
      "Epoch: 0035 loss_train (RMSE): 0.0884 loss_val (RMSE): 0.2825\n",
      "Epoch: 0036 loss_train (RMSE): 0.0874 loss_val (RMSE): 0.2803\n",
      "Epoch: 0037 loss_train (RMSE): 0.0864 loss_val (RMSE): 0.2781\n",
      "Epoch: 0038 loss_train (RMSE): 0.0856 loss_val (RMSE): 0.2760\n",
      "Epoch: 0039 loss_train (RMSE): 0.0848 loss_val (RMSE): 0.2740\n",
      "Epoch: 0040 loss_train (RMSE): 0.0840 loss_val (RMSE): 0.2721\n",
      "Epoch: 0041 loss_train (RMSE): 0.0833 loss_val (RMSE): 0.2703\n",
      "Epoch: 0042 loss_train (RMSE): 0.0827 loss_val (RMSE): 0.2685\n",
      "Epoch: 0043 loss_train (RMSE): 0.0821 loss_val (RMSE): 0.2669\n",
      "Epoch: 0044 loss_train (RMSE): 0.0815 loss_val (RMSE): 0.2654\n",
      "Epoch: 0045 loss_train (RMSE): 0.0811 loss_val (RMSE): 0.2639\n",
      "Epoch: 0046 loss_train (RMSE): 0.0806 loss_val (RMSE): 0.2626\n",
      "Epoch: 0047 loss_train (RMSE): 0.0802 loss_val (RMSE): 0.2613\n",
      "Epoch: 0048 loss_train (RMSE): 0.0799 loss_val (RMSE): 0.2602\n",
      "Epoch: 0049 loss_train (RMSE): 0.0796 loss_val (RMSE): 0.2591\n",
      "Epoch: 0050 loss_train (RMSE): 0.0793 loss_val (RMSE): 0.2582\n",
      "Epoch: 0051 loss_train (RMSE): 0.0791 loss_val (RMSE): 0.2573\n",
      "Epoch: 0052 loss_train (RMSE): 0.0789 loss_val (RMSE): 0.2565\n",
      "Epoch: 0053 loss_train (RMSE): 0.0787 loss_val (RMSE): 0.2558\n",
      "Epoch: 0054 loss_train (RMSE): 0.0785 loss_val (RMSE): 0.2552\n",
      "Epoch: 0055 loss_train (RMSE): 0.0784 loss_val (RMSE): 0.2547\n",
      "Epoch: 0056 loss_train (RMSE): 0.0782 loss_val (RMSE): 0.2542\n",
      "Epoch: 0057 loss_train (RMSE): 0.0781 loss_val (RMSE): 0.2538\n",
      "Epoch: 0058 loss_train (RMSE): 0.0780 loss_val (RMSE): 0.2534\n",
      "Epoch: 0059 loss_train (RMSE): 0.0779 loss_val (RMSE): 0.2532\n",
      "Epoch: 0060 loss_train (RMSE): 0.0779 loss_val (RMSE): 0.2529\n",
      "Epoch: 0061 loss_train (RMSE): 0.0778 loss_val (RMSE): 0.2527\n",
      "Epoch: 0062 loss_train (RMSE): 0.0778 loss_val (RMSE): 0.2526\n",
      "Epoch: 0063 loss_train (RMSE): 0.0777 loss_val (RMSE): 0.2524\n",
      "Epoch: 0064 loss_train (RMSE): 0.0777 loss_val (RMSE): 0.2524\n",
      "Epoch: 0065 loss_train (RMSE): 0.0777 loss_val (RMSE): 0.2523\n",
      "Epoch: 0066 loss_train (RMSE): 0.0776 loss_val (RMSE): 0.2523\n",
      "Epoch: 0067 loss_train (RMSE): 0.0776 loss_val (RMSE): 0.2523\n",
      "Epoch: 0068 loss_train (RMSE): 0.0776 loss_val (RMSE): 0.2523\n",
      "Epoch: 0069 loss_train (RMSE): 0.0776 loss_val (RMSE): 0.2523\n",
      "Epoch: 0070 loss_train (RMSE): 0.0776 loss_val (RMSE): 0.2523\n",
      "Epoch: 0071 loss_train (RMSE): 0.0776 loss_val (RMSE): 0.2524\n",
      "Epoch: 0072 loss_train (RMSE): 0.0776 loss_val (RMSE): 0.2524\n",
      "Epoch: 0073 loss_train (RMSE): 0.0776 loss_val (RMSE): 0.2525\n",
      "Epoch: 0074 loss_train (RMSE): 0.0776 loss_val (RMSE): 0.2525\n",
      "Epoch: 0075 loss_train (RMSE): 0.0776 loss_val (RMSE): 0.2525\n",
      "Epoch: 0076 loss_train (RMSE): 0.0776 loss_val (RMSE): 0.2526\n",
      "Epoch: 0077 loss_train (RMSE): 0.0776 loss_val (RMSE): 0.2526\n",
      "Epoch: 0078 loss_train (RMSE): 0.0776 loss_val (RMSE): 0.2526\n",
      "Epoch: 0079 loss_train (RMSE): 0.0776 loss_val (RMSE): 0.2527\n",
      "Epoch: 0080 loss_train (RMSE): 0.0776 loss_val (RMSE): 0.2527\n",
      "Epoch: 0081 loss_train (RMSE): 0.0776 loss_val (RMSE): 0.2527\n",
      "Epoch: 0082 loss_train (RMSE): 0.0776 loss_val (RMSE): 0.2527\n",
      "Epoch: 0083 loss_train (RMSE): 0.0776 loss_val (RMSE): 0.2527\n",
      "Epoch: 0084 loss_train (RMSE): 0.0776 loss_val (RMSE): 0.2526\n",
      "Epoch: 0085 loss_train (RMSE): 0.0776 loss_val (RMSE): 0.2526\n",
      "Epoch: 0086 loss_train (RMSE): 0.0776 loss_val (RMSE): 0.2526\n",
      "Epoch: 0087 loss_train (RMSE): 0.0776 loss_val (RMSE): 0.2526\n",
      "Epoch: 0088 loss_train (RMSE): 0.0776 loss_val (RMSE): 0.2525\n",
      "Epoch: 0089 loss_train (RMSE): 0.0776 loss_val (RMSE): 0.2525\n",
      "Epoch: 0090 loss_train (RMSE): 0.0776 loss_val (RMSE): 0.2525\n",
      "Epoch: 0091 loss_train (RMSE): 0.0776 loss_val (RMSE): 0.2524\n",
      "Epoch: 0092 loss_train (RMSE): 0.0776 loss_val (RMSE): 0.2524\n",
      "Epoch: 0093 loss_train (RMSE): 0.0776 loss_val (RMSE): 0.2524\n",
      "Epoch: 0094 loss_train (RMSE): 0.0776 loss_val (RMSE): 0.2523\n",
      "Epoch: 0095 loss_train (RMSE): 0.0776 loss_val (RMSE): 0.2523\n",
      "Epoch: 0096 loss_train (RMSE): 0.0776 loss_val (RMSE): 0.2523\n",
      "Epoch: 0097 loss_train (RMSE): 0.0776 loss_val (RMSE): 0.2523\n",
      "Epoch: 0098 loss_train (RMSE): 0.0776 loss_val (RMSE): 0.2522\n",
      "Epoch: 0099 loss_train (RMSE): 0.0776 loss_val (RMSE): 0.2522\n",
      "Epoch: 0100 loss_train (RMSE): 0.0776 loss_val (RMSE): 0.2522\n",
      "Epoch: 0101 loss_train (RMSE): 0.0776 loss_val (RMSE): 0.2522\n",
      "Epoch: 0102 loss_train (RMSE): 0.0776 loss_val (RMSE): 0.2522\n",
      "Epoch: 0103 loss_train (RMSE): 0.0776 loss_val (RMSE): 0.2522\n",
      "Epoch: 0104 loss_train (RMSE): 0.0776 loss_val (RMSE): 0.2522\n",
      "Epoch: 0105 loss_train (RMSE): 0.0776 loss_val (RMSE): 0.2522\n",
      "Epoch: 0106 loss_train (RMSE): 0.0776 loss_val (RMSE): 0.2523\n",
      "Epoch: 0107 loss_train (RMSE): 0.0776 loss_val (RMSE): 0.2523\n",
      "Epoch: 0108 loss_train (RMSE): 0.0776 loss_val (RMSE): 0.2523\n",
      "Epoch: 0109 loss_train (RMSE): 0.0776 loss_val (RMSE): 0.2523\n",
      "Epoch: 0110 loss_train (RMSE): 0.0776 loss_val (RMSE): 0.2523\n",
      "Epoch: 0111 loss_train (RMSE): 0.0776 loss_val (RMSE): 0.2523\n",
      "Epoch: 0112 loss_train (RMSE): 0.0776 loss_val (RMSE): 0.2524\n",
      "Epoch: 0113 loss_train (RMSE): 0.0776 loss_val (RMSE): 0.2524\n",
      "Epoch: 0114 loss_train (RMSE): 0.0776 loss_val (RMSE): 0.2524\n",
      "Epoch: 0115 loss_train (RMSE): 0.0776 loss_val (RMSE): 0.2524\n",
      "Epoch: 0116 loss_train (RMSE): 0.0776 loss_val (RMSE): 0.2525\n",
      "Epoch: 0117 loss_train (RMSE): 0.0776 loss_val (RMSE): 0.2525\n",
      "Epoch: 0118 loss_train (RMSE): 0.0776 loss_val (RMSE): 0.2525\n",
      "Epoch: 0119 loss_train (RMSE): 0.0776 loss_val (RMSE): 0.2525\n",
      "Epoch: 0120 loss_train (RMSE): 0.0776 loss_val (RMSE): 0.2525\n",
      "Epoch: 0121 loss_train (RMSE): 0.0776 loss_val (RMSE): 0.2525\n",
      "Epoch: 0122 loss_train (RMSE): 0.0776 loss_val (RMSE): 0.2526\n",
      "Epoch: 0123 loss_train (RMSE): 0.0776 loss_val (RMSE): 0.2526\n",
      "Epoch: 0124 loss_train (RMSE): 0.0776 loss_val (RMSE): 0.2526\n",
      "Epoch: 0125 loss_train (RMSE): 0.0776 loss_val (RMSE): 0.2526\n",
      "Epoch: 0126 loss_train (RMSE): 0.0776 loss_val (RMSE): 0.2526\n",
      "Epoch: 0127 loss_train (RMSE): 0.0776 loss_val (RMSE): 0.2526\n",
      "Epoch: 0128 loss_train (RMSE): 0.0776 loss_val (RMSE): 0.2526\n",
      "Epoch: 0129 loss_train (RMSE): 0.0776 loss_val (RMSE): 0.2526\n",
      "Epoch: 0130 loss_train (RMSE): 0.0776 loss_val (RMSE): 0.2526\n",
      "Epoch: 0131 loss_train (RMSE): 0.0776 loss_val (RMSE): 0.2526\n",
      "Epoch: 0132 loss_train (RMSE): 0.0776 loss_val (RMSE): 0.2526\n",
      "Epoch: 0133 loss_train (RMSE): 0.0776 loss_val (RMSE): 0.2526\n",
      "Epoch: 0134 loss_train (RMSE): 0.0776 loss_val (RMSE): 0.2526\n",
      "Epoch: 0135 loss_train (RMSE): 0.0776 loss_val (RMSE): 0.2526\n",
      "Epoch: 0136 loss_train (RMSE): 0.0776 loss_val (RMSE): 0.2526\n",
      "Epoch: 0137 loss_train (RMSE): 0.0776 loss_val (RMSE): 0.2526\n",
      "Epoch: 0138 loss_train (RMSE): 0.0776 loss_val (RMSE): 0.2526\n",
      "Epoch: 0139 loss_train (RMSE): 0.0776 loss_val (RMSE): 0.2526\n",
      "Epoch: 0140 loss_train (RMSE): 0.0776 loss_val (RMSE): 0.2526\n",
      "Epoch: 0141 loss_train (RMSE): 0.0776 loss_val (RMSE): 0.2526\n",
      "Epoch: 0142 loss_train (RMSE): 0.0776 loss_val (RMSE): 0.2526\n",
      "Epoch: 0143 loss_train (RMSE): 0.0776 loss_val (RMSE): 0.2526\n",
      "Epoch: 0144 loss_train (RMSE): 0.0776 loss_val (RMSE): 0.2526\n",
      "Epoch: 0145 loss_train (RMSE): 0.0776 loss_val (RMSE): 0.2526\n",
      "Epoch: 0146 loss_train (RMSE): 0.0776 loss_val (RMSE): 0.2526\n",
      "Epoch: 0147 loss_train (RMSE): 0.0776 loss_val (RMSE): 0.2526\n",
      "Epoch: 0148 loss_train (RMSE): 0.0776 loss_val (RMSE): 0.2526\n",
      "Epoch: 0149 loss_train (RMSE): 0.0776 loss_val (RMSE): 0.2526\n",
      "Epoch: 0150 loss_train (RMSE): 0.0776 loss_val (RMSE): 0.2526\n",
      "Epoch: 0151 loss_train (RMSE): 0.0776 loss_val (RMSE): 0.2526\n",
      "Epoch: 0152 loss_train (RMSE): 0.0776 loss_val (RMSE): 0.2526\n",
      "Epoch: 0153 loss_train (RMSE): 0.0776 loss_val (RMSE): 0.2526\n",
      "Epoch: 0154 loss_train (RMSE): 0.0776 loss_val (RMSE): 0.2526\n",
      "Epoch: 0155 loss_train (RMSE): 0.0776 loss_val (RMSE): 0.2526\n",
      "Epoch: 0156 loss_train (RMSE): 0.0776 loss_val (RMSE): 0.2526\n",
      "Epoch: 0157 loss_train (RMSE): 0.0776 loss_val (RMSE): 0.2526\n",
      "Epoch: 0158 loss_train (RMSE): 0.0776 loss_val (RMSE): 0.2526\n",
      "Epoch: 0159 loss_train (RMSE): 0.0776 loss_val (RMSE): 0.2526\n",
      "Epoch: 0160 loss_train (RMSE): 0.0776 loss_val (RMSE): 0.2526\n",
      "Epoch: 0161 loss_train (RMSE): 0.0776 loss_val (RMSE): 0.2526\n",
      "Epoch: 0162 loss_train (RMSE): 0.0776 loss_val (RMSE): 0.2526\n",
      "Epoch: 0163 loss_train (RMSE): 0.0776 loss_val (RMSE): 0.2526\n",
      "Epoch: 0164 loss_train (RMSE): 0.0776 loss_val (RMSE): 0.2526\n",
      "Epoch: 0165 loss_train (RMSE): 0.0776 loss_val (RMSE): 0.2526\n",
      "Epoch: 0166 loss_train (RMSE): 0.0776 loss_val (RMSE): 0.2526\n",
      "Epoch: 0167 loss_train (RMSE): 0.0776 loss_val (RMSE): 0.2526\n",
      "Epoch: 0168 loss_train (RMSE): 0.0776 loss_val (RMSE): 0.2526\n",
      "Epoch: 0169 loss_train (RMSE): 0.0776 loss_val (RMSE): 0.2526\n",
      "Epoch: 0170 loss_train (RMSE): 0.0776 loss_val (RMSE): 0.2526\n",
      "Epoch: 0171 loss_train (RMSE): 0.0776 loss_val (RMSE): 0.2525\n",
      "Epoch: 0172 loss_train (RMSE): 0.0776 loss_val (RMSE): 0.2525\n",
      "Epoch: 0173 loss_train (RMSE): 0.0776 loss_val (RMSE): 0.2525\n",
      "Epoch: 0174 loss_train (RMSE): 0.0776 loss_val (RMSE): 0.2525\n",
      "Epoch: 0175 loss_train (RMSE): 0.0776 loss_val (RMSE): 0.2525\n",
      "Epoch: 0176 loss_train (RMSE): 0.0776 loss_val (RMSE): 0.2525\n",
      "Epoch: 0177 loss_train (RMSE): 0.0776 loss_val (RMSE): 0.2525\n",
      "Epoch: 0178 loss_train (RMSE): 0.0776 loss_val (RMSE): 0.2525\n",
      "Epoch: 0179 loss_train (RMSE): 0.0776 loss_val (RMSE): 0.2525\n",
      "Epoch: 0180 loss_train (RMSE): 0.0776 loss_val (RMSE): 0.2525\n",
      "Epoch: 0181 loss_train (RMSE): 0.0776 loss_val (RMSE): 0.2525\n",
      "Epoch: 0182 loss_train (RMSE): 0.0776 loss_val (RMSE): 0.2525\n",
      "Epoch: 0183 loss_train (RMSE): 0.0776 loss_val (RMSE): 0.2525\n",
      "Epoch: 0184 loss_train (RMSE): 0.0776 loss_val (RMSE): 0.2525\n",
      "Epoch: 0185 loss_train (RMSE): 0.0776 loss_val (RMSE): 0.2525\n",
      "Epoch: 0186 loss_train (RMSE): 0.0776 loss_val (RMSE): 0.2525\n",
      "Epoch: 0187 loss_train (RMSE): 0.0776 loss_val (RMSE): 0.2525\n",
      "Epoch: 0188 loss_train (RMSE): 0.0776 loss_val (RMSE): 0.2525\n",
      "Epoch: 0189 loss_train (RMSE): 0.0776 loss_val (RMSE): 0.2525\n",
      "Epoch: 0190 loss_train (RMSE): 0.0776 loss_val (RMSE): 0.2525\n",
      "Epoch: 0191 loss_train (RMSE): 0.0776 loss_val (RMSE): 0.2525\n",
      "Epoch: 0192 loss_train (RMSE): 0.0776 loss_val (RMSE): 0.2525\n",
      "Epoch: 0193 loss_train (RMSE): 0.0776 loss_val (RMSE): 0.2525\n",
      "Epoch: 0194 loss_train (RMSE): 0.0776 loss_val (RMSE): 0.2525\n",
      "Epoch: 0195 loss_train (RMSE): 0.0776 loss_val (RMSE): 0.2525\n",
      "Epoch: 0196 loss_train (RMSE): 0.0776 loss_val (RMSE): 0.2525\n",
      "Epoch: 0197 loss_train (RMSE): 0.0776 loss_val (RMSE): 0.2525\n",
      "Epoch: 0198 loss_train (RMSE): 0.0776 loss_val (RMSE): 0.2525\n",
      "Epoch: 0199 loss_train (RMSE): 0.0776 loss_val (RMSE): 0.2525\n",
      "Epoch: 0200 loss_train (RMSE): 0.0776 loss_val (RMSE): 0.2525\n",
      "Optimization Finished!\n",
      "Total time elapsed: 0.3877s\n",
      "Test set results: loss= 0.2870 (RMSE)\n",
      "GCN(\n",
      "  (gc1): GraphConvolution (10 -> 2000)\n",
      "  (gc5): GraphConvolution (2000 -> 2)\n",
      ")\n",
      "Epoch: 0001 loss_train (RMSE): 0.6384 loss_val (RMSE): 0.7310\n",
      "Epoch: 0002 loss_train (RMSE): 0.6240 loss_val (RMSE): 0.7218\n",
      "Epoch: 0003 loss_train (RMSE): 0.6099 loss_val (RMSE): 0.7127\n",
      "Epoch: 0004 loss_train (RMSE): 0.5960 loss_val (RMSE): 0.7037\n",
      "Epoch: 0005 loss_train (RMSE): 0.5823 loss_val (RMSE): 0.6946\n",
      "Epoch: 0006 loss_train (RMSE): 0.5689 loss_val (RMSE): 0.6856\n",
      "Epoch: 0007 loss_train (RMSE): 0.5556 loss_val (RMSE): 0.6767\n",
      "Epoch: 0008 loss_train (RMSE): 0.5426 loss_val (RMSE): 0.6678\n",
      "Epoch: 0009 loss_train (RMSE): 0.5298 loss_val (RMSE): 0.6589\n",
      "Epoch: 0010 loss_train (RMSE): 0.5172 loss_val (RMSE): 0.6501\n",
      "Epoch: 0011 loss_train (RMSE): 0.5048 loss_val (RMSE): 0.6414\n",
      "Epoch: 0012 loss_train (RMSE): 0.4927 loss_val (RMSE): 0.6327\n",
      "Epoch: 0013 loss_train (RMSE): 0.4808 loss_val (RMSE): 0.6241\n",
      "Epoch: 0014 loss_train (RMSE): 0.4691 loss_val (RMSE): 0.6155\n",
      "Epoch: 0015 loss_train (RMSE): 0.4577 loss_val (RMSE): 0.6071\n",
      "Epoch: 0016 loss_train (RMSE): 0.4465 loss_val (RMSE): 0.5987\n",
      "Epoch: 0017 loss_train (RMSE): 0.4355 loss_val (RMSE): 0.5904\n",
      "Epoch: 0018 loss_train (RMSE): 0.4248 loss_val (RMSE): 0.5822\n",
      "Epoch: 0019 loss_train (RMSE): 0.4143 loss_val (RMSE): 0.5740\n",
      "Epoch: 0020 loss_train (RMSE): 0.4040 loss_val (RMSE): 0.5660\n",
      "Epoch: 0021 loss_train (RMSE): 0.3940 loss_val (RMSE): 0.5581\n",
      "Epoch: 0022 loss_train (RMSE): 0.3842 loss_val (RMSE): 0.5502\n",
      "Epoch: 0023 loss_train (RMSE): 0.3746 loss_val (RMSE): 0.5425\n",
      "Epoch: 0024 loss_train (RMSE): 0.3652 loss_val (RMSE): 0.5349\n",
      "Epoch: 0025 loss_train (RMSE): 0.3561 loss_val (RMSE): 0.5273\n",
      "Epoch: 0026 loss_train (RMSE): 0.3472 loss_val (RMSE): 0.5199\n",
      "Epoch: 0027 loss_train (RMSE): 0.3385 loss_val (RMSE): 0.5126\n",
      "Epoch: 0028 loss_train (RMSE): 0.3301 loss_val (RMSE): 0.5054\n",
      "Epoch: 0029 loss_train (RMSE): 0.3219 loss_val (RMSE): 0.4984\n",
      "Epoch: 0030 loss_train (RMSE): 0.3138 loss_val (RMSE): 0.4914\n",
      "Epoch: 0031 loss_train (RMSE): 0.3060 loss_val (RMSE): 0.4846\n",
      "Epoch: 0032 loss_train (RMSE): 0.2984 loss_val (RMSE): 0.4779\n",
      "Epoch: 0033 loss_train (RMSE): 0.2910 loss_val (RMSE): 0.4713\n",
      "Epoch: 0034 loss_train (RMSE): 0.2838 loss_val (RMSE): 0.4649\n",
      "Epoch: 0035 loss_train (RMSE): 0.2768 loss_val (RMSE): 0.4586\n",
      "Epoch: 0036 loss_train (RMSE): 0.2700 loss_val (RMSE): 0.4524\n",
      "Epoch: 0037 loss_train (RMSE): 0.2634 loss_val (RMSE): 0.4464\n",
      "Epoch: 0038 loss_train (RMSE): 0.2570 loss_val (RMSE): 0.4405\n",
      "Epoch: 0039 loss_train (RMSE): 0.2508 loss_val (RMSE): 0.4347\n",
      "Epoch: 0040 loss_train (RMSE): 0.2447 loss_val (RMSE): 0.4291\n",
      "Epoch: 0041 loss_train (RMSE): 0.2388 loss_val (RMSE): 0.4236\n",
      "Epoch: 0042 loss_train (RMSE): 0.2331 loss_val (RMSE): 0.4182\n",
      "Epoch: 0043 loss_train (RMSE): 0.2275 loss_val (RMSE): 0.4130\n",
      "Epoch: 0044 loss_train (RMSE): 0.2222 loss_val (RMSE): 0.4079\n",
      "Epoch: 0045 loss_train (RMSE): 0.2169 loss_val (RMSE): 0.4030\n",
      "Epoch: 0046 loss_train (RMSE): 0.2119 loss_val (RMSE): 0.3981\n",
      "Epoch: 0047 loss_train (RMSE): 0.2069 loss_val (RMSE): 0.3935\n",
      "Epoch: 0048 loss_train (RMSE): 0.2022 loss_val (RMSE): 0.3889\n",
      "Epoch: 0049 loss_train (RMSE): 0.1975 loss_val (RMSE): 0.3846\n",
      "Epoch: 0050 loss_train (RMSE): 0.1930 loss_val (RMSE): 0.3803\n",
      "Epoch: 0051 loss_train (RMSE): 0.1887 loss_val (RMSE): 0.3762\n",
      "Epoch: 0052 loss_train (RMSE): 0.1844 loss_val (RMSE): 0.3722\n",
      "Epoch: 0053 loss_train (RMSE): 0.1803 loss_val (RMSE): 0.3683\n",
      "Epoch: 0054 loss_train (RMSE): 0.1763 loss_val (RMSE): 0.3646\n",
      "Epoch: 0055 loss_train (RMSE): 0.1725 loss_val (RMSE): 0.3610\n",
      "Epoch: 0056 loss_train (RMSE): 0.1687 loss_val (RMSE): 0.3576\n",
      "Epoch: 0057 loss_train (RMSE): 0.1651 loss_val (RMSE): 0.3543\n",
      "Epoch: 0058 loss_train (RMSE): 0.1616 loss_val (RMSE): 0.3511\n",
      "Epoch: 0059 loss_train (RMSE): 0.1582 loss_val (RMSE): 0.3480\n",
      "Epoch: 0060 loss_train (RMSE): 0.1549 loss_val (RMSE): 0.3450\n",
      "Epoch: 0061 loss_train (RMSE): 0.1516 loss_val (RMSE): 0.3422\n",
      "Epoch: 0062 loss_train (RMSE): 0.1485 loss_val (RMSE): 0.3395\n",
      "Epoch: 0063 loss_train (RMSE): 0.1455 loss_val (RMSE): 0.3369\n",
      "Epoch: 0064 loss_train (RMSE): 0.1426 loss_val (RMSE): 0.3345\n",
      "Epoch: 0065 loss_train (RMSE): 0.1397 loss_val (RMSE): 0.3321\n",
      "Epoch: 0066 loss_train (RMSE): 0.1370 loss_val (RMSE): 0.3299\n",
      "Epoch: 0067 loss_train (RMSE): 0.1343 loss_val (RMSE): 0.3277\n",
      "Epoch: 0068 loss_train (RMSE): 0.1317 loss_val (RMSE): 0.3257\n",
      "Epoch: 0069 loss_train (RMSE): 0.1292 loss_val (RMSE): 0.3238\n",
      "Epoch: 0070 loss_train (RMSE): 0.1267 loss_val (RMSE): 0.3220\n",
      "Epoch: 0071 loss_train (RMSE): 0.1244 loss_val (RMSE): 0.3203\n",
      "Epoch: 0072 loss_train (RMSE): 0.1221 loss_val (RMSE): 0.3187\n",
      "Epoch: 0073 loss_train (RMSE): 0.1198 loss_val (RMSE): 0.3171\n",
      "Epoch: 0074 loss_train (RMSE): 0.1177 loss_val (RMSE): 0.3157\n",
      "Epoch: 0075 loss_train (RMSE): 0.1156 loss_val (RMSE): 0.3144\n",
      "Epoch: 0076 loss_train (RMSE): 0.1136 loss_val (RMSE): 0.3132\n",
      "Epoch: 0077 loss_train (RMSE): 0.1116 loss_val (RMSE): 0.3120\n",
      "Epoch: 0078 loss_train (RMSE): 0.1097 loss_val (RMSE): 0.3109\n",
      "Epoch: 0079 loss_train (RMSE): 0.1079 loss_val (RMSE): 0.3100\n",
      "Epoch: 0080 loss_train (RMSE): 0.1061 loss_val (RMSE): 0.3091\n",
      "Epoch: 0081 loss_train (RMSE): 0.1044 loss_val (RMSE): 0.3082\n",
      "Epoch: 0082 loss_train (RMSE): 0.1027 loss_val (RMSE): 0.3075\n",
      "Epoch: 0083 loss_train (RMSE): 0.1011 loss_val (RMSE): 0.3068\n",
      "Epoch: 0084 loss_train (RMSE): 0.0995 loss_val (RMSE): 0.3062\n",
      "Epoch: 0085 loss_train (RMSE): 0.0980 loss_val (RMSE): 0.3056\n",
      "Epoch: 0086 loss_train (RMSE): 0.0965 loss_val (RMSE): 0.3052\n",
      "Epoch: 0087 loss_train (RMSE): 0.0951 loss_val (RMSE): 0.3047\n",
      "Epoch: 0088 loss_train (RMSE): 0.0937 loss_val (RMSE): 0.3044\n",
      "Epoch: 0089 loss_train (RMSE): 0.0924 loss_val (RMSE): 0.3041\n",
      "Epoch: 0090 loss_train (RMSE): 0.0911 loss_val (RMSE): 0.3038\n",
      "Epoch: 0091 loss_train (RMSE): 0.0899 loss_val (RMSE): 0.3036\n",
      "Epoch: 0092 loss_train (RMSE): 0.0887 loss_val (RMSE): 0.3035\n",
      "Epoch: 0093 loss_train (RMSE): 0.0875 loss_val (RMSE): 0.3034\n",
      "Epoch: 0094 loss_train (RMSE): 0.0864 loss_val (RMSE): 0.3033\n",
      "Epoch: 0095 loss_train (RMSE): 0.0853 loss_val (RMSE): 0.3033\n",
      "Epoch: 0096 loss_train (RMSE): 0.0843 loss_val (RMSE): 0.3034\n",
      "Epoch: 0097 loss_train (RMSE): 0.0833 loss_val (RMSE): 0.3034\n",
      "Epoch: 0098 loss_train (RMSE): 0.0823 loss_val (RMSE): 0.3036\n",
      "Epoch: 0099 loss_train (RMSE): 0.0814 loss_val (RMSE): 0.3037\n",
      "Epoch: 0100 loss_train (RMSE): 0.0805 loss_val (RMSE): 0.3039\n",
      "Epoch: 0101 loss_train (RMSE): 0.0796 loss_val (RMSE): 0.3041\n",
      "Epoch: 0102 loss_train (RMSE): 0.0788 loss_val (RMSE): 0.3043\n",
      "Epoch: 0103 loss_train (RMSE): 0.0780 loss_val (RMSE): 0.3046\n",
      "Epoch: 0104 loss_train (RMSE): 0.0772 loss_val (RMSE): 0.3049\n",
      "Epoch: 0105 loss_train (RMSE): 0.0764 loss_val (RMSE): 0.3052\n",
      "Epoch: 0106 loss_train (RMSE): 0.0757 loss_val (RMSE): 0.3055\n",
      "Epoch: 0107 loss_train (RMSE): 0.0750 loss_val (RMSE): 0.3059\n",
      "Epoch: 0108 loss_train (RMSE): 0.0744 loss_val (RMSE): 0.3063\n",
      "Epoch: 0109 loss_train (RMSE): 0.0737 loss_val (RMSE): 0.3067\n",
      "Epoch: 0110 loss_train (RMSE): 0.0731 loss_val (RMSE): 0.3071\n",
      "Epoch: 0111 loss_train (RMSE): 0.0725 loss_val (RMSE): 0.3075\n",
      "Epoch: 0112 loss_train (RMSE): 0.0720 loss_val (RMSE): 0.3080\n",
      "Epoch: 0113 loss_train (RMSE): 0.0714 loss_val (RMSE): 0.3084\n",
      "Epoch: 0114 loss_train (RMSE): 0.0709 loss_val (RMSE): 0.3089\n",
      "Epoch: 0115 loss_train (RMSE): 0.0704 loss_val (RMSE): 0.3094\n",
      "Epoch: 0116 loss_train (RMSE): 0.0699 loss_val (RMSE): 0.3098\n",
      "Epoch: 0117 loss_train (RMSE): 0.0694 loss_val (RMSE): 0.3103\n",
      "Epoch: 0118 loss_train (RMSE): 0.0690 loss_val (RMSE): 0.3108\n",
      "Epoch: 0119 loss_train (RMSE): 0.0686 loss_val (RMSE): 0.3113\n",
      "Epoch: 0120 loss_train (RMSE): 0.0682 loss_val (RMSE): 0.3118\n",
      "Epoch: 0121 loss_train (RMSE): 0.0678 loss_val (RMSE): 0.3124\n",
      "Epoch: 0122 loss_train (RMSE): 0.0674 loss_val (RMSE): 0.3129\n",
      "Epoch: 0123 loss_train (RMSE): 0.0670 loss_val (RMSE): 0.3134\n",
      "Epoch: 0124 loss_train (RMSE): 0.0667 loss_val (RMSE): 0.3139\n",
      "Epoch: 0125 loss_train (RMSE): 0.0664 loss_val (RMSE): 0.3144\n",
      "Epoch: 0126 loss_train (RMSE): 0.0661 loss_val (RMSE): 0.3150\n",
      "Epoch: 0127 loss_train (RMSE): 0.0658 loss_val (RMSE): 0.3155\n",
      "Epoch: 0128 loss_train (RMSE): 0.0655 loss_val (RMSE): 0.3160\n",
      "Epoch: 0129 loss_train (RMSE): 0.0652 loss_val (RMSE): 0.3165\n",
      "Epoch: 0130 loss_train (RMSE): 0.0649 loss_val (RMSE): 0.3170\n",
      "Epoch: 0131 loss_train (RMSE): 0.0647 loss_val (RMSE): 0.3176\n",
      "Epoch: 0132 loss_train (RMSE): 0.0644 loss_val (RMSE): 0.3181\n",
      "Epoch: 0133 loss_train (RMSE): 0.0642 loss_val (RMSE): 0.3186\n",
      "Epoch: 0134 loss_train (RMSE): 0.0640 loss_val (RMSE): 0.3191\n",
      "Epoch: 0135 loss_train (RMSE): 0.0638 loss_val (RMSE): 0.3196\n",
      "Epoch: 0136 loss_train (RMSE): 0.0636 loss_val (RMSE): 0.3201\n",
      "Epoch: 0137 loss_train (RMSE): 0.0634 loss_val (RMSE): 0.3206\n",
      "Epoch: 0138 loss_train (RMSE): 0.0632 loss_val (RMSE): 0.3211\n",
      "Epoch: 0139 loss_train (RMSE): 0.0630 loss_val (RMSE): 0.3216\n",
      "Epoch: 0140 loss_train (RMSE): 0.0629 loss_val (RMSE): 0.3220\n",
      "Epoch: 0141 loss_train (RMSE): 0.0627 loss_val (RMSE): 0.3225\n",
      "Epoch: 0142 loss_train (RMSE): 0.0626 loss_val (RMSE): 0.3230\n",
      "Epoch: 0143 loss_train (RMSE): 0.0624 loss_val (RMSE): 0.3234\n",
      "Epoch: 0144 loss_train (RMSE): 0.0623 loss_val (RMSE): 0.3239\n",
      "Epoch: 0145 loss_train (RMSE): 0.0622 loss_val (RMSE): 0.3244\n",
      "Epoch: 0146 loss_train (RMSE): 0.0620 loss_val (RMSE): 0.3248\n",
      "Epoch: 0147 loss_train (RMSE): 0.0619 loss_val (RMSE): 0.3252\n",
      "Epoch: 0148 loss_train (RMSE): 0.0618 loss_val (RMSE): 0.3257\n",
      "Epoch: 0149 loss_train (RMSE): 0.0617 loss_val (RMSE): 0.3261\n",
      "Epoch: 0150 loss_train (RMSE): 0.0616 loss_val (RMSE): 0.3265\n",
      "Epoch: 0151 loss_train (RMSE): 0.0615 loss_val (RMSE): 0.3269\n",
      "Epoch: 0152 loss_train (RMSE): 0.0614 loss_val (RMSE): 0.3273\n",
      "Epoch: 0153 loss_train (RMSE): 0.0614 loss_val (RMSE): 0.3277\n",
      "Epoch: 0154 loss_train (RMSE): 0.0613 loss_val (RMSE): 0.3281\n",
      "Epoch: 0155 loss_train (RMSE): 0.0612 loss_val (RMSE): 0.3285\n",
      "Epoch: 0156 loss_train (RMSE): 0.0611 loss_val (RMSE): 0.3289\n",
      "Epoch: 0157 loss_train (RMSE): 0.0611 loss_val (RMSE): 0.3293\n",
      "Epoch: 0158 loss_train (RMSE): 0.0610 loss_val (RMSE): 0.3296\n",
      "Epoch: 0159 loss_train (RMSE): 0.0609 loss_val (RMSE): 0.3300\n",
      "Epoch: 0160 loss_train (RMSE): 0.0609 loss_val (RMSE): 0.3303\n",
      "Epoch: 0161 loss_train (RMSE): 0.0608 loss_val (RMSE): 0.3307\n",
      "Epoch: 0162 loss_train (RMSE): 0.0608 loss_val (RMSE): 0.3310\n",
      "Epoch: 0163 loss_train (RMSE): 0.0607 loss_val (RMSE): 0.3313\n",
      "Epoch: 0164 loss_train (RMSE): 0.0607 loss_val (RMSE): 0.3317\n",
      "Epoch: 0165 loss_train (RMSE): 0.0606 loss_val (RMSE): 0.3320\n",
      "Epoch: 0166 loss_train (RMSE): 0.0606 loss_val (RMSE): 0.3323\n",
      "Epoch: 0167 loss_train (RMSE): 0.0605 loss_val (RMSE): 0.3326\n",
      "Epoch: 0168 loss_train (RMSE): 0.0605 loss_val (RMSE): 0.3329\n",
      "Epoch: 0169 loss_train (RMSE): 0.0605 loss_val (RMSE): 0.3332\n",
      "Epoch: 0170 loss_train (RMSE): 0.0604 loss_val (RMSE): 0.3335\n",
      "Epoch: 0171 loss_train (RMSE): 0.0604 loss_val (RMSE): 0.3338\n",
      "Epoch: 0172 loss_train (RMSE): 0.0604 loss_val (RMSE): 0.3340\n",
      "Epoch: 0173 loss_train (RMSE): 0.0603 loss_val (RMSE): 0.3343\n",
      "Epoch: 0174 loss_train (RMSE): 0.0603 loss_val (RMSE): 0.3345\n",
      "Epoch: 0175 loss_train (RMSE): 0.0603 loss_val (RMSE): 0.3348\n",
      "Epoch: 0176 loss_train (RMSE): 0.0603 loss_val (RMSE): 0.3351\n",
      "Epoch: 0177 loss_train (RMSE): 0.0603 loss_val (RMSE): 0.3353\n",
      "Epoch: 0178 loss_train (RMSE): 0.0602 loss_val (RMSE): 0.3355\n",
      "Epoch: 0179 loss_train (RMSE): 0.0602 loss_val (RMSE): 0.3358\n",
      "Epoch: 0180 loss_train (RMSE): 0.0602 loss_val (RMSE): 0.3360\n",
      "Epoch: 0181 loss_train (RMSE): 0.0602 loss_val (RMSE): 0.3362\n",
      "Epoch: 0182 loss_train (RMSE): 0.0602 loss_val (RMSE): 0.3364\n",
      "Epoch: 0183 loss_train (RMSE): 0.0601 loss_val (RMSE): 0.3366\n",
      "Epoch: 0184 loss_train (RMSE): 0.0601 loss_val (RMSE): 0.3368\n",
      "Epoch: 0185 loss_train (RMSE): 0.0601 loss_val (RMSE): 0.3370\n",
      "Epoch: 0186 loss_train (RMSE): 0.0601 loss_val (RMSE): 0.3372\n",
      "Epoch: 0187 loss_train (RMSE): 0.0601 loss_val (RMSE): 0.3374\n",
      "Epoch: 0188 loss_train (RMSE): 0.0601 loss_val (RMSE): 0.3376\n",
      "Epoch: 0189 loss_train (RMSE): 0.0601 loss_val (RMSE): 0.3378\n",
      "Epoch: 0190 loss_train (RMSE): 0.0601 loss_val (RMSE): 0.3379\n",
      "Epoch: 0191 loss_train (RMSE): 0.0601 loss_val (RMSE): 0.3381\n",
      "Epoch: 0192 loss_train (RMSE): 0.0600 loss_val (RMSE): 0.3383\n",
      "Epoch: 0193 loss_train (RMSE): 0.0600 loss_val (RMSE): 0.3384\n",
      "Epoch: 0194 loss_train (RMSE): 0.0600 loss_val (RMSE): 0.3386\n",
      "Epoch: 0195 loss_train (RMSE): 0.0600 loss_val (RMSE): 0.3387\n",
      "Epoch: 0196 loss_train (RMSE): 0.0600 loss_val (RMSE): 0.3389\n",
      "Epoch: 0197 loss_train (RMSE): 0.0600 loss_val (RMSE): 0.3390\n",
      "Epoch: 0198 loss_train (RMSE): 0.0600 loss_val (RMSE): 0.3392\n",
      "Epoch: 0199 loss_train (RMSE): 0.0600 loss_val (RMSE): 0.3393\n",
      "Epoch: 0200 loss_train (RMSE): 0.0600 loss_val (RMSE): 0.3394\n",
      "Optimization Finished!\n",
      "Total time elapsed: 0.3759s\n",
      "Test set results: loss= 0.3070 (RMSE)\n",
      "GCN(\n",
      "  (gc1): GraphConvolution (10 -> 2000)\n",
      "  (gc5): GraphConvolution (2000 -> 2)\n",
      ")\n",
      "Epoch: 0001 loss_train (RMSE): 0.3942 loss_val (RMSE): 0.5675\n",
      "Epoch: 0002 loss_train (RMSE): 0.3840 loss_val (RMSE): 0.5592\n",
      "Epoch: 0003 loss_train (RMSE): 0.3739 loss_val (RMSE): 0.5511\n",
      "Epoch: 0004 loss_train (RMSE): 0.3641 loss_val (RMSE): 0.5430\n",
      "Epoch: 0005 loss_train (RMSE): 0.3546 loss_val (RMSE): 0.5350\n",
      "Epoch: 0006 loss_train (RMSE): 0.3452 loss_val (RMSE): 0.5271\n",
      "Epoch: 0007 loss_train (RMSE): 0.3360 loss_val (RMSE): 0.5192\n",
      "Epoch: 0008 loss_train (RMSE): 0.3271 loss_val (RMSE): 0.5115\n",
      "Epoch: 0009 loss_train (RMSE): 0.3184 loss_val (RMSE): 0.5039\n",
      "Epoch: 0010 loss_train (RMSE): 0.3099 loss_val (RMSE): 0.4964\n",
      "Epoch: 0011 loss_train (RMSE): 0.3016 loss_val (RMSE): 0.4889\n",
      "Epoch: 0012 loss_train (RMSE): 0.2935 loss_val (RMSE): 0.4817\n",
      "Epoch: 0013 loss_train (RMSE): 0.2857 loss_val (RMSE): 0.4745\n",
      "Epoch: 0014 loss_train (RMSE): 0.2781 loss_val (RMSE): 0.4675\n",
      "Epoch: 0015 loss_train (RMSE): 0.2707 loss_val (RMSE): 0.4606\n",
      "Epoch: 0016 loss_train (RMSE): 0.2635 loss_val (RMSE): 0.4538\n",
      "Epoch: 0017 loss_train (RMSE): 0.2566 loss_val (RMSE): 0.4472\n",
      "Epoch: 0018 loss_train (RMSE): 0.2498 loss_val (RMSE): 0.4407\n",
      "Epoch: 0019 loss_train (RMSE): 0.2433 loss_val (RMSE): 0.4344\n",
      "Epoch: 0020 loss_train (RMSE): 0.2370 loss_val (RMSE): 0.4282\n",
      "Epoch: 0021 loss_train (RMSE): 0.2308 loss_val (RMSE): 0.4222\n",
      "Epoch: 0022 loss_train (RMSE): 0.2249 loss_val (RMSE): 0.4163\n",
      "Epoch: 0023 loss_train (RMSE): 0.2191 loss_val (RMSE): 0.4105\n",
      "Epoch: 0024 loss_train (RMSE): 0.2136 loss_val (RMSE): 0.4050\n",
      "Epoch: 0025 loss_train (RMSE): 0.2082 loss_val (RMSE): 0.3995\n",
      "Epoch: 0026 loss_train (RMSE): 0.2030 loss_val (RMSE): 0.3942\n",
      "Epoch: 0027 loss_train (RMSE): 0.1980 loss_val (RMSE): 0.3891\n",
      "Epoch: 0028 loss_train (RMSE): 0.1931 loss_val (RMSE): 0.3841\n",
      "Epoch: 0029 loss_train (RMSE): 0.1884 loss_val (RMSE): 0.3792\n",
      "Epoch: 0030 loss_train (RMSE): 0.1838 loss_val (RMSE): 0.3745\n",
      "Epoch: 0031 loss_train (RMSE): 0.1793 loss_val (RMSE): 0.3699\n",
      "Epoch: 0032 loss_train (RMSE): 0.1750 loss_val (RMSE): 0.3655\n",
      "Epoch: 0033 loss_train (RMSE): 0.1709 loss_val (RMSE): 0.3612\n",
      "Epoch: 0034 loss_train (RMSE): 0.1668 loss_val (RMSE): 0.3570\n",
      "Epoch: 0035 loss_train (RMSE): 0.1629 loss_val (RMSE): 0.3529\n",
      "Epoch: 0036 loss_train (RMSE): 0.1591 loss_val (RMSE): 0.3490\n",
      "Epoch: 0037 loss_train (RMSE): 0.1554 loss_val (RMSE): 0.3452\n",
      "Epoch: 0038 loss_train (RMSE): 0.1518 loss_val (RMSE): 0.3415\n",
      "Epoch: 0039 loss_train (RMSE): 0.1484 loss_val (RMSE): 0.3379\n",
      "Epoch: 0040 loss_train (RMSE): 0.1450 loss_val (RMSE): 0.3344\n",
      "Epoch: 0041 loss_train (RMSE): 0.1417 loss_val (RMSE): 0.3311\n",
      "Epoch: 0042 loss_train (RMSE): 0.1385 loss_val (RMSE): 0.3278\n",
      "Epoch: 0043 loss_train (RMSE): 0.1354 loss_val (RMSE): 0.3247\n",
      "Epoch: 0044 loss_train (RMSE): 0.1324 loss_val (RMSE): 0.3217\n",
      "Epoch: 0045 loss_train (RMSE): 0.1295 loss_val (RMSE): 0.3188\n",
      "Epoch: 0046 loss_train (RMSE): 0.1267 loss_val (RMSE): 0.3160\n",
      "Epoch: 0047 loss_train (RMSE): 0.1240 loss_val (RMSE): 0.3133\n",
      "Epoch: 0048 loss_train (RMSE): 0.1213 loss_val (RMSE): 0.3107\n",
      "Epoch: 0049 loss_train (RMSE): 0.1188 loss_val (RMSE): 0.3082\n",
      "Epoch: 0050 loss_train (RMSE): 0.1163 loss_val (RMSE): 0.3058\n",
      "Epoch: 0051 loss_train (RMSE): 0.1139 loss_val (RMSE): 0.3036\n",
      "Epoch: 0052 loss_train (RMSE): 0.1116 loss_val (RMSE): 0.3014\n",
      "Epoch: 0053 loss_train (RMSE): 0.1093 loss_val (RMSE): 0.2993\n",
      "Epoch: 0054 loss_train (RMSE): 0.1071 loss_val (RMSE): 0.2974\n",
      "Epoch: 0055 loss_train (RMSE): 0.1051 loss_val (RMSE): 0.2955\n",
      "Epoch: 0056 loss_train (RMSE): 0.1030 loss_val (RMSE): 0.2937\n",
      "Epoch: 0057 loss_train (RMSE): 0.1011 loss_val (RMSE): 0.2921\n",
      "Epoch: 0058 loss_train (RMSE): 0.0992 loss_val (RMSE): 0.2905\n",
      "Epoch: 0059 loss_train (RMSE): 0.0974 loss_val (RMSE): 0.2890\n",
      "Epoch: 0060 loss_train (RMSE): 0.0957 loss_val (RMSE): 0.2876\n",
      "Epoch: 0061 loss_train (RMSE): 0.0940 loss_val (RMSE): 0.2862\n",
      "Epoch: 0062 loss_train (RMSE): 0.0924 loss_val (RMSE): 0.2850\n",
      "Epoch: 0063 loss_train (RMSE): 0.0909 loss_val (RMSE): 0.2838\n",
      "Epoch: 0064 loss_train (RMSE): 0.0894 loss_val (RMSE): 0.2828\n",
      "Epoch: 0065 loss_train (RMSE): 0.0880 loss_val (RMSE): 0.2817\n",
      "Epoch: 0066 loss_train (RMSE): 0.0867 loss_val (RMSE): 0.2808\n",
      "Epoch: 0067 loss_train (RMSE): 0.0854 loss_val (RMSE): 0.2799\n",
      "Epoch: 0068 loss_train (RMSE): 0.0841 loss_val (RMSE): 0.2791\n",
      "Epoch: 0069 loss_train (RMSE): 0.0829 loss_val (RMSE): 0.2784\n",
      "Epoch: 0070 loss_train (RMSE): 0.0818 loss_val (RMSE): 0.2777\n",
      "Epoch: 0071 loss_train (RMSE): 0.0807 loss_val (RMSE): 0.2770\n",
      "Epoch: 0072 loss_train (RMSE): 0.0797 loss_val (RMSE): 0.2765\n",
      "Epoch: 0073 loss_train (RMSE): 0.0787 loss_val (RMSE): 0.2759\n",
      "Epoch: 0074 loss_train (RMSE): 0.0777 loss_val (RMSE): 0.2754\n",
      "Epoch: 0075 loss_train (RMSE): 0.0768 loss_val (RMSE): 0.2750\n",
      "Epoch: 0076 loss_train (RMSE): 0.0759 loss_val (RMSE): 0.2746\n",
      "Epoch: 0077 loss_train (RMSE): 0.0751 loss_val (RMSE): 0.2743\n",
      "Epoch: 0078 loss_train (RMSE): 0.0743 loss_val (RMSE): 0.2740\n",
      "Epoch: 0079 loss_train (RMSE): 0.0736 loss_val (RMSE): 0.2737\n",
      "Epoch: 0080 loss_train (RMSE): 0.0728 loss_val (RMSE): 0.2735\n",
      "Epoch: 0081 loss_train (RMSE): 0.0721 loss_val (RMSE): 0.2733\n",
      "Epoch: 0082 loss_train (RMSE): 0.0715 loss_val (RMSE): 0.2731\n",
      "Epoch: 0083 loss_train (RMSE): 0.0709 loss_val (RMSE): 0.2730\n",
      "Epoch: 0084 loss_train (RMSE): 0.0703 loss_val (RMSE): 0.2729\n",
      "Epoch: 0085 loss_train (RMSE): 0.0697 loss_val (RMSE): 0.2728\n",
      "Epoch: 0086 loss_train (RMSE): 0.0692 loss_val (RMSE): 0.2727\n",
      "Epoch: 0087 loss_train (RMSE): 0.0687 loss_val (RMSE): 0.2727\n",
      "Epoch: 0088 loss_train (RMSE): 0.0682 loss_val (RMSE): 0.2727\n",
      "Epoch: 0089 loss_train (RMSE): 0.0677 loss_val (RMSE): 0.2727\n",
      "Epoch: 0090 loss_train (RMSE): 0.0673 loss_val (RMSE): 0.2728\n",
      "Epoch: 0091 loss_train (RMSE): 0.0669 loss_val (RMSE): 0.2729\n",
      "Epoch: 0092 loss_train (RMSE): 0.0665 loss_val (RMSE): 0.2729\n",
      "Epoch: 0093 loss_train (RMSE): 0.0661 loss_val (RMSE): 0.2730\n",
      "Epoch: 0094 loss_train (RMSE): 0.0658 loss_val (RMSE): 0.2732\n",
      "Epoch: 0095 loss_train (RMSE): 0.0654 loss_val (RMSE): 0.2733\n",
      "Epoch: 0096 loss_train (RMSE): 0.0651 loss_val (RMSE): 0.2735\n",
      "Epoch: 0097 loss_train (RMSE): 0.0648 loss_val (RMSE): 0.2736\n",
      "Epoch: 0098 loss_train (RMSE): 0.0646 loss_val (RMSE): 0.2738\n",
      "Epoch: 0099 loss_train (RMSE): 0.0643 loss_val (RMSE): 0.2740\n",
      "Epoch: 0100 loss_train (RMSE): 0.0640 loss_val (RMSE): 0.2742\n",
      "Epoch: 0101 loss_train (RMSE): 0.0638 loss_val (RMSE): 0.2744\n",
      "Epoch: 0102 loss_train (RMSE): 0.0636 loss_val (RMSE): 0.2746\n",
      "Epoch: 0103 loss_train (RMSE): 0.0634 loss_val (RMSE): 0.2749\n",
      "Epoch: 0104 loss_train (RMSE): 0.0632 loss_val (RMSE): 0.2751\n",
      "Epoch: 0105 loss_train (RMSE): 0.0630 loss_val (RMSE): 0.2754\n",
      "Epoch: 0106 loss_train (RMSE): 0.0628 loss_val (RMSE): 0.2756\n",
      "Epoch: 0107 loss_train (RMSE): 0.0627 loss_val (RMSE): 0.2759\n",
      "Epoch: 0108 loss_train (RMSE): 0.0625 loss_val (RMSE): 0.2761\n",
      "Epoch: 0109 loss_train (RMSE): 0.0624 loss_val (RMSE): 0.2764\n",
      "Epoch: 0110 loss_train (RMSE): 0.0623 loss_val (RMSE): 0.2766\n",
      "Epoch: 0111 loss_train (RMSE): 0.0621 loss_val (RMSE): 0.2769\n",
      "Epoch: 0112 loss_train (RMSE): 0.0620 loss_val (RMSE): 0.2771\n",
      "Epoch: 0113 loss_train (RMSE): 0.0619 loss_val (RMSE): 0.2774\n",
      "Epoch: 0114 loss_train (RMSE): 0.0618 loss_val (RMSE): 0.2777\n",
      "Epoch: 0115 loss_train (RMSE): 0.0617 loss_val (RMSE): 0.2779\n",
      "Epoch: 0116 loss_train (RMSE): 0.0616 loss_val (RMSE): 0.2782\n",
      "Epoch: 0117 loss_train (RMSE): 0.0615 loss_val (RMSE): 0.2785\n",
      "Epoch: 0118 loss_train (RMSE): 0.0615 loss_val (RMSE): 0.2787\n",
      "Epoch: 0119 loss_train (RMSE): 0.0614 loss_val (RMSE): 0.2790\n",
      "Epoch: 0120 loss_train (RMSE): 0.0613 loss_val (RMSE): 0.2792\n",
      "Epoch: 0121 loss_train (RMSE): 0.0613 loss_val (RMSE): 0.2795\n",
      "Epoch: 0122 loss_train (RMSE): 0.0612 loss_val (RMSE): 0.2797\n",
      "Epoch: 0123 loss_train (RMSE): 0.0611 loss_val (RMSE): 0.2799\n",
      "Epoch: 0124 loss_train (RMSE): 0.0611 loss_val (RMSE): 0.2802\n",
      "Epoch: 0125 loss_train (RMSE): 0.0610 loss_val (RMSE): 0.2804\n",
      "Epoch: 0126 loss_train (RMSE): 0.0610 loss_val (RMSE): 0.2806\n",
      "Epoch: 0127 loss_train (RMSE): 0.0610 loss_val (RMSE): 0.2809\n",
      "Epoch: 0128 loss_train (RMSE): 0.0609 loss_val (RMSE): 0.2811\n",
      "Epoch: 0129 loss_train (RMSE): 0.0609 loss_val (RMSE): 0.2813\n",
      "Epoch: 0130 loss_train (RMSE): 0.0609 loss_val (RMSE): 0.2815\n",
      "Epoch: 0131 loss_train (RMSE): 0.0608 loss_val (RMSE): 0.2817\n",
      "Epoch: 0132 loss_train (RMSE): 0.0608 loss_val (RMSE): 0.2819\n",
      "Epoch: 0133 loss_train (RMSE): 0.0608 loss_val (RMSE): 0.2821\n",
      "Epoch: 0134 loss_train (RMSE): 0.0608 loss_val (RMSE): 0.2822\n",
      "Epoch: 0135 loss_train (RMSE): 0.0607 loss_val (RMSE): 0.2824\n",
      "Epoch: 0136 loss_train (RMSE): 0.0607 loss_val (RMSE): 0.2826\n",
      "Epoch: 0137 loss_train (RMSE): 0.0607 loss_val (RMSE): 0.2828\n",
      "Epoch: 0138 loss_train (RMSE): 0.0607 loss_val (RMSE): 0.2829\n",
      "Epoch: 0139 loss_train (RMSE): 0.0607 loss_val (RMSE): 0.2831\n",
      "Epoch: 0140 loss_train (RMSE): 0.0606 loss_val (RMSE): 0.2832\n",
      "Epoch: 0141 loss_train (RMSE): 0.0606 loss_val (RMSE): 0.2834\n",
      "Epoch: 0142 loss_train (RMSE): 0.0606 loss_val (RMSE): 0.2835\n",
      "Epoch: 0143 loss_train (RMSE): 0.0606 loss_val (RMSE): 0.2837\n",
      "Epoch: 0144 loss_train (RMSE): 0.0606 loss_val (RMSE): 0.2838\n",
      "Epoch: 0145 loss_train (RMSE): 0.0606 loss_val (RMSE): 0.2839\n",
      "Epoch: 0146 loss_train (RMSE): 0.0606 loss_val (RMSE): 0.2840\n",
      "Epoch: 0147 loss_train (RMSE): 0.0606 loss_val (RMSE): 0.2842\n",
      "Epoch: 0148 loss_train (RMSE): 0.0606 loss_val (RMSE): 0.2843\n",
      "Epoch: 0149 loss_train (RMSE): 0.0606 loss_val (RMSE): 0.2844\n",
      "Epoch: 0150 loss_train (RMSE): 0.0606 loss_val (RMSE): 0.2845\n",
      "Epoch: 0151 loss_train (RMSE): 0.0606 loss_val (RMSE): 0.2846\n",
      "Epoch: 0152 loss_train (RMSE): 0.0605 loss_val (RMSE): 0.2847\n",
      "Epoch: 0153 loss_train (RMSE): 0.0605 loss_val (RMSE): 0.2848\n",
      "Epoch: 0154 loss_train (RMSE): 0.0605 loss_val (RMSE): 0.2849\n",
      "Epoch: 0155 loss_train (RMSE): 0.0605 loss_val (RMSE): 0.2850\n",
      "Epoch: 0156 loss_train (RMSE): 0.0605 loss_val (RMSE): 0.2850\n",
      "Epoch: 0157 loss_train (RMSE): 0.0605 loss_val (RMSE): 0.2851\n",
      "Epoch: 0158 loss_train (RMSE): 0.0605 loss_val (RMSE): 0.2852\n",
      "Epoch: 0159 loss_train (RMSE): 0.0605 loss_val (RMSE): 0.2853\n",
      "Epoch: 0160 loss_train (RMSE): 0.0605 loss_val (RMSE): 0.2853\n",
      "Epoch: 0161 loss_train (RMSE): 0.0605 loss_val (RMSE): 0.2854\n",
      "Epoch: 0162 loss_train (RMSE): 0.0605 loss_val (RMSE): 0.2855\n",
      "Epoch: 0163 loss_train (RMSE): 0.0605 loss_val (RMSE): 0.2855\n",
      "Epoch: 0164 loss_train (RMSE): 0.0605 loss_val (RMSE): 0.2856\n",
      "Epoch: 0165 loss_train (RMSE): 0.0605 loss_val (RMSE): 0.2856\n",
      "Epoch: 0166 loss_train (RMSE): 0.0605 loss_val (RMSE): 0.2857\n",
      "Epoch: 0167 loss_train (RMSE): 0.0605 loss_val (RMSE): 0.2857\n",
      "Epoch: 0168 loss_train (RMSE): 0.0605 loss_val (RMSE): 0.2858\n",
      "Epoch: 0169 loss_train (RMSE): 0.0605 loss_val (RMSE): 0.2858\n",
      "Epoch: 0170 loss_train (RMSE): 0.0605 loss_val (RMSE): 0.2859\n",
      "Epoch: 0171 loss_train (RMSE): 0.0605 loss_val (RMSE): 0.2859\n",
      "Epoch: 0172 loss_train (RMSE): 0.0605 loss_val (RMSE): 0.2860\n",
      "Epoch: 0173 loss_train (RMSE): 0.0605 loss_val (RMSE): 0.2860\n",
      "Epoch: 0174 loss_train (RMSE): 0.0605 loss_val (RMSE): 0.2860\n",
      "Epoch: 0175 loss_train (RMSE): 0.0605 loss_val (RMSE): 0.2861\n",
      "Epoch: 0176 loss_train (RMSE): 0.0605 loss_val (RMSE): 0.2861\n",
      "Epoch: 0177 loss_train (RMSE): 0.0605 loss_val (RMSE): 0.2861\n",
      "Epoch: 0178 loss_train (RMSE): 0.0605 loss_val (RMSE): 0.2862\n",
      "Epoch: 0179 loss_train (RMSE): 0.0605 loss_val (RMSE): 0.2862\n",
      "Epoch: 0180 loss_train (RMSE): 0.0605 loss_val (RMSE): 0.2862\n",
      "Epoch: 0181 loss_train (RMSE): 0.0605 loss_val (RMSE): 0.2862\n",
      "Epoch: 0182 loss_train (RMSE): 0.0605 loss_val (RMSE): 0.2863\n",
      "Epoch: 0183 loss_train (RMSE): 0.0605 loss_val (RMSE): 0.2863\n",
      "Epoch: 0184 loss_train (RMSE): 0.0605 loss_val (RMSE): 0.2863\n",
      "Epoch: 0185 loss_train (RMSE): 0.0605 loss_val (RMSE): 0.2863\n",
      "Epoch: 0186 loss_train (RMSE): 0.0605 loss_val (RMSE): 0.2863\n",
      "Epoch: 0187 loss_train (RMSE): 0.0605 loss_val (RMSE): 0.2863\n",
      "Epoch: 0188 loss_train (RMSE): 0.0605 loss_val (RMSE): 0.2864\n",
      "Epoch: 0189 loss_train (RMSE): 0.0605 loss_val (RMSE): 0.2864\n",
      "Epoch: 0190 loss_train (RMSE): 0.0605 loss_val (RMSE): 0.2864\n",
      "Epoch: 0191 loss_train (RMSE): 0.0605 loss_val (RMSE): 0.2864\n",
      "Epoch: 0192 loss_train (RMSE): 0.0605 loss_val (RMSE): 0.2864\n",
      "Epoch: 0193 loss_train (RMSE): 0.0605 loss_val (RMSE): 0.2864\n",
      "Epoch: 0194 loss_train (RMSE): 0.0605 loss_val (RMSE): 0.2864\n",
      "Epoch: 0195 loss_train (RMSE): 0.0605 loss_val (RMSE): 0.2864\n",
      "Epoch: 0196 loss_train (RMSE): 0.0605 loss_val (RMSE): 0.2865\n",
      "Epoch: 0197 loss_train (RMSE): 0.0605 loss_val (RMSE): 0.2865\n",
      "Epoch: 0198 loss_train (RMSE): 0.0605 loss_val (RMSE): 0.2865\n",
      "Epoch: 0199 loss_train (RMSE): 0.0605 loss_val (RMSE): 0.2865\n",
      "Epoch: 0200 loss_train (RMSE): 0.0605 loss_val (RMSE): 0.2865\n",
      "Optimization Finished!\n",
      "Total time elapsed: 0.3809s\n",
      "Test set results: loss= 0.2790 (RMSE)\n",
      "GCN(\n",
      "  (gc1): GraphConvolution (10 -> 2000)\n",
      "  (gc5): GraphConvolution (2000 -> 2)\n",
      ")\n",
      "Epoch: 0001 loss_train (RMSE): 0.8574 loss_val (RMSE): 0.8950\n",
      "Epoch: 0002 loss_train (RMSE): 0.8418 loss_val (RMSE): 0.8863\n",
      "Epoch: 0003 loss_train (RMSE): 0.8265 loss_val (RMSE): 0.8775\n",
      "Epoch: 0004 loss_train (RMSE): 0.8114 loss_val (RMSE): 0.8688\n",
      "Epoch: 0005 loss_train (RMSE): 0.7965 loss_val (RMSE): 0.8602\n",
      "Epoch: 0006 loss_train (RMSE): 0.7818 loss_val (RMSE): 0.8516\n",
      "Epoch: 0007 loss_train (RMSE): 0.7673 loss_val (RMSE): 0.8430\n",
      "Epoch: 0008 loss_train (RMSE): 0.7531 loss_val (RMSE): 0.8345\n",
      "Epoch: 0009 loss_train (RMSE): 0.7391 loss_val (RMSE): 0.8260\n",
      "Epoch: 0010 loss_train (RMSE): 0.7253 loss_val (RMSE): 0.8176\n",
      "Epoch: 0011 loss_train (RMSE): 0.7117 loss_val (RMSE): 0.8092\n",
      "Epoch: 0012 loss_train (RMSE): 0.6983 loss_val (RMSE): 0.8009\n",
      "Epoch: 0013 loss_train (RMSE): 0.6852 loss_val (RMSE): 0.7927\n",
      "Epoch: 0014 loss_train (RMSE): 0.6723 loss_val (RMSE): 0.7845\n",
      "Epoch: 0015 loss_train (RMSE): 0.6597 loss_val (RMSE): 0.7764\n",
      "Epoch: 0016 loss_train (RMSE): 0.6473 loss_val (RMSE): 0.7684\n",
      "Epoch: 0017 loss_train (RMSE): 0.6351 loss_val (RMSE): 0.7605\n",
      "Epoch: 0018 loss_train (RMSE): 0.6231 loss_val (RMSE): 0.7526\n",
      "Epoch: 0019 loss_train (RMSE): 0.6113 loss_val (RMSE): 0.7448\n",
      "Epoch: 0020 loss_train (RMSE): 0.5998 loss_val (RMSE): 0.7370\n",
      "Epoch: 0021 loss_train (RMSE): 0.5885 loss_val (RMSE): 0.7294\n",
      "Epoch: 0022 loss_train (RMSE): 0.5775 loss_val (RMSE): 0.7218\n",
      "Epoch: 0023 loss_train (RMSE): 0.5666 loss_val (RMSE): 0.7144\n",
      "Epoch: 0024 loss_train (RMSE): 0.5560 loss_val (RMSE): 0.7070\n",
      "Epoch: 0025 loss_train (RMSE): 0.5456 loss_val (RMSE): 0.6997\n",
      "Epoch: 0026 loss_train (RMSE): 0.5354 loss_val (RMSE): 0.6925\n",
      "Epoch: 0027 loss_train (RMSE): 0.5254 loss_val (RMSE): 0.6853\n",
      "Epoch: 0028 loss_train (RMSE): 0.5156 loss_val (RMSE): 0.6783\n",
      "Epoch: 0029 loss_train (RMSE): 0.5061 loss_val (RMSE): 0.6714\n",
      "Epoch: 0030 loss_train (RMSE): 0.4967 loss_val (RMSE): 0.6645\n",
      "Epoch: 0031 loss_train (RMSE): 0.4875 loss_val (RMSE): 0.6577\n",
      "Epoch: 0032 loss_train (RMSE): 0.4785 loss_val (RMSE): 0.6511\n",
      "Epoch: 0033 loss_train (RMSE): 0.4697 loss_val (RMSE): 0.6445\n",
      "Epoch: 0034 loss_train (RMSE): 0.4611 loss_val (RMSE): 0.6380\n",
      "Epoch: 0035 loss_train (RMSE): 0.4527 loss_val (RMSE): 0.6316\n",
      "Epoch: 0036 loss_train (RMSE): 0.4444 loss_val (RMSE): 0.6253\n",
      "Epoch: 0037 loss_train (RMSE): 0.4363 loss_val (RMSE): 0.6190\n",
      "Epoch: 0038 loss_train (RMSE): 0.4284 loss_val (RMSE): 0.6129\n",
      "Epoch: 0039 loss_train (RMSE): 0.4207 loss_val (RMSE): 0.6068\n",
      "Epoch: 0040 loss_train (RMSE): 0.4131 loss_val (RMSE): 0.6009\n",
      "Epoch: 0041 loss_train (RMSE): 0.4056 loss_val (RMSE): 0.5950\n",
      "Epoch: 0042 loss_train (RMSE): 0.3983 loss_val (RMSE): 0.5892\n",
      "Epoch: 0043 loss_train (RMSE): 0.3911 loss_val (RMSE): 0.5835\n",
      "Epoch: 0044 loss_train (RMSE): 0.3841 loss_val (RMSE): 0.5779\n",
      "Epoch: 0045 loss_train (RMSE): 0.3773 loss_val (RMSE): 0.5723\n",
      "Epoch: 0046 loss_train (RMSE): 0.3705 loss_val (RMSE): 0.5668\n",
      "Epoch: 0047 loss_train (RMSE): 0.3639 loss_val (RMSE): 0.5614\n",
      "Epoch: 0048 loss_train (RMSE): 0.3574 loss_val (RMSE): 0.5561\n",
      "Epoch: 0049 loss_train (RMSE): 0.3511 loss_val (RMSE): 0.5509\n",
      "Epoch: 0050 loss_train (RMSE): 0.3448 loss_val (RMSE): 0.5457\n",
      "Epoch: 0051 loss_train (RMSE): 0.3387 loss_val (RMSE): 0.5407\n",
      "Epoch: 0052 loss_train (RMSE): 0.3327 loss_val (RMSE): 0.5357\n",
      "Epoch: 0053 loss_train (RMSE): 0.3268 loss_val (RMSE): 0.5307\n",
      "Epoch: 0054 loss_train (RMSE): 0.3210 loss_val (RMSE): 0.5259\n",
      "Epoch: 0055 loss_train (RMSE): 0.3154 loss_val (RMSE): 0.5211\n",
      "Epoch: 0056 loss_train (RMSE): 0.3098 loss_val (RMSE): 0.5164\n",
      "Epoch: 0057 loss_train (RMSE): 0.3044 loss_val (RMSE): 0.5117\n",
      "Epoch: 0058 loss_train (RMSE): 0.2990 loss_val (RMSE): 0.5072\n",
      "Epoch: 0059 loss_train (RMSE): 0.2937 loss_val (RMSE): 0.5026\n",
      "Epoch: 0060 loss_train (RMSE): 0.2886 loss_val (RMSE): 0.4982\n",
      "Epoch: 0061 loss_train (RMSE): 0.2835 loss_val (RMSE): 0.4938\n",
      "Epoch: 0062 loss_train (RMSE): 0.2786 loss_val (RMSE): 0.4896\n",
      "Epoch: 0063 loss_train (RMSE): 0.2737 loss_val (RMSE): 0.4853\n",
      "Epoch: 0064 loss_train (RMSE): 0.2689 loss_val (RMSE): 0.4812\n",
      "Epoch: 0065 loss_train (RMSE): 0.2642 loss_val (RMSE): 0.4771\n",
      "Epoch: 0066 loss_train (RMSE): 0.2597 loss_val (RMSE): 0.4730\n",
      "Epoch: 0067 loss_train (RMSE): 0.2552 loss_val (RMSE): 0.4691\n",
      "Epoch: 0068 loss_train (RMSE): 0.2507 loss_val (RMSE): 0.4652\n",
      "Epoch: 0069 loss_train (RMSE): 0.2464 loss_val (RMSE): 0.4614\n",
      "Epoch: 0070 loss_train (RMSE): 0.2422 loss_val (RMSE): 0.4576\n",
      "Epoch: 0071 loss_train (RMSE): 0.2380 loss_val (RMSE): 0.4539\n",
      "Epoch: 0072 loss_train (RMSE): 0.2340 loss_val (RMSE): 0.4502\n",
      "Epoch: 0073 loss_train (RMSE): 0.2300 loss_val (RMSE): 0.4467\n",
      "Epoch: 0074 loss_train (RMSE): 0.2261 loss_val (RMSE): 0.4431\n",
      "Epoch: 0075 loss_train (RMSE): 0.2222 loss_val (RMSE): 0.4397\n",
      "Epoch: 0076 loss_train (RMSE): 0.2185 loss_val (RMSE): 0.4363\n",
      "Epoch: 0077 loss_train (RMSE): 0.2148 loss_val (RMSE): 0.4330\n",
      "Epoch: 0078 loss_train (RMSE): 0.2112 loss_val (RMSE): 0.4297\n",
      "Epoch: 0079 loss_train (RMSE): 0.2077 loss_val (RMSE): 0.4265\n",
      "Epoch: 0080 loss_train (RMSE): 0.2043 loss_val (RMSE): 0.4233\n",
      "Epoch: 0081 loss_train (RMSE): 0.2009 loss_val (RMSE): 0.4202\n",
      "Epoch: 0082 loss_train (RMSE): 0.1977 loss_val (RMSE): 0.4172\n",
      "Epoch: 0083 loss_train (RMSE): 0.1944 loss_val (RMSE): 0.4142\n",
      "Epoch: 0084 loss_train (RMSE): 0.1913 loss_val (RMSE): 0.4113\n",
      "Epoch: 0085 loss_train (RMSE): 0.1882 loss_val (RMSE): 0.4085\n",
      "Epoch: 0086 loss_train (RMSE): 0.1852 loss_val (RMSE): 0.4056\n",
      "Epoch: 0087 loss_train (RMSE): 0.1823 loss_val (RMSE): 0.4029\n",
      "Epoch: 0088 loss_train (RMSE): 0.1794 loss_val (RMSE): 0.4002\n",
      "Epoch: 0089 loss_train (RMSE): 0.1766 loss_val (RMSE): 0.3976\n",
      "Epoch: 0090 loss_train (RMSE): 0.1739 loss_val (RMSE): 0.3950\n",
      "Epoch: 0091 loss_train (RMSE): 0.1712 loss_val (RMSE): 0.3924\n",
      "Epoch: 0092 loss_train (RMSE): 0.1686 loss_val (RMSE): 0.3899\n",
      "Epoch: 0093 loss_train (RMSE): 0.1660 loss_val (RMSE): 0.3875\n",
      "Epoch: 0094 loss_train (RMSE): 0.1635 loss_val (RMSE): 0.3851\n",
      "Epoch: 0095 loss_train (RMSE): 0.1611 loss_val (RMSE): 0.3828\n",
      "Epoch: 0096 loss_train (RMSE): 0.1587 loss_val (RMSE): 0.3805\n",
      "Epoch: 0097 loss_train (RMSE): 0.1564 loss_val (RMSE): 0.3783\n",
      "Epoch: 0098 loss_train (RMSE): 0.1541 loss_val (RMSE): 0.3761\n",
      "Epoch: 0099 loss_train (RMSE): 0.1519 loss_val (RMSE): 0.3740\n",
      "Epoch: 0100 loss_train (RMSE): 0.1498 loss_val (RMSE): 0.3719\n",
      "Epoch: 0101 loss_train (RMSE): 0.1477 loss_val (RMSE): 0.3699\n",
      "Epoch: 0102 loss_train (RMSE): 0.1456 loss_val (RMSE): 0.3679\n",
      "Epoch: 0103 loss_train (RMSE): 0.1436 loss_val (RMSE): 0.3659\n",
      "Epoch: 0104 loss_train (RMSE): 0.1417 loss_val (RMSE): 0.3640\n",
      "Epoch: 0105 loss_train (RMSE): 0.1398 loss_val (RMSE): 0.3622\n",
      "Epoch: 0106 loss_train (RMSE): 0.1379 loss_val (RMSE): 0.3604\n",
      "Epoch: 0107 loss_train (RMSE): 0.1361 loss_val (RMSE): 0.3586\n",
      "Epoch: 0108 loss_train (RMSE): 0.1344 loss_val (RMSE): 0.3569\n",
      "Epoch: 0109 loss_train (RMSE): 0.1327 loss_val (RMSE): 0.3552\n",
      "Epoch: 0110 loss_train (RMSE): 0.1310 loss_val (RMSE): 0.3536\n",
      "Epoch: 0111 loss_train (RMSE): 0.1294 loss_val (RMSE): 0.3520\n",
      "Epoch: 0112 loss_train (RMSE): 0.1278 loss_val (RMSE): 0.3505\n",
      "Epoch: 0113 loss_train (RMSE): 0.1262 loss_val (RMSE): 0.3490\n",
      "Epoch: 0114 loss_train (RMSE): 0.1247 loss_val (RMSE): 0.3475\n",
      "Epoch: 0115 loss_train (RMSE): 0.1233 loss_val (RMSE): 0.3461\n",
      "Epoch: 0116 loss_train (RMSE): 0.1219 loss_val (RMSE): 0.3448\n",
      "Epoch: 0117 loss_train (RMSE): 0.1205 loss_val (RMSE): 0.3434\n",
      "Epoch: 0118 loss_train (RMSE): 0.1191 loss_val (RMSE): 0.3421\n",
      "Epoch: 0119 loss_train (RMSE): 0.1178 loss_val (RMSE): 0.3409\n",
      "Epoch: 0120 loss_train (RMSE): 0.1165 loss_val (RMSE): 0.3397\n",
      "Epoch: 0121 loss_train (RMSE): 0.1153 loss_val (RMSE): 0.3385\n",
      "Epoch: 0122 loss_train (RMSE): 0.1141 loss_val (RMSE): 0.3374\n",
      "Epoch: 0123 loss_train (RMSE): 0.1129 loss_val (RMSE): 0.3363\n",
      "Epoch: 0124 loss_train (RMSE): 0.1118 loss_val (RMSE): 0.3352\n",
      "Epoch: 0125 loss_train (RMSE): 0.1107 loss_val (RMSE): 0.3342\n",
      "Epoch: 0126 loss_train (RMSE): 0.1096 loss_val (RMSE): 0.3332\n",
      "Epoch: 0127 loss_train (RMSE): 0.1086 loss_val (RMSE): 0.3322\n",
      "Epoch: 0128 loss_train (RMSE): 0.1076 loss_val (RMSE): 0.3313\n",
      "Epoch: 0129 loss_train (RMSE): 0.1066 loss_val (RMSE): 0.3304\n",
      "Epoch: 0130 loss_train (RMSE): 0.1056 loss_val (RMSE): 0.3296\n",
      "Epoch: 0131 loss_train (RMSE): 0.1047 loss_val (RMSE): 0.3287\n",
      "Epoch: 0132 loss_train (RMSE): 0.1038 loss_val (RMSE): 0.3279\n",
      "Epoch: 0133 loss_train (RMSE): 0.1029 loss_val (RMSE): 0.3272\n",
      "Epoch: 0134 loss_train (RMSE): 0.1021 loss_val (RMSE): 0.3264\n",
      "Epoch: 0135 loss_train (RMSE): 0.1012 loss_val (RMSE): 0.3257\n",
      "Epoch: 0136 loss_train (RMSE): 0.1004 loss_val (RMSE): 0.3251\n",
      "Epoch: 0137 loss_train (RMSE): 0.0997 loss_val (RMSE): 0.3244\n",
      "Epoch: 0138 loss_train (RMSE): 0.0989 loss_val (RMSE): 0.3238\n",
      "Epoch: 0139 loss_train (RMSE): 0.0982 loss_val (RMSE): 0.3232\n",
      "Epoch: 0140 loss_train (RMSE): 0.0975 loss_val (RMSE): 0.3227\n",
      "Epoch: 0141 loss_train (RMSE): 0.0968 loss_val (RMSE): 0.3221\n",
      "Epoch: 0142 loss_train (RMSE): 0.0961 loss_val (RMSE): 0.3216\n",
      "Epoch: 0143 loss_train (RMSE): 0.0955 loss_val (RMSE): 0.3211\n",
      "Epoch: 0144 loss_train (RMSE): 0.0949 loss_val (RMSE): 0.3207\n",
      "Epoch: 0145 loss_train (RMSE): 0.0943 loss_val (RMSE): 0.3202\n",
      "Epoch: 0146 loss_train (RMSE): 0.0937 loss_val (RMSE): 0.3198\n",
      "Epoch: 0147 loss_train (RMSE): 0.0931 loss_val (RMSE): 0.3194\n",
      "Epoch: 0148 loss_train (RMSE): 0.0926 loss_val (RMSE): 0.3190\n",
      "Epoch: 0149 loss_train (RMSE): 0.0920 loss_val (RMSE): 0.3187\n",
      "Epoch: 0150 loss_train (RMSE): 0.0915 loss_val (RMSE): 0.3183\n",
      "Epoch: 0151 loss_train (RMSE): 0.0910 loss_val (RMSE): 0.3180\n",
      "Epoch: 0152 loss_train (RMSE): 0.0906 loss_val (RMSE): 0.3177\n",
      "Epoch: 0153 loss_train (RMSE): 0.0901 loss_val (RMSE): 0.3174\n",
      "Epoch: 0154 loss_train (RMSE): 0.0896 loss_val (RMSE): 0.3172\n",
      "Epoch: 0155 loss_train (RMSE): 0.0892 loss_val (RMSE): 0.3169\n",
      "Epoch: 0156 loss_train (RMSE): 0.0888 loss_val (RMSE): 0.3167\n",
      "Epoch: 0157 loss_train (RMSE): 0.0884 loss_val (RMSE): 0.3165\n",
      "Epoch: 0158 loss_train (RMSE): 0.0880 loss_val (RMSE): 0.3163\n",
      "Epoch: 0159 loss_train (RMSE): 0.0876 loss_val (RMSE): 0.3161\n",
      "Epoch: 0160 loss_train (RMSE): 0.0872 loss_val (RMSE): 0.3159\n",
      "Epoch: 0161 loss_train (RMSE): 0.0869 loss_val (RMSE): 0.3157\n",
      "Epoch: 0162 loss_train (RMSE): 0.0865 loss_val (RMSE): 0.3156\n",
      "Epoch: 0163 loss_train (RMSE): 0.0862 loss_val (RMSE): 0.3155\n",
      "Epoch: 0164 loss_train (RMSE): 0.0859 loss_val (RMSE): 0.3153\n",
      "Epoch: 0165 loss_train (RMSE): 0.0856 loss_val (RMSE): 0.3152\n",
      "Epoch: 0166 loss_train (RMSE): 0.0853 loss_val (RMSE): 0.3151\n",
      "Epoch: 0167 loss_train (RMSE): 0.0850 loss_val (RMSE): 0.3150\n",
      "Epoch: 0168 loss_train (RMSE): 0.0847 loss_val (RMSE): 0.3150\n",
      "Epoch: 0169 loss_train (RMSE): 0.0845 loss_val (RMSE): 0.3149\n",
      "Epoch: 0170 loss_train (RMSE): 0.0842 loss_val (RMSE): 0.3148\n",
      "Epoch: 0171 loss_train (RMSE): 0.0840 loss_val (RMSE): 0.3148\n",
      "Epoch: 0172 loss_train (RMSE): 0.0837 loss_val (RMSE): 0.3147\n",
      "Epoch: 0173 loss_train (RMSE): 0.0835 loss_val (RMSE): 0.3147\n",
      "Epoch: 0174 loss_train (RMSE): 0.0833 loss_val (RMSE): 0.3147\n",
      "Epoch: 0175 loss_train (RMSE): 0.0830 loss_val (RMSE): 0.3146\n",
      "Epoch: 0176 loss_train (RMSE): 0.0828 loss_val (RMSE): 0.3146\n",
      "Epoch: 0177 loss_train (RMSE): 0.0826 loss_val (RMSE): 0.3146\n",
      "Epoch: 0178 loss_train (RMSE): 0.0824 loss_val (RMSE): 0.3146\n",
      "Epoch: 0179 loss_train (RMSE): 0.0823 loss_val (RMSE): 0.3146\n",
      "Epoch: 0180 loss_train (RMSE): 0.0821 loss_val (RMSE): 0.3146\n",
      "Epoch: 0181 loss_train (RMSE): 0.0819 loss_val (RMSE): 0.3146\n",
      "Epoch: 0182 loss_train (RMSE): 0.0818 loss_val (RMSE): 0.3147\n",
      "Epoch: 0183 loss_train (RMSE): 0.0816 loss_val (RMSE): 0.3147\n",
      "Epoch: 0184 loss_train (RMSE): 0.0814 loss_val (RMSE): 0.3147\n",
      "Epoch: 0185 loss_train (RMSE): 0.0813 loss_val (RMSE): 0.3147\n",
      "Epoch: 0186 loss_train (RMSE): 0.0812 loss_val (RMSE): 0.3148\n",
      "Epoch: 0187 loss_train (RMSE): 0.0810 loss_val (RMSE): 0.3148\n",
      "Epoch: 0188 loss_train (RMSE): 0.0809 loss_val (RMSE): 0.3149\n",
      "Epoch: 0189 loss_train (RMSE): 0.0808 loss_val (RMSE): 0.3149\n",
      "Epoch: 0190 loss_train (RMSE): 0.0806 loss_val (RMSE): 0.3150\n",
      "Epoch: 0191 loss_train (RMSE): 0.0805 loss_val (RMSE): 0.3150\n",
      "Epoch: 0192 loss_train (RMSE): 0.0804 loss_val (RMSE): 0.3151\n",
      "Epoch: 0193 loss_train (RMSE): 0.0803 loss_val (RMSE): 0.3151\n",
      "Epoch: 0194 loss_train (RMSE): 0.0802 loss_val (RMSE): 0.3152\n",
      "Epoch: 0195 loss_train (RMSE): 0.0801 loss_val (RMSE): 0.3153\n",
      "Epoch: 0196 loss_train (RMSE): 0.0800 loss_val (RMSE): 0.3153\n",
      "Epoch: 0197 loss_train (RMSE): 0.0799 loss_val (RMSE): 0.3154\n",
      "Epoch: 0198 loss_train (RMSE): 0.0798 loss_val (RMSE): 0.3155\n",
      "Epoch: 0199 loss_train (RMSE): 0.0797 loss_val (RMSE): 0.3155\n",
      "Epoch: 0200 loss_train (RMSE): 0.0797 loss_val (RMSE): 0.3156\n",
      "Optimization Finished!\n",
      "Total time elapsed: 0.3967s\n",
      "Test set results: loss= 0.3398 (RMSE)\n",
      "GCN(\n",
      "  (gc1): GraphConvolution (10 -> 2000)\n",
      "  (gc5): GraphConvolution (2000 -> 2)\n",
      ")\n",
      "Epoch: 0001 loss_train (RMSE): 0.6129 loss_val (RMSE): 0.7710\n",
      "Epoch: 0002 loss_train (RMSE): 0.5985 loss_val (RMSE): 0.7619\n",
      "Epoch: 0003 loss_train (RMSE): 0.5843 loss_val (RMSE): 0.7529\n",
      "Epoch: 0004 loss_train (RMSE): 0.5703 loss_val (RMSE): 0.7438\n",
      "Epoch: 0005 loss_train (RMSE): 0.5565 loss_val (RMSE): 0.7348\n",
      "Epoch: 0006 loss_train (RMSE): 0.5430 loss_val (RMSE): 0.7259\n",
      "Epoch: 0007 loss_train (RMSE): 0.5296 loss_val (RMSE): 0.7169\n",
      "Epoch: 0008 loss_train (RMSE): 0.5165 loss_val (RMSE): 0.7081\n",
      "Epoch: 0009 loss_train (RMSE): 0.5036 loss_val (RMSE): 0.6992\n",
      "Epoch: 0010 loss_train (RMSE): 0.4909 loss_val (RMSE): 0.6904\n",
      "Epoch: 0011 loss_train (RMSE): 0.4785 loss_val (RMSE): 0.6817\n",
      "Epoch: 0012 loss_train (RMSE): 0.4663 loss_val (RMSE): 0.6731\n",
      "Epoch: 0013 loss_train (RMSE): 0.4543 loss_val (RMSE): 0.6644\n",
      "Epoch: 0014 loss_train (RMSE): 0.4425 loss_val (RMSE): 0.6559\n",
      "Epoch: 0015 loss_train (RMSE): 0.4310 loss_val (RMSE): 0.6474\n",
      "Epoch: 0016 loss_train (RMSE): 0.4197 loss_val (RMSE): 0.6390\n",
      "Epoch: 0017 loss_train (RMSE): 0.4087 loss_val (RMSE): 0.6307\n",
      "Epoch: 0018 loss_train (RMSE): 0.3979 loss_val (RMSE): 0.6225\n",
      "Epoch: 0019 loss_train (RMSE): 0.3873 loss_val (RMSE): 0.6143\n",
      "Epoch: 0020 loss_train (RMSE): 0.3770 loss_val (RMSE): 0.6062\n",
      "Epoch: 0021 loss_train (RMSE): 0.3669 loss_val (RMSE): 0.5982\n",
      "Epoch: 0022 loss_train (RMSE): 0.3570 loss_val (RMSE): 0.5903\n",
      "Epoch: 0023 loss_train (RMSE): 0.3474 loss_val (RMSE): 0.5825\n",
      "Epoch: 0024 loss_train (RMSE): 0.3380 loss_val (RMSE): 0.5748\n",
      "Epoch: 0025 loss_train (RMSE): 0.3288 loss_val (RMSE): 0.5671\n",
      "Epoch: 0026 loss_train (RMSE): 0.3199 loss_val (RMSE): 0.5596\n",
      "Epoch: 0027 loss_train (RMSE): 0.3112 loss_val (RMSE): 0.5522\n",
      "Epoch: 0028 loss_train (RMSE): 0.3027 loss_val (RMSE): 0.5449\n",
      "Epoch: 0029 loss_train (RMSE): 0.2945 loss_val (RMSE): 0.5377\n",
      "Epoch: 0030 loss_train (RMSE): 0.2865 loss_val (RMSE): 0.5306\n",
      "Epoch: 0031 loss_train (RMSE): 0.2787 loss_val (RMSE): 0.5236\n",
      "Epoch: 0032 loss_train (RMSE): 0.2711 loss_val (RMSE): 0.5167\n",
      "Epoch: 0033 loss_train (RMSE): 0.2637 loss_val (RMSE): 0.5100\n",
      "Epoch: 0034 loss_train (RMSE): 0.2566 loss_val (RMSE): 0.5033\n",
      "Epoch: 0035 loss_train (RMSE): 0.2497 loss_val (RMSE): 0.4968\n",
      "Epoch: 0036 loss_train (RMSE): 0.2430 loss_val (RMSE): 0.4904\n",
      "Epoch: 0037 loss_train (RMSE): 0.2364 loss_val (RMSE): 0.4842\n",
      "Epoch: 0038 loss_train (RMSE): 0.2301 loss_val (RMSE): 0.4780\n",
      "Epoch: 0039 loss_train (RMSE): 0.2240 loss_val (RMSE): 0.4720\n",
      "Epoch: 0040 loss_train (RMSE): 0.2181 loss_val (RMSE): 0.4661\n",
      "Epoch: 0041 loss_train (RMSE): 0.2124 loss_val (RMSE): 0.4603\n",
      "Epoch: 0042 loss_train (RMSE): 0.2069 loss_val (RMSE): 0.4547\n",
      "Epoch: 0043 loss_train (RMSE): 0.2015 loss_val (RMSE): 0.4492\n",
      "Epoch: 0044 loss_train (RMSE): 0.1964 loss_val (RMSE): 0.4438\n",
      "Epoch: 0045 loss_train (RMSE): 0.1914 loss_val (RMSE): 0.4385\n",
      "Epoch: 0046 loss_train (RMSE): 0.1866 loss_val (RMSE): 0.4334\n",
      "Epoch: 0047 loss_train (RMSE): 0.1819 loss_val (RMSE): 0.4284\n",
      "Epoch: 0048 loss_train (RMSE): 0.1775 loss_val (RMSE): 0.4235\n",
      "Epoch: 0049 loss_train (RMSE): 0.1732 loss_val (RMSE): 0.4188\n",
      "Epoch: 0050 loss_train (RMSE): 0.1690 loss_val (RMSE): 0.4142\n",
      "Epoch: 0051 loss_train (RMSE): 0.1650 loss_val (RMSE): 0.4097\n",
      "Epoch: 0052 loss_train (RMSE): 0.1612 loss_val (RMSE): 0.4054\n",
      "Epoch: 0053 loss_train (RMSE): 0.1575 loss_val (RMSE): 0.4011\n",
      "Epoch: 0054 loss_train (RMSE): 0.1539 loss_val (RMSE): 0.3970\n",
      "Epoch: 0055 loss_train (RMSE): 0.1505 loss_val (RMSE): 0.3931\n",
      "Epoch: 0056 loss_train (RMSE): 0.1472 loss_val (RMSE): 0.3892\n",
      "Epoch: 0057 loss_train (RMSE): 0.1441 loss_val (RMSE): 0.3855\n",
      "Epoch: 0058 loss_train (RMSE): 0.1411 loss_val (RMSE): 0.3819\n",
      "Epoch: 0059 loss_train (RMSE): 0.1382 loss_val (RMSE): 0.3784\n",
      "Epoch: 0060 loss_train (RMSE): 0.1354 loss_val (RMSE): 0.3751\n",
      "Epoch: 0061 loss_train (RMSE): 0.1328 loss_val (RMSE): 0.3718\n",
      "Epoch: 0062 loss_train (RMSE): 0.1302 loss_val (RMSE): 0.3687\n",
      "Epoch: 0063 loss_train (RMSE): 0.1278 loss_val (RMSE): 0.3657\n",
      "Epoch: 0064 loss_train (RMSE): 0.1255 loss_val (RMSE): 0.3628\n",
      "Epoch: 0065 loss_train (RMSE): 0.1232 loss_val (RMSE): 0.3600\n",
      "Epoch: 0066 loss_train (RMSE): 0.1211 loss_val (RMSE): 0.3573\n",
      "Epoch: 0067 loss_train (RMSE): 0.1191 loss_val (RMSE): 0.3548\n",
      "Epoch: 0068 loss_train (RMSE): 0.1171 loss_val (RMSE): 0.3523\n",
      "Epoch: 0069 loss_train (RMSE): 0.1153 loss_val (RMSE): 0.3499\n",
      "Epoch: 0070 loss_train (RMSE): 0.1135 loss_val (RMSE): 0.3476\n",
      "Epoch: 0071 loss_train (RMSE): 0.1118 loss_val (RMSE): 0.3455\n",
      "Epoch: 0072 loss_train (RMSE): 0.1102 loss_val (RMSE): 0.3434\n",
      "Epoch: 0073 loss_train (RMSE): 0.1087 loss_val (RMSE): 0.3414\n",
      "Epoch: 0074 loss_train (RMSE): 0.1073 loss_val (RMSE): 0.3395\n",
      "Epoch: 0075 loss_train (RMSE): 0.1059 loss_val (RMSE): 0.3377\n",
      "Epoch: 0076 loss_train (RMSE): 0.1046 loss_val (RMSE): 0.3359\n",
      "Epoch: 0077 loss_train (RMSE): 0.1033 loss_val (RMSE): 0.3343\n",
      "Epoch: 0078 loss_train (RMSE): 0.1021 loss_val (RMSE): 0.3327\n",
      "Epoch: 0079 loss_train (RMSE): 0.1010 loss_val (RMSE): 0.3312\n",
      "Epoch: 0080 loss_train (RMSE): 0.1000 loss_val (RMSE): 0.3298\n",
      "Epoch: 0081 loss_train (RMSE): 0.0989 loss_val (RMSE): 0.3284\n",
      "Epoch: 0082 loss_train (RMSE): 0.0980 loss_val (RMSE): 0.3271\n",
      "Epoch: 0083 loss_train (RMSE): 0.0971 loss_val (RMSE): 0.3259\n",
      "Epoch: 0084 loss_train (RMSE): 0.0962 loss_val (RMSE): 0.3247\n",
      "Epoch: 0085 loss_train (RMSE): 0.0954 loss_val (RMSE): 0.3236\n",
      "Epoch: 0086 loss_train (RMSE): 0.0946 loss_val (RMSE): 0.3226\n",
      "Epoch: 0087 loss_train (RMSE): 0.0939 loss_val (RMSE): 0.3216\n",
      "Epoch: 0088 loss_train (RMSE): 0.0932 loss_val (RMSE): 0.3207\n",
      "Epoch: 0089 loss_train (RMSE): 0.0926 loss_val (RMSE): 0.3198\n",
      "Epoch: 0090 loss_train (RMSE): 0.0919 loss_val (RMSE): 0.3189\n",
      "Epoch: 0091 loss_train (RMSE): 0.0914 loss_val (RMSE): 0.3181\n",
      "Epoch: 0092 loss_train (RMSE): 0.0908 loss_val (RMSE): 0.3174\n",
      "Epoch: 0093 loss_train (RMSE): 0.0903 loss_val (RMSE): 0.3167\n",
      "Epoch: 0094 loss_train (RMSE): 0.0898 loss_val (RMSE): 0.3160\n",
      "Epoch: 0095 loss_train (RMSE): 0.0894 loss_val (RMSE): 0.3154\n",
      "Epoch: 0096 loss_train (RMSE): 0.0889 loss_val (RMSE): 0.3148\n",
      "Epoch: 0097 loss_train (RMSE): 0.0885 loss_val (RMSE): 0.3142\n",
      "Epoch: 0098 loss_train (RMSE): 0.0881 loss_val (RMSE): 0.3137\n",
      "Epoch: 0099 loss_train (RMSE): 0.0878 loss_val (RMSE): 0.3132\n",
      "Epoch: 0100 loss_train (RMSE): 0.0874 loss_val (RMSE): 0.3128\n",
      "Epoch: 0101 loss_train (RMSE): 0.0871 loss_val (RMSE): 0.3123\n",
      "Epoch: 0102 loss_train (RMSE): 0.0868 loss_val (RMSE): 0.3119\n",
      "Epoch: 0103 loss_train (RMSE): 0.0865 loss_val (RMSE): 0.3115\n",
      "Epoch: 0104 loss_train (RMSE): 0.0863 loss_val (RMSE): 0.3112\n",
      "Epoch: 0105 loss_train (RMSE): 0.0860 loss_val (RMSE): 0.3108\n",
      "Epoch: 0106 loss_train (RMSE): 0.0858 loss_val (RMSE): 0.3105\n",
      "Epoch: 0107 loss_train (RMSE): 0.0856 loss_val (RMSE): 0.3102\n",
      "Epoch: 0108 loss_train (RMSE): 0.0854 loss_val (RMSE): 0.3099\n",
      "Epoch: 0109 loss_train (RMSE): 0.0852 loss_val (RMSE): 0.3097\n",
      "Epoch: 0110 loss_train (RMSE): 0.0850 loss_val (RMSE): 0.3094\n",
      "Epoch: 0111 loss_train (RMSE): 0.0849 loss_val (RMSE): 0.3092\n",
      "Epoch: 0112 loss_train (RMSE): 0.0847 loss_val (RMSE): 0.3090\n",
      "Epoch: 0113 loss_train (RMSE): 0.0846 loss_val (RMSE): 0.3088\n",
      "Epoch: 0114 loss_train (RMSE): 0.0844 loss_val (RMSE): 0.3086\n",
      "Epoch: 0115 loss_train (RMSE): 0.0843 loss_val (RMSE): 0.3084\n",
      "Epoch: 0116 loss_train (RMSE): 0.0842 loss_val (RMSE): 0.3083\n",
      "Epoch: 0117 loss_train (RMSE): 0.0841 loss_val (RMSE): 0.3081\n",
      "Epoch: 0118 loss_train (RMSE): 0.0840 loss_val (RMSE): 0.3080\n",
      "Epoch: 0119 loss_train (RMSE): 0.0839 loss_val (RMSE): 0.3078\n",
      "Epoch: 0120 loss_train (RMSE): 0.0838 loss_val (RMSE): 0.3077\n",
      "Epoch: 0121 loss_train (RMSE): 0.0837 loss_val (RMSE): 0.3076\n",
      "Epoch: 0122 loss_train (RMSE): 0.0836 loss_val (RMSE): 0.3075\n",
      "Epoch: 0123 loss_train (RMSE): 0.0836 loss_val (RMSE): 0.3074\n",
      "Epoch: 0124 loss_train (RMSE): 0.0835 loss_val (RMSE): 0.3073\n",
      "Epoch: 0125 loss_train (RMSE): 0.0834 loss_val (RMSE): 0.3072\n",
      "Epoch: 0126 loss_train (RMSE): 0.0834 loss_val (RMSE): 0.3071\n",
      "Epoch: 0127 loss_train (RMSE): 0.0833 loss_val (RMSE): 0.3070\n",
      "Epoch: 0128 loss_train (RMSE): 0.0833 loss_val (RMSE): 0.3069\n",
      "Epoch: 0129 loss_train (RMSE): 0.0832 loss_val (RMSE): 0.3069\n",
      "Epoch: 0130 loss_train (RMSE): 0.0832 loss_val (RMSE): 0.3068\n",
      "Epoch: 0131 loss_train (RMSE): 0.0832 loss_val (RMSE): 0.3068\n",
      "Epoch: 0132 loss_train (RMSE): 0.0831 loss_val (RMSE): 0.3067\n",
      "Epoch: 0133 loss_train (RMSE): 0.0831 loss_val (RMSE): 0.3066\n",
      "Epoch: 0134 loss_train (RMSE): 0.0831 loss_val (RMSE): 0.3066\n",
      "Epoch: 0135 loss_train (RMSE): 0.0830 loss_val (RMSE): 0.3065\n",
      "Epoch: 0136 loss_train (RMSE): 0.0830 loss_val (RMSE): 0.3065\n",
      "Epoch: 0137 loss_train (RMSE): 0.0830 loss_val (RMSE): 0.3065\n",
      "Epoch: 0138 loss_train (RMSE): 0.0830 loss_val (RMSE): 0.3064\n",
      "Epoch: 0139 loss_train (RMSE): 0.0829 loss_val (RMSE): 0.3064\n",
      "Epoch: 0140 loss_train (RMSE): 0.0829 loss_val (RMSE): 0.3064\n",
      "Epoch: 0141 loss_train (RMSE): 0.0829 loss_val (RMSE): 0.3063\n",
      "Epoch: 0142 loss_train (RMSE): 0.0829 loss_val (RMSE): 0.3063\n",
      "Epoch: 0143 loss_train (RMSE): 0.0829 loss_val (RMSE): 0.3063\n",
      "Epoch: 0144 loss_train (RMSE): 0.0829 loss_val (RMSE): 0.3062\n",
      "Epoch: 0145 loss_train (RMSE): 0.0828 loss_val (RMSE): 0.3062\n",
      "Epoch: 0146 loss_train (RMSE): 0.0828 loss_val (RMSE): 0.3062\n",
      "Epoch: 0147 loss_train (RMSE): 0.0828 loss_val (RMSE): 0.3062\n",
      "Epoch: 0148 loss_train (RMSE): 0.0828 loss_val (RMSE): 0.3062\n",
      "Epoch: 0149 loss_train (RMSE): 0.0828 loss_val (RMSE): 0.3061\n",
      "Epoch: 0150 loss_train (RMSE): 0.0828 loss_val (RMSE): 0.3061\n",
      "Epoch: 0151 loss_train (RMSE): 0.0828 loss_val (RMSE): 0.3061\n",
      "Epoch: 0152 loss_train (RMSE): 0.0828 loss_val (RMSE): 0.3061\n",
      "Epoch: 0153 loss_train (RMSE): 0.0828 loss_val (RMSE): 0.3061\n",
      "Epoch: 0154 loss_train (RMSE): 0.0828 loss_val (RMSE): 0.3061\n",
      "Epoch: 0155 loss_train (RMSE): 0.0828 loss_val (RMSE): 0.3060\n",
      "Epoch: 0156 loss_train (RMSE): 0.0828 loss_val (RMSE): 0.3060\n",
      "Epoch: 0157 loss_train (RMSE): 0.0828 loss_val (RMSE): 0.3060\n",
      "Epoch: 0158 loss_train (RMSE): 0.0828 loss_val (RMSE): 0.3060\n",
      "Epoch: 0159 loss_train (RMSE): 0.0828 loss_val (RMSE): 0.3060\n",
      "Epoch: 0160 loss_train (RMSE): 0.0827 loss_val (RMSE): 0.3060\n",
      "Epoch: 0161 loss_train (RMSE): 0.0827 loss_val (RMSE): 0.3060\n",
      "Epoch: 0162 loss_train (RMSE): 0.0827 loss_val (RMSE): 0.3060\n",
      "Epoch: 0163 loss_train (RMSE): 0.0827 loss_val (RMSE): 0.3059\n",
      "Epoch: 0164 loss_train (RMSE): 0.0827 loss_val (RMSE): 0.3059\n",
      "Epoch: 0165 loss_train (RMSE): 0.0827 loss_val (RMSE): 0.3059\n",
      "Epoch: 0166 loss_train (RMSE): 0.0827 loss_val (RMSE): 0.3059\n",
      "Epoch: 0167 loss_train (RMSE): 0.0827 loss_val (RMSE): 0.3059\n",
      "Epoch: 0168 loss_train (RMSE): 0.0827 loss_val (RMSE): 0.3059\n",
      "Epoch: 0169 loss_train (RMSE): 0.0827 loss_val (RMSE): 0.3059\n",
      "Epoch: 0170 loss_train (RMSE): 0.0827 loss_val (RMSE): 0.3059\n",
      "Epoch: 0171 loss_train (RMSE): 0.0827 loss_val (RMSE): 0.3059\n",
      "Epoch: 0172 loss_train (RMSE): 0.0827 loss_val (RMSE): 0.3059\n",
      "Epoch: 0173 loss_train (RMSE): 0.0827 loss_val (RMSE): 0.3059\n",
      "Epoch: 0174 loss_train (RMSE): 0.0827 loss_val (RMSE): 0.3059\n",
      "Epoch: 0175 loss_train (RMSE): 0.0827 loss_val (RMSE): 0.3059\n",
      "Epoch: 0176 loss_train (RMSE): 0.0827 loss_val (RMSE): 0.3059\n",
      "Epoch: 0177 loss_train (RMSE): 0.0827 loss_val (RMSE): 0.3059\n",
      "Epoch: 0178 loss_train (RMSE): 0.0827 loss_val (RMSE): 0.3058\n",
      "Epoch: 0179 loss_train (RMSE): 0.0827 loss_val (RMSE): 0.3058\n",
      "Epoch: 0180 loss_train (RMSE): 0.0827 loss_val (RMSE): 0.3058\n",
      "Epoch: 0181 loss_train (RMSE): 0.0827 loss_val (RMSE): 0.3058\n",
      "Epoch: 0182 loss_train (RMSE): 0.0827 loss_val (RMSE): 0.3058\n",
      "Epoch: 0183 loss_train (RMSE): 0.0827 loss_val (RMSE): 0.3058\n",
      "Epoch: 0184 loss_train (RMSE): 0.0827 loss_val (RMSE): 0.3058\n",
      "Epoch: 0185 loss_train (RMSE): 0.0827 loss_val (RMSE): 0.3058\n",
      "Epoch: 0186 loss_train (RMSE): 0.0827 loss_val (RMSE): 0.3058\n",
      "Epoch: 0187 loss_train (RMSE): 0.0827 loss_val (RMSE): 0.3058\n",
      "Epoch: 0188 loss_train (RMSE): 0.0827 loss_val (RMSE): 0.3058\n",
      "Epoch: 0189 loss_train (RMSE): 0.0827 loss_val (RMSE): 0.3058\n",
      "Epoch: 0190 loss_train (RMSE): 0.0827 loss_val (RMSE): 0.3058\n",
      "Epoch: 0191 loss_train (RMSE): 0.0827 loss_val (RMSE): 0.3058\n",
      "Epoch: 0192 loss_train (RMSE): 0.0827 loss_val (RMSE): 0.3058\n",
      "Epoch: 0193 loss_train (RMSE): 0.0827 loss_val (RMSE): 0.3058\n",
      "Epoch: 0194 loss_train (RMSE): 0.0827 loss_val (RMSE): 0.3058\n",
      "Epoch: 0195 loss_train (RMSE): 0.0827 loss_val (RMSE): 0.3058\n",
      "Epoch: 0196 loss_train (RMSE): 0.0827 loss_val (RMSE): 0.3058\n",
      "Epoch: 0197 loss_train (RMSE): 0.0827 loss_val (RMSE): 0.3058\n",
      "Epoch: 0198 loss_train (RMSE): 0.0827 loss_val (RMSE): 0.3058\n",
      "Epoch: 0199 loss_train (RMSE): 0.0827 loss_val (RMSE): 0.3058\n",
      "Epoch: 0200 loss_train (RMSE): 0.0827 loss_val (RMSE): 0.3058\n",
      "Optimization Finished!\n",
      "Total time elapsed: 0.3936s\n",
      "Test set results: loss= 0.2838 (RMSE)\n",
      "GCN(\n",
      "  (gc1): GraphConvolution (10 -> 2000)\n",
      "  (gc5): GraphConvolution (2000 -> 2)\n",
      ")\n",
      "Epoch: 0001 loss_train (RMSE): 0.3268 loss_val (RMSE): 0.5011\n",
      "Epoch: 0002 loss_train (RMSE): 0.3184 loss_val (RMSE): 0.4940\n",
      "Epoch: 0003 loss_train (RMSE): 0.3103 loss_val (RMSE): 0.4871\n",
      "Epoch: 0004 loss_train (RMSE): 0.3024 loss_val (RMSE): 0.4802\n",
      "Epoch: 0005 loss_train (RMSE): 0.2946 loss_val (RMSE): 0.4735\n",
      "Epoch: 0006 loss_train (RMSE): 0.2871 loss_val (RMSE): 0.4669\n",
      "Epoch: 0007 loss_train (RMSE): 0.2798 loss_val (RMSE): 0.4605\n",
      "Epoch: 0008 loss_train (RMSE): 0.2728 loss_val (RMSE): 0.4541\n",
      "Epoch: 0009 loss_train (RMSE): 0.2659 loss_val (RMSE): 0.4480\n",
      "Epoch: 0010 loss_train (RMSE): 0.2593 loss_val (RMSE): 0.4420\n",
      "Epoch: 0011 loss_train (RMSE): 0.2528 loss_val (RMSE): 0.4361\n",
      "Epoch: 0012 loss_train (RMSE): 0.2466 loss_val (RMSE): 0.4304\n",
      "Epoch: 0013 loss_train (RMSE): 0.2406 loss_val (RMSE): 0.4249\n",
      "Epoch: 0014 loss_train (RMSE): 0.2349 loss_val (RMSE): 0.4195\n",
      "Epoch: 0015 loss_train (RMSE): 0.2293 loss_val (RMSE): 0.4143\n",
      "Epoch: 0016 loss_train (RMSE): 0.2239 loss_val (RMSE): 0.4092\n",
      "Epoch: 0017 loss_train (RMSE): 0.2187 loss_val (RMSE): 0.4042\n",
      "Epoch: 0018 loss_train (RMSE): 0.2137 loss_val (RMSE): 0.3995\n",
      "Epoch: 0019 loss_train (RMSE): 0.2089 loss_val (RMSE): 0.3948\n",
      "Epoch: 0020 loss_train (RMSE): 0.2042 loss_val (RMSE): 0.3903\n",
      "Epoch: 0021 loss_train (RMSE): 0.1997 loss_val (RMSE): 0.3859\n",
      "Epoch: 0022 loss_train (RMSE): 0.1954 loss_val (RMSE): 0.3816\n",
      "Epoch: 0023 loss_train (RMSE): 0.1912 loss_val (RMSE): 0.3775\n",
      "Epoch: 0024 loss_train (RMSE): 0.1872 loss_val (RMSE): 0.3734\n",
      "Epoch: 0025 loss_train (RMSE): 0.1833 loss_val (RMSE): 0.3695\n",
      "Epoch: 0026 loss_train (RMSE): 0.1795 loss_val (RMSE): 0.3656\n",
      "Epoch: 0027 loss_train (RMSE): 0.1758 loss_val (RMSE): 0.3618\n",
      "Epoch: 0028 loss_train (RMSE): 0.1722 loss_val (RMSE): 0.3581\n",
      "Epoch: 0029 loss_train (RMSE): 0.1688 loss_val (RMSE): 0.3545\n",
      "Epoch: 0030 loss_train (RMSE): 0.1655 loss_val (RMSE): 0.3510\n",
      "Epoch: 0031 loss_train (RMSE): 0.1622 loss_val (RMSE): 0.3475\n",
      "Epoch: 0032 loss_train (RMSE): 0.1591 loss_val (RMSE): 0.3441\n",
      "Epoch: 0033 loss_train (RMSE): 0.1560 loss_val (RMSE): 0.3408\n",
      "Epoch: 0034 loss_train (RMSE): 0.1531 loss_val (RMSE): 0.3376\n",
      "Epoch: 0035 loss_train (RMSE): 0.1502 loss_val (RMSE): 0.3344\n",
      "Epoch: 0036 loss_train (RMSE): 0.1474 loss_val (RMSE): 0.3314\n",
      "Epoch: 0037 loss_train (RMSE): 0.1448 loss_val (RMSE): 0.3284\n",
      "Epoch: 0038 loss_train (RMSE): 0.1422 loss_val (RMSE): 0.3255\n",
      "Epoch: 0039 loss_train (RMSE): 0.1397 loss_val (RMSE): 0.3227\n",
      "Epoch: 0040 loss_train (RMSE): 0.1373 loss_val (RMSE): 0.3200\n",
      "Epoch: 0041 loss_train (RMSE): 0.1349 loss_val (RMSE): 0.3174\n",
      "Epoch: 0042 loss_train (RMSE): 0.1327 loss_val (RMSE): 0.3149\n",
      "Epoch: 0043 loss_train (RMSE): 0.1306 loss_val (RMSE): 0.3125\n",
      "Epoch: 0044 loss_train (RMSE): 0.1285 loss_val (RMSE): 0.3102\n",
      "Epoch: 0045 loss_train (RMSE): 0.1266 loss_val (RMSE): 0.3080\n",
      "Epoch: 0046 loss_train (RMSE): 0.1247 loss_val (RMSE): 0.3059\n",
      "Epoch: 0047 loss_train (RMSE): 0.1229 loss_val (RMSE): 0.3039\n",
      "Epoch: 0048 loss_train (RMSE): 0.1212 loss_val (RMSE): 0.3020\n",
      "Epoch: 0049 loss_train (RMSE): 0.1195 loss_val (RMSE): 0.3002\n",
      "Epoch: 0050 loss_train (RMSE): 0.1180 loss_val (RMSE): 0.2986\n",
      "Epoch: 0051 loss_train (RMSE): 0.1165 loss_val (RMSE): 0.2970\n",
      "Epoch: 0052 loss_train (RMSE): 0.1151 loss_val (RMSE): 0.2955\n",
      "Epoch: 0053 loss_train (RMSE): 0.1137 loss_val (RMSE): 0.2941\n",
      "Epoch: 0054 loss_train (RMSE): 0.1124 loss_val (RMSE): 0.2928\n",
      "Epoch: 0055 loss_train (RMSE): 0.1112 loss_val (RMSE): 0.2916\n",
      "Epoch: 0056 loss_train (RMSE): 0.1100 loss_val (RMSE): 0.2905\n",
      "Epoch: 0057 loss_train (RMSE): 0.1089 loss_val (RMSE): 0.2895\n",
      "Epoch: 0058 loss_train (RMSE): 0.1079 loss_val (RMSE): 0.2885\n",
      "Epoch: 0059 loss_train (RMSE): 0.1069 loss_val (RMSE): 0.2876\n",
      "Epoch: 0060 loss_train (RMSE): 0.1059 loss_val (RMSE): 0.2868\n",
      "Epoch: 0061 loss_train (RMSE): 0.1050 loss_val (RMSE): 0.2861\n",
      "Epoch: 0062 loss_train (RMSE): 0.1042 loss_val (RMSE): 0.2854\n",
      "Epoch: 0063 loss_train (RMSE): 0.1034 loss_val (RMSE): 0.2848\n",
      "Epoch: 0064 loss_train (RMSE): 0.1026 loss_val (RMSE): 0.2843\n",
      "Epoch: 0065 loss_train (RMSE): 0.1019 loss_val (RMSE): 0.2838\n",
      "Epoch: 0066 loss_train (RMSE): 0.1012 loss_val (RMSE): 0.2834\n",
      "Epoch: 0067 loss_train (RMSE): 0.1006 loss_val (RMSE): 0.2830\n",
      "Epoch: 0068 loss_train (RMSE): 0.0999 loss_val (RMSE): 0.2826\n",
      "Epoch: 0069 loss_train (RMSE): 0.0994 loss_val (RMSE): 0.2823\n",
      "Epoch: 0070 loss_train (RMSE): 0.0988 loss_val (RMSE): 0.2821\n",
      "Epoch: 0071 loss_train (RMSE): 0.0983 loss_val (RMSE): 0.2819\n",
      "Epoch: 0072 loss_train (RMSE): 0.0979 loss_val (RMSE): 0.2817\n",
      "Epoch: 0073 loss_train (RMSE): 0.0974 loss_val (RMSE): 0.2816\n",
      "Epoch: 0074 loss_train (RMSE): 0.0970 loss_val (RMSE): 0.2815\n",
      "Epoch: 0075 loss_train (RMSE): 0.0966 loss_val (RMSE): 0.2814\n",
      "Epoch: 0076 loss_train (RMSE): 0.0962 loss_val (RMSE): 0.2814\n",
      "Epoch: 0077 loss_train (RMSE): 0.0959 loss_val (RMSE): 0.2813\n",
      "Epoch: 0078 loss_train (RMSE): 0.0956 loss_val (RMSE): 0.2813\n",
      "Epoch: 0079 loss_train (RMSE): 0.0953 loss_val (RMSE): 0.2813\n",
      "Epoch: 0080 loss_train (RMSE): 0.0950 loss_val (RMSE): 0.2814\n",
      "Epoch: 0081 loss_train (RMSE): 0.0947 loss_val (RMSE): 0.2814\n",
      "Epoch: 0082 loss_train (RMSE): 0.0945 loss_val (RMSE): 0.2815\n",
      "Epoch: 0083 loss_train (RMSE): 0.0943 loss_val (RMSE): 0.2815\n",
      "Epoch: 0084 loss_train (RMSE): 0.0941 loss_val (RMSE): 0.2816\n",
      "Epoch: 0085 loss_train (RMSE): 0.0939 loss_val (RMSE): 0.2817\n",
      "Epoch: 0086 loss_train (RMSE): 0.0937 loss_val (RMSE): 0.2818\n",
      "Epoch: 0087 loss_train (RMSE): 0.0935 loss_val (RMSE): 0.2819\n",
      "Epoch: 0088 loss_train (RMSE): 0.0934 loss_val (RMSE): 0.2821\n",
      "Epoch: 0089 loss_train (RMSE): 0.0932 loss_val (RMSE): 0.2822\n",
      "Epoch: 0090 loss_train (RMSE): 0.0931 loss_val (RMSE): 0.2823\n",
      "Epoch: 0091 loss_train (RMSE): 0.0930 loss_val (RMSE): 0.2824\n",
      "Epoch: 0092 loss_train (RMSE): 0.0929 loss_val (RMSE): 0.2826\n",
      "Epoch: 0093 loss_train (RMSE): 0.0928 loss_val (RMSE): 0.2827\n",
      "Epoch: 0094 loss_train (RMSE): 0.0927 loss_val (RMSE): 0.2828\n",
      "Epoch: 0095 loss_train (RMSE): 0.0926 loss_val (RMSE): 0.2830\n",
      "Epoch: 0096 loss_train (RMSE): 0.0925 loss_val (RMSE): 0.2831\n",
      "Epoch: 0097 loss_train (RMSE): 0.0924 loss_val (RMSE): 0.2832\n",
      "Epoch: 0098 loss_train (RMSE): 0.0924 loss_val (RMSE): 0.2834\n",
      "Epoch: 0099 loss_train (RMSE): 0.0923 loss_val (RMSE): 0.2835\n",
      "Epoch: 0100 loss_train (RMSE): 0.0923 loss_val (RMSE): 0.2837\n",
      "Epoch: 0101 loss_train (RMSE): 0.0922 loss_val (RMSE): 0.2838\n",
      "Epoch: 0102 loss_train (RMSE): 0.0922 loss_val (RMSE): 0.2839\n",
      "Epoch: 0103 loss_train (RMSE): 0.0921 loss_val (RMSE): 0.2841\n",
      "Epoch: 0104 loss_train (RMSE): 0.0921 loss_val (RMSE): 0.2842\n",
      "Epoch: 0105 loss_train (RMSE): 0.0920 loss_val (RMSE): 0.2843\n",
      "Epoch: 0106 loss_train (RMSE): 0.0920 loss_val (RMSE): 0.2845\n",
      "Epoch: 0107 loss_train (RMSE): 0.0920 loss_val (RMSE): 0.2846\n",
      "Epoch: 0108 loss_train (RMSE): 0.0920 loss_val (RMSE): 0.2847\n",
      "Epoch: 0109 loss_train (RMSE): 0.0919 loss_val (RMSE): 0.2849\n",
      "Epoch: 0110 loss_train (RMSE): 0.0919 loss_val (RMSE): 0.2850\n",
      "Epoch: 0111 loss_train (RMSE): 0.0919 loss_val (RMSE): 0.2851\n",
      "Epoch: 0112 loss_train (RMSE): 0.0919 loss_val (RMSE): 0.2852\n",
      "Epoch: 0113 loss_train (RMSE): 0.0919 loss_val (RMSE): 0.2853\n",
      "Epoch: 0114 loss_train (RMSE): 0.0919 loss_val (RMSE): 0.2854\n",
      "Epoch: 0115 loss_train (RMSE): 0.0918 loss_val (RMSE): 0.2855\n",
      "Epoch: 0116 loss_train (RMSE): 0.0918 loss_val (RMSE): 0.2856\n",
      "Epoch: 0117 loss_train (RMSE): 0.0918 loss_val (RMSE): 0.2857\n",
      "Epoch: 0118 loss_train (RMSE): 0.0918 loss_val (RMSE): 0.2858\n",
      "Epoch: 0119 loss_train (RMSE): 0.0918 loss_val (RMSE): 0.2859\n",
      "Epoch: 0120 loss_train (RMSE): 0.0918 loss_val (RMSE): 0.2860\n",
      "Epoch: 0121 loss_train (RMSE): 0.0918 loss_val (RMSE): 0.2861\n",
      "Epoch: 0122 loss_train (RMSE): 0.0918 loss_val (RMSE): 0.2862\n",
      "Epoch: 0123 loss_train (RMSE): 0.0918 loss_val (RMSE): 0.2862\n",
      "Epoch: 0124 loss_train (RMSE): 0.0918 loss_val (RMSE): 0.2863\n",
      "Epoch: 0125 loss_train (RMSE): 0.0918 loss_val (RMSE): 0.2864\n",
      "Epoch: 0126 loss_train (RMSE): 0.0918 loss_val (RMSE): 0.2864\n",
      "Epoch: 0127 loss_train (RMSE): 0.0918 loss_val (RMSE): 0.2865\n",
      "Epoch: 0128 loss_train (RMSE): 0.0918 loss_val (RMSE): 0.2865\n",
      "Epoch: 0129 loss_train (RMSE): 0.0918 loss_val (RMSE): 0.2866\n",
      "Epoch: 0130 loss_train (RMSE): 0.0918 loss_val (RMSE): 0.2866\n",
      "Epoch: 0131 loss_train (RMSE): 0.0918 loss_val (RMSE): 0.2867\n",
      "Epoch: 0132 loss_train (RMSE): 0.0918 loss_val (RMSE): 0.2867\n",
      "Epoch: 0133 loss_train (RMSE): 0.0918 loss_val (RMSE): 0.2868\n",
      "Epoch: 0134 loss_train (RMSE): 0.0918 loss_val (RMSE): 0.2868\n",
      "Epoch: 0135 loss_train (RMSE): 0.0918 loss_val (RMSE): 0.2869\n",
      "Epoch: 0136 loss_train (RMSE): 0.0918 loss_val (RMSE): 0.2869\n",
      "Epoch: 0137 loss_train (RMSE): 0.0918 loss_val (RMSE): 0.2869\n",
      "Epoch: 0138 loss_train (RMSE): 0.0918 loss_val (RMSE): 0.2869\n",
      "Epoch: 0139 loss_train (RMSE): 0.0918 loss_val (RMSE): 0.2870\n",
      "Epoch: 0140 loss_train (RMSE): 0.0918 loss_val (RMSE): 0.2870\n",
      "Epoch: 0141 loss_train (RMSE): 0.0918 loss_val (RMSE): 0.2870\n",
      "Epoch: 0142 loss_train (RMSE): 0.0918 loss_val (RMSE): 0.2870\n",
      "Epoch: 0143 loss_train (RMSE): 0.0918 loss_val (RMSE): 0.2871\n",
      "Epoch: 0144 loss_train (RMSE): 0.0918 loss_val (RMSE): 0.2871\n",
      "Epoch: 0145 loss_train (RMSE): 0.0918 loss_val (RMSE): 0.2871\n",
      "Epoch: 0146 loss_train (RMSE): 0.0918 loss_val (RMSE): 0.2871\n",
      "Epoch: 0147 loss_train (RMSE): 0.0918 loss_val (RMSE): 0.2871\n",
      "Epoch: 0148 loss_train (RMSE): 0.0918 loss_val (RMSE): 0.2871\n",
      "Epoch: 0149 loss_train (RMSE): 0.0918 loss_val (RMSE): 0.2871\n",
      "Epoch: 0150 loss_train (RMSE): 0.0918 loss_val (RMSE): 0.2871\n",
      "Epoch: 0151 loss_train (RMSE): 0.0918 loss_val (RMSE): 0.2872\n",
      "Epoch: 0152 loss_train (RMSE): 0.0918 loss_val (RMSE): 0.2872\n",
      "Epoch: 0153 loss_train (RMSE): 0.0918 loss_val (RMSE): 0.2872\n",
      "Epoch: 0154 loss_train (RMSE): 0.0918 loss_val (RMSE): 0.2872\n",
      "Epoch: 0155 loss_train (RMSE): 0.0918 loss_val (RMSE): 0.2872\n",
      "Epoch: 0156 loss_train (RMSE): 0.0918 loss_val (RMSE): 0.2872\n",
      "Epoch: 0157 loss_train (RMSE): 0.0918 loss_val (RMSE): 0.2872\n",
      "Epoch: 0158 loss_train (RMSE): 0.0918 loss_val (RMSE): 0.2872\n",
      "Epoch: 0159 loss_train (RMSE): 0.0918 loss_val (RMSE): 0.2872\n",
      "Epoch: 0160 loss_train (RMSE): 0.0918 loss_val (RMSE): 0.2872\n",
      "Epoch: 0161 loss_train (RMSE): 0.0918 loss_val (RMSE): 0.2872\n",
      "Epoch: 0162 loss_train (RMSE): 0.0918 loss_val (RMSE): 0.2872\n",
      "Epoch: 0163 loss_train (RMSE): 0.0918 loss_val (RMSE): 0.2872\n",
      "Epoch: 0164 loss_train (RMSE): 0.0918 loss_val (RMSE): 0.2872\n",
      "Epoch: 0165 loss_train (RMSE): 0.0918 loss_val (RMSE): 0.2872\n",
      "Epoch: 0166 loss_train (RMSE): 0.0918 loss_val (RMSE): 0.2872\n",
      "Epoch: 0167 loss_train (RMSE): 0.0918 loss_val (RMSE): 0.2872\n",
      "Epoch: 0168 loss_train (RMSE): 0.0918 loss_val (RMSE): 0.2872\n",
      "Epoch: 0169 loss_train (RMSE): 0.0918 loss_val (RMSE): 0.2872\n",
      "Epoch: 0170 loss_train (RMSE): 0.0918 loss_val (RMSE): 0.2872\n",
      "Epoch: 0171 loss_train (RMSE): 0.0918 loss_val (RMSE): 0.2872\n",
      "Epoch: 0172 loss_train (RMSE): 0.0918 loss_val (RMSE): 0.2872\n",
      "Epoch: 0173 loss_train (RMSE): 0.0918 loss_val (RMSE): 0.2872\n",
      "Epoch: 0174 loss_train (RMSE): 0.0918 loss_val (RMSE): 0.2872\n",
      "Epoch: 0175 loss_train (RMSE): 0.0918 loss_val (RMSE): 0.2872\n",
      "Epoch: 0176 loss_train (RMSE): 0.0918 loss_val (RMSE): 0.2872\n",
      "Epoch: 0177 loss_train (RMSE): 0.0918 loss_val (RMSE): 0.2872\n",
      "Epoch: 0178 loss_train (RMSE): 0.0918 loss_val (RMSE): 0.2872\n",
      "Epoch: 0179 loss_train (RMSE): 0.0918 loss_val (RMSE): 0.2872\n",
      "Epoch: 0180 loss_train (RMSE): 0.0918 loss_val (RMSE): 0.2872\n",
      "Epoch: 0181 loss_train (RMSE): 0.0918 loss_val (RMSE): 0.2872\n",
      "Epoch: 0182 loss_train (RMSE): 0.0918 loss_val (RMSE): 0.2872\n",
      "Epoch: 0183 loss_train (RMSE): 0.0918 loss_val (RMSE): 0.2872\n",
      "Epoch: 0184 loss_train (RMSE): 0.0918 loss_val (RMSE): 0.2872\n",
      "Epoch: 0185 loss_train (RMSE): 0.0918 loss_val (RMSE): 0.2872\n",
      "Epoch: 0186 loss_train (RMSE): 0.0918 loss_val (RMSE): 0.2872\n",
      "Epoch: 0187 loss_train (RMSE): 0.0918 loss_val (RMSE): 0.2872\n",
      "Epoch: 0188 loss_train (RMSE): 0.0918 loss_val (RMSE): 0.2872\n",
      "Epoch: 0189 loss_train (RMSE): 0.0918 loss_val (RMSE): 0.2872\n",
      "Epoch: 0190 loss_train (RMSE): 0.0918 loss_val (RMSE): 0.2871\n",
      "Epoch: 0191 loss_train (RMSE): 0.0918 loss_val (RMSE): 0.2871\n",
      "Epoch: 0192 loss_train (RMSE): 0.0918 loss_val (RMSE): 0.2871\n",
      "Epoch: 0193 loss_train (RMSE): 0.0918 loss_val (RMSE): 0.2871\n",
      "Epoch: 0194 loss_train (RMSE): 0.0918 loss_val (RMSE): 0.2871\n",
      "Epoch: 0195 loss_train (RMSE): 0.0918 loss_val (RMSE): 0.2871\n",
      "Epoch: 0196 loss_train (RMSE): 0.0918 loss_val (RMSE): 0.2871\n",
      "Epoch: 0197 loss_train (RMSE): 0.0918 loss_val (RMSE): 0.2871\n",
      "Epoch: 0198 loss_train (RMSE): 0.0918 loss_val (RMSE): 0.2871\n",
      "Epoch: 0199 loss_train (RMSE): 0.0918 loss_val (RMSE): 0.2871\n",
      "Epoch: 0200 loss_train (RMSE): 0.0918 loss_val (RMSE): 0.2871\n",
      "Optimization Finished!\n",
      "Total time elapsed: 0.3881s\n",
      "Test set results: loss= 0.2887 (RMSE)\n",
      "GCN(\n",
      "  (gc1): GraphConvolution (10 -> 2000)\n",
      "  (gc5): GraphConvolution (2000 -> 2)\n",
      ")\n",
      "Epoch: 0001 loss_train (RMSE): 0.1526 loss_val (RMSE): 0.2906\n",
      "Epoch: 0002 loss_train (RMSE): 0.1479 loss_val (RMSE): 0.2874\n",
      "Epoch: 0003 loss_train (RMSE): 0.1434 loss_val (RMSE): 0.2846\n",
      "Epoch: 0004 loss_train (RMSE): 0.1392 loss_val (RMSE): 0.2821\n",
      "Epoch: 0005 loss_train (RMSE): 0.1351 loss_val (RMSE): 0.2799\n",
      "Epoch: 0006 loss_train (RMSE): 0.1313 loss_val (RMSE): 0.2780\n",
      "Epoch: 0007 loss_train (RMSE): 0.1277 loss_val (RMSE): 0.2765\n",
      "Epoch: 0008 loss_train (RMSE): 0.1243 loss_val (RMSE): 0.2753\n",
      "Epoch: 0009 loss_train (RMSE): 0.1211 loss_val (RMSE): 0.2745\n",
      "Epoch: 0010 loss_train (RMSE): 0.1181 loss_val (RMSE): 0.2739\n",
      "Epoch: 0011 loss_train (RMSE): 0.1154 loss_val (RMSE): 0.2737\n",
      "Epoch: 0012 loss_train (RMSE): 0.1128 loss_val (RMSE): 0.2737\n",
      "Epoch: 0013 loss_train (RMSE): 0.1105 loss_val (RMSE): 0.2740\n",
      "Epoch: 0014 loss_train (RMSE): 0.1083 loss_val (RMSE): 0.2746\n",
      "Epoch: 0015 loss_train (RMSE): 0.1063 loss_val (RMSE): 0.2753\n",
      "Epoch: 0016 loss_train (RMSE): 0.1046 loss_val (RMSE): 0.2763\n",
      "Epoch: 0017 loss_train (RMSE): 0.1030 loss_val (RMSE): 0.2774\n",
      "Epoch: 0018 loss_train (RMSE): 0.1015 loss_val (RMSE): 0.2786\n",
      "Epoch: 0019 loss_train (RMSE): 0.1002 loss_val (RMSE): 0.2799\n",
      "Epoch: 0020 loss_train (RMSE): 0.0991 loss_val (RMSE): 0.2813\n",
      "Epoch: 0021 loss_train (RMSE): 0.0980 loss_val (RMSE): 0.2827\n",
      "Epoch: 0022 loss_train (RMSE): 0.0971 loss_val (RMSE): 0.2841\n",
      "Epoch: 0023 loss_train (RMSE): 0.0963 loss_val (RMSE): 0.2856\n",
      "Epoch: 0024 loss_train (RMSE): 0.0956 loss_val (RMSE): 0.2869\n",
      "Epoch: 0025 loss_train (RMSE): 0.0950 loss_val (RMSE): 0.2882\n",
      "Epoch: 0026 loss_train (RMSE): 0.0945 loss_val (RMSE): 0.2895\n",
      "Epoch: 0027 loss_train (RMSE): 0.0940 loss_val (RMSE): 0.2907\n",
      "Epoch: 0028 loss_train (RMSE): 0.0936 loss_val (RMSE): 0.2918\n",
      "Epoch: 0029 loss_train (RMSE): 0.0933 loss_val (RMSE): 0.2927\n",
      "Epoch: 0030 loss_train (RMSE): 0.0930 loss_val (RMSE): 0.2936\n",
      "Epoch: 0031 loss_train (RMSE): 0.0927 loss_val (RMSE): 0.2944\n",
      "Epoch: 0032 loss_train (RMSE): 0.0924 loss_val (RMSE): 0.2951\n",
      "Epoch: 0033 loss_train (RMSE): 0.0922 loss_val (RMSE): 0.2957\n",
      "Epoch: 0034 loss_train (RMSE): 0.0921 loss_val (RMSE): 0.2962\n",
      "Epoch: 0035 loss_train (RMSE): 0.0919 loss_val (RMSE): 0.2966\n",
      "Epoch: 0036 loss_train (RMSE): 0.0918 loss_val (RMSE): 0.2969\n",
      "Epoch: 0037 loss_train (RMSE): 0.0917 loss_val (RMSE): 0.2972\n",
      "Epoch: 0038 loss_train (RMSE): 0.0916 loss_val (RMSE): 0.2974\n",
      "Epoch: 0039 loss_train (RMSE): 0.0915 loss_val (RMSE): 0.2975\n",
      "Epoch: 0040 loss_train (RMSE): 0.0914 loss_val (RMSE): 0.2975\n",
      "Epoch: 0041 loss_train (RMSE): 0.0914 loss_val (RMSE): 0.2976\n",
      "Epoch: 0042 loss_train (RMSE): 0.0914 loss_val (RMSE): 0.2975\n",
      "Epoch: 0043 loss_train (RMSE): 0.0913 loss_val (RMSE): 0.2975\n",
      "Epoch: 0044 loss_train (RMSE): 0.0913 loss_val (RMSE): 0.2974\n",
      "Epoch: 0045 loss_train (RMSE): 0.0913 loss_val (RMSE): 0.2973\n",
      "Epoch: 0046 loss_train (RMSE): 0.0913 loss_val (RMSE): 0.2972\n",
      "Epoch: 0047 loss_train (RMSE): 0.0913 loss_val (RMSE): 0.2970\n",
      "Epoch: 0048 loss_train (RMSE): 0.0913 loss_val (RMSE): 0.2969\n",
      "Epoch: 0049 loss_train (RMSE): 0.0914 loss_val (RMSE): 0.2967\n",
      "Epoch: 0050 loss_train (RMSE): 0.0914 loss_val (RMSE): 0.2966\n",
      "Epoch: 0051 loss_train (RMSE): 0.0914 loss_val (RMSE): 0.2964\n",
      "Epoch: 0052 loss_train (RMSE): 0.0914 loss_val (RMSE): 0.2963\n",
      "Epoch: 0053 loss_train (RMSE): 0.0914 loss_val (RMSE): 0.2961\n",
      "Epoch: 0054 loss_train (RMSE): 0.0914 loss_val (RMSE): 0.2960\n",
      "Epoch: 0055 loss_train (RMSE): 0.0914 loss_val (RMSE): 0.2958\n",
      "Epoch: 0056 loss_train (RMSE): 0.0914 loss_val (RMSE): 0.2957\n",
      "Epoch: 0057 loss_train (RMSE): 0.0914 loss_val (RMSE): 0.2956\n",
      "Epoch: 0058 loss_train (RMSE): 0.0914 loss_val (RMSE): 0.2955\n",
      "Epoch: 0059 loss_train (RMSE): 0.0914 loss_val (RMSE): 0.2954\n",
      "Epoch: 0060 loss_train (RMSE): 0.0914 loss_val (RMSE): 0.2953\n",
      "Epoch: 0061 loss_train (RMSE): 0.0914 loss_val (RMSE): 0.2952\n",
      "Epoch: 0062 loss_train (RMSE): 0.0914 loss_val (RMSE): 0.2951\n",
      "Epoch: 0063 loss_train (RMSE): 0.0914 loss_val (RMSE): 0.2951\n",
      "Epoch: 0064 loss_train (RMSE): 0.0914 loss_val (RMSE): 0.2950\n",
      "Epoch: 0065 loss_train (RMSE): 0.0913 loss_val (RMSE): 0.2950\n",
      "Epoch: 0066 loss_train (RMSE): 0.0913 loss_val (RMSE): 0.2950\n",
      "Epoch: 0067 loss_train (RMSE): 0.0913 loss_val (RMSE): 0.2949\n",
      "Epoch: 0068 loss_train (RMSE): 0.0913 loss_val (RMSE): 0.2949\n",
      "Epoch: 0069 loss_train (RMSE): 0.0913 loss_val (RMSE): 0.2949\n",
      "Epoch: 0070 loss_train (RMSE): 0.0913 loss_val (RMSE): 0.2948\n",
      "Epoch: 0071 loss_train (RMSE): 0.0913 loss_val (RMSE): 0.2948\n",
      "Epoch: 0072 loss_train (RMSE): 0.0913 loss_val (RMSE): 0.2948\n",
      "Epoch: 0073 loss_train (RMSE): 0.0913 loss_val (RMSE): 0.2947\n",
      "Epoch: 0074 loss_train (RMSE): 0.0913 loss_val (RMSE): 0.2947\n",
      "Epoch: 0075 loss_train (RMSE): 0.0913 loss_val (RMSE): 0.2946\n",
      "Epoch: 0076 loss_train (RMSE): 0.0913 loss_val (RMSE): 0.2946\n",
      "Epoch: 0077 loss_train (RMSE): 0.0913 loss_val (RMSE): 0.2945\n",
      "Epoch: 0078 loss_train (RMSE): 0.0913 loss_val (RMSE): 0.2945\n",
      "Epoch: 0079 loss_train (RMSE): 0.0913 loss_val (RMSE): 0.2944\n",
      "Epoch: 0080 loss_train (RMSE): 0.0913 loss_val (RMSE): 0.2943\n",
      "Epoch: 0081 loss_train (RMSE): 0.0913 loss_val (RMSE): 0.2943\n",
      "Epoch: 0082 loss_train (RMSE): 0.0913 loss_val (RMSE): 0.2942\n",
      "Epoch: 0083 loss_train (RMSE): 0.0913 loss_val (RMSE): 0.2941\n",
      "Epoch: 0084 loss_train (RMSE): 0.0913 loss_val (RMSE): 0.2940\n",
      "Epoch: 0085 loss_train (RMSE): 0.0913 loss_val (RMSE): 0.2939\n",
      "Epoch: 0086 loss_train (RMSE): 0.0913 loss_val (RMSE): 0.2938\n",
      "Epoch: 0087 loss_train (RMSE): 0.0913 loss_val (RMSE): 0.2937\n",
      "Epoch: 0088 loss_train (RMSE): 0.0913 loss_val (RMSE): 0.2937\n",
      "Epoch: 0089 loss_train (RMSE): 0.0913 loss_val (RMSE): 0.2936\n",
      "Epoch: 0090 loss_train (RMSE): 0.0913 loss_val (RMSE): 0.2935\n",
      "Epoch: 0091 loss_train (RMSE): 0.0913 loss_val (RMSE): 0.2934\n",
      "Epoch: 0092 loss_train (RMSE): 0.0913 loss_val (RMSE): 0.2934\n",
      "Epoch: 0093 loss_train (RMSE): 0.0913 loss_val (RMSE): 0.2933\n",
      "Epoch: 0094 loss_train (RMSE): 0.0913 loss_val (RMSE): 0.2933\n",
      "Epoch: 0095 loss_train (RMSE): 0.0913 loss_val (RMSE): 0.2932\n",
      "Epoch: 0096 loss_train (RMSE): 0.0913 loss_val (RMSE): 0.2932\n",
      "Epoch: 0097 loss_train (RMSE): 0.0913 loss_val (RMSE): 0.2932\n",
      "Epoch: 0098 loss_train (RMSE): 0.0913 loss_val (RMSE): 0.2931\n",
      "Epoch: 0099 loss_train (RMSE): 0.0913 loss_val (RMSE): 0.2931\n",
      "Epoch: 0100 loss_train (RMSE): 0.0913 loss_val (RMSE): 0.2931\n",
      "Epoch: 0101 loss_train (RMSE): 0.0913 loss_val (RMSE): 0.2931\n",
      "Epoch: 0102 loss_train (RMSE): 0.0913 loss_val (RMSE): 0.2931\n",
      "Epoch: 0103 loss_train (RMSE): 0.0913 loss_val (RMSE): 0.2932\n",
      "Epoch: 0104 loss_train (RMSE): 0.0913 loss_val (RMSE): 0.2932\n",
      "Epoch: 0105 loss_train (RMSE): 0.0913 loss_val (RMSE): 0.2932\n",
      "Epoch: 0106 loss_train (RMSE): 0.0913 loss_val (RMSE): 0.2932\n",
      "Epoch: 0107 loss_train (RMSE): 0.0913 loss_val (RMSE): 0.2933\n",
      "Epoch: 0108 loss_train (RMSE): 0.0913 loss_val (RMSE): 0.2933\n",
      "Epoch: 0109 loss_train (RMSE): 0.0913 loss_val (RMSE): 0.2933\n",
      "Epoch: 0110 loss_train (RMSE): 0.0913 loss_val (RMSE): 0.2934\n",
      "Epoch: 0111 loss_train (RMSE): 0.0913 loss_val (RMSE): 0.2934\n",
      "Epoch: 0112 loss_train (RMSE): 0.0913 loss_val (RMSE): 0.2935\n",
      "Epoch: 0113 loss_train (RMSE): 0.0913 loss_val (RMSE): 0.2935\n",
      "Epoch: 0114 loss_train (RMSE): 0.0913 loss_val (RMSE): 0.2936\n",
      "Epoch: 0115 loss_train (RMSE): 0.0913 loss_val (RMSE): 0.2936\n",
      "Epoch: 0116 loss_train (RMSE): 0.0913 loss_val (RMSE): 0.2936\n",
      "Epoch: 0117 loss_train (RMSE): 0.0913 loss_val (RMSE): 0.2937\n",
      "Epoch: 0118 loss_train (RMSE): 0.0913 loss_val (RMSE): 0.2937\n",
      "Epoch: 0119 loss_train (RMSE): 0.0913 loss_val (RMSE): 0.2937\n",
      "Epoch: 0120 loss_train (RMSE): 0.0913 loss_val (RMSE): 0.2937\n",
      "Epoch: 0121 loss_train (RMSE): 0.0913 loss_val (RMSE): 0.2937\n",
      "Epoch: 0122 loss_train (RMSE): 0.0913 loss_val (RMSE): 0.2938\n",
      "Epoch: 0123 loss_train (RMSE): 0.0913 loss_val (RMSE): 0.2938\n",
      "Epoch: 0124 loss_train (RMSE): 0.0913 loss_val (RMSE): 0.2938\n",
      "Epoch: 0125 loss_train (RMSE): 0.0913 loss_val (RMSE): 0.2938\n",
      "Epoch: 0126 loss_train (RMSE): 0.0913 loss_val (RMSE): 0.2938\n",
      "Epoch: 0127 loss_train (RMSE): 0.0913 loss_val (RMSE): 0.2938\n",
      "Epoch: 0128 loss_train (RMSE): 0.0913 loss_val (RMSE): 0.2938\n",
      "Epoch: 0129 loss_train (RMSE): 0.0913 loss_val (RMSE): 0.2938\n",
      "Epoch: 0130 loss_train (RMSE): 0.0913 loss_val (RMSE): 0.2938\n",
      "Epoch: 0131 loss_train (RMSE): 0.0913 loss_val (RMSE): 0.2938\n",
      "Epoch: 0132 loss_train (RMSE): 0.0913 loss_val (RMSE): 0.2938\n",
      "Epoch: 0133 loss_train (RMSE): 0.0913 loss_val (RMSE): 0.2937\n",
      "Epoch: 0134 loss_train (RMSE): 0.0913 loss_val (RMSE): 0.2937\n",
      "Epoch: 0135 loss_train (RMSE): 0.0913 loss_val (RMSE): 0.2937\n",
      "Epoch: 0136 loss_train (RMSE): 0.0913 loss_val (RMSE): 0.2937\n",
      "Epoch: 0137 loss_train (RMSE): 0.0913 loss_val (RMSE): 0.2937\n",
      "Epoch: 0138 loss_train (RMSE): 0.0913 loss_val (RMSE): 0.2937\n",
      "Epoch: 0139 loss_train (RMSE): 0.0913 loss_val (RMSE): 0.2937\n",
      "Epoch: 0140 loss_train (RMSE): 0.0913 loss_val (RMSE): 0.2937\n",
      "Epoch: 0141 loss_train (RMSE): 0.0913 loss_val (RMSE): 0.2937\n",
      "Epoch: 0142 loss_train (RMSE): 0.0913 loss_val (RMSE): 0.2937\n",
      "Epoch: 0143 loss_train (RMSE): 0.0913 loss_val (RMSE): 0.2937\n",
      "Epoch: 0144 loss_train (RMSE): 0.0913 loss_val (RMSE): 0.2936\n",
      "Epoch: 0145 loss_train (RMSE): 0.0913 loss_val (RMSE): 0.2936\n",
      "Epoch: 0146 loss_train (RMSE): 0.0913 loss_val (RMSE): 0.2936\n",
      "Epoch: 0147 loss_train (RMSE): 0.0913 loss_val (RMSE): 0.2936\n",
      "Epoch: 0148 loss_train (RMSE): 0.0913 loss_val (RMSE): 0.2936\n",
      "Epoch: 0149 loss_train (RMSE): 0.0913 loss_val (RMSE): 0.2936\n",
      "Epoch: 0150 loss_train (RMSE): 0.0913 loss_val (RMSE): 0.2936\n",
      "Epoch: 0151 loss_train (RMSE): 0.0913 loss_val (RMSE): 0.2936\n",
      "Epoch: 0152 loss_train (RMSE): 0.0913 loss_val (RMSE): 0.2936\n",
      "Epoch: 0153 loss_train (RMSE): 0.0913 loss_val (RMSE): 0.2936\n",
      "Epoch: 0154 loss_train (RMSE): 0.0913 loss_val (RMSE): 0.2936\n",
      "Epoch: 0155 loss_train (RMSE): 0.0913 loss_val (RMSE): 0.2936\n",
      "Epoch: 0156 loss_train (RMSE): 0.0913 loss_val (RMSE): 0.2936\n",
      "Epoch: 0157 loss_train (RMSE): 0.0913 loss_val (RMSE): 0.2936\n",
      "Epoch: 0158 loss_train (RMSE): 0.0913 loss_val (RMSE): 0.2936\n",
      "Epoch: 0159 loss_train (RMSE): 0.0913 loss_val (RMSE): 0.2936\n",
      "Epoch: 0160 loss_train (RMSE): 0.0913 loss_val (RMSE): 0.2936\n",
      "Epoch: 0161 loss_train (RMSE): 0.0913 loss_val (RMSE): 0.2936\n",
      "Epoch: 0162 loss_train (RMSE): 0.0913 loss_val (RMSE): 0.2936\n",
      "Epoch: 0163 loss_train (RMSE): 0.0913 loss_val (RMSE): 0.2936\n",
      "Epoch: 0164 loss_train (RMSE): 0.0913 loss_val (RMSE): 0.2936\n",
      "Epoch: 0165 loss_train (RMSE): 0.0913 loss_val (RMSE): 0.2936\n",
      "Epoch: 0166 loss_train (RMSE): 0.0913 loss_val (RMSE): 0.2936\n",
      "Epoch: 0167 loss_train (RMSE): 0.0913 loss_val (RMSE): 0.2936\n",
      "Epoch: 0168 loss_train (RMSE): 0.0913 loss_val (RMSE): 0.2936\n",
      "Epoch: 0169 loss_train (RMSE): 0.0913 loss_val (RMSE): 0.2936\n",
      "Epoch: 0170 loss_train (RMSE): 0.0913 loss_val (RMSE): 0.2936\n",
      "Epoch: 0171 loss_train (RMSE): 0.0913 loss_val (RMSE): 0.2936\n",
      "Epoch: 0172 loss_train (RMSE): 0.0913 loss_val (RMSE): 0.2936\n",
      "Epoch: 0173 loss_train (RMSE): 0.0913 loss_val (RMSE): 0.2936\n",
      "Epoch: 0174 loss_train (RMSE): 0.0913 loss_val (RMSE): 0.2936\n",
      "Epoch: 0175 loss_train (RMSE): 0.0913 loss_val (RMSE): 0.2936\n",
      "Epoch: 0176 loss_train (RMSE): 0.0913 loss_val (RMSE): 0.2936\n",
      "Epoch: 0177 loss_train (RMSE): 0.0913 loss_val (RMSE): 0.2936\n",
      "Epoch: 0178 loss_train (RMSE): 0.0913 loss_val (RMSE): 0.2936\n",
      "Epoch: 0179 loss_train (RMSE): 0.0913 loss_val (RMSE): 0.2936\n",
      "Epoch: 0180 loss_train (RMSE): 0.0913 loss_val (RMSE): 0.2936\n",
      "Epoch: 0181 loss_train (RMSE): 0.0913 loss_val (RMSE): 0.2936\n",
      "Epoch: 0182 loss_train (RMSE): 0.0913 loss_val (RMSE): 0.2936\n",
      "Epoch: 0183 loss_train (RMSE): 0.0913 loss_val (RMSE): 0.2936\n",
      "Epoch: 0184 loss_train (RMSE): 0.0913 loss_val (RMSE): 0.2937\n",
      "Epoch: 0185 loss_train (RMSE): 0.0913 loss_val (RMSE): 0.2937\n",
      "Epoch: 0186 loss_train (RMSE): 0.0913 loss_val (RMSE): 0.2937\n",
      "Epoch: 0187 loss_train (RMSE): 0.0913 loss_val (RMSE): 0.2937\n",
      "Epoch: 0188 loss_train (RMSE): 0.0913 loss_val (RMSE): 0.2937\n",
      "Epoch: 0189 loss_train (RMSE): 0.0913 loss_val (RMSE): 0.2937\n",
      "Epoch: 0190 loss_train (RMSE): 0.0913 loss_val (RMSE): 0.2937\n",
      "Epoch: 0191 loss_train (RMSE): 0.0913 loss_val (RMSE): 0.2937\n",
      "Epoch: 0192 loss_train (RMSE): 0.0913 loss_val (RMSE): 0.2937\n",
      "Epoch: 0193 loss_train (RMSE): 0.0913 loss_val (RMSE): 0.2937\n",
      "Epoch: 0194 loss_train (RMSE): 0.0913 loss_val (RMSE): 0.2937\n",
      "Epoch: 0195 loss_train (RMSE): 0.0913 loss_val (RMSE): 0.2937\n",
      "Epoch: 0196 loss_train (RMSE): 0.0913 loss_val (RMSE): 0.2937\n",
      "Epoch: 0197 loss_train (RMSE): 0.0913 loss_val (RMSE): 0.2937\n",
      "Epoch: 0198 loss_train (RMSE): 0.0913 loss_val (RMSE): 0.2937\n",
      "Epoch: 0199 loss_train (RMSE): 0.0913 loss_val (RMSE): 0.2937\n",
      "Epoch: 0200 loss_train (RMSE): 0.0913 loss_val (RMSE): 0.2937\n",
      "Optimization Finished!\n",
      "Total time elapsed: 0.4113s\n",
      "Test set results: loss= 0.3397 (RMSE)\n",
      "GCN(\n",
      "  (gc1): GraphConvolution (10 -> 2000)\n",
      "  (gc5): GraphConvolution (2000 -> 2)\n",
      ")\n",
      "Epoch: 0001 loss_train (RMSE): 0.3437 loss_val (RMSE): 0.5380\n",
      "Epoch: 0002 loss_train (RMSE): 0.3359 loss_val (RMSE): 0.5322\n",
      "Epoch: 0003 loss_train (RMSE): 0.3284 loss_val (RMSE): 0.5265\n",
      "Epoch: 0004 loss_train (RMSE): 0.3211 loss_val (RMSE): 0.5209\n",
      "Epoch: 0005 loss_train (RMSE): 0.3140 loss_val (RMSE): 0.5155\n",
      "Epoch: 0006 loss_train (RMSE): 0.3071 loss_val (RMSE): 0.5101\n",
      "Epoch: 0007 loss_train (RMSE): 0.3004 loss_val (RMSE): 0.5049\n",
      "Epoch: 0008 loss_train (RMSE): 0.2939 loss_val (RMSE): 0.4997\n",
      "Epoch: 0009 loss_train (RMSE): 0.2876 loss_val (RMSE): 0.4946\n",
      "Epoch: 0010 loss_train (RMSE): 0.2814 loss_val (RMSE): 0.4895\n",
      "Epoch: 0011 loss_train (RMSE): 0.2754 loss_val (RMSE): 0.4844\n",
      "Epoch: 0012 loss_train (RMSE): 0.2695 loss_val (RMSE): 0.4793\n",
      "Epoch: 0013 loss_train (RMSE): 0.2638 loss_val (RMSE): 0.4742\n",
      "Epoch: 0014 loss_train (RMSE): 0.2581 loss_val (RMSE): 0.4690\n",
      "Epoch: 0015 loss_train (RMSE): 0.2525 loss_val (RMSE): 0.4638\n",
      "Epoch: 0016 loss_train (RMSE): 0.2470 loss_val (RMSE): 0.4586\n",
      "Epoch: 0017 loss_train (RMSE): 0.2416 loss_val (RMSE): 0.4534\n",
      "Epoch: 0018 loss_train (RMSE): 0.2364 loss_val (RMSE): 0.4482\n",
      "Epoch: 0019 loss_train (RMSE): 0.2312 loss_val (RMSE): 0.4430\n",
      "Epoch: 0020 loss_train (RMSE): 0.2261 loss_val (RMSE): 0.4379\n",
      "Epoch: 0021 loss_train (RMSE): 0.2212 loss_val (RMSE): 0.4328\n",
      "Epoch: 0022 loss_train (RMSE): 0.2163 loss_val (RMSE): 0.4279\n",
      "Epoch: 0023 loss_train (RMSE): 0.2116 loss_val (RMSE): 0.4230\n",
      "Epoch: 0024 loss_train (RMSE): 0.2071 loss_val (RMSE): 0.4182\n",
      "Epoch: 0025 loss_train (RMSE): 0.2026 loss_val (RMSE): 0.4135\n",
      "Epoch: 0026 loss_train (RMSE): 0.1983 loss_val (RMSE): 0.4089\n",
      "Epoch: 0027 loss_train (RMSE): 0.1941 loss_val (RMSE): 0.4045\n",
      "Epoch: 0028 loss_train (RMSE): 0.1901 loss_val (RMSE): 0.4002\n",
      "Epoch: 0029 loss_train (RMSE): 0.1861 loss_val (RMSE): 0.3960\n",
      "Epoch: 0030 loss_train (RMSE): 0.1823 loss_val (RMSE): 0.3919\n",
      "Epoch: 0031 loss_train (RMSE): 0.1785 loss_val (RMSE): 0.3879\n",
      "Epoch: 0032 loss_train (RMSE): 0.1749 loss_val (RMSE): 0.3841\n",
      "Epoch: 0033 loss_train (RMSE): 0.1714 loss_val (RMSE): 0.3804\n",
      "Epoch: 0034 loss_train (RMSE): 0.1679 loss_val (RMSE): 0.3768\n",
      "Epoch: 0035 loss_train (RMSE): 0.1646 loss_val (RMSE): 0.3734\n",
      "Epoch: 0036 loss_train (RMSE): 0.1614 loss_val (RMSE): 0.3701\n",
      "Epoch: 0037 loss_train (RMSE): 0.1583 loss_val (RMSE): 0.3668\n",
      "Epoch: 0038 loss_train (RMSE): 0.1552 loss_val (RMSE): 0.3637\n",
      "Epoch: 0039 loss_train (RMSE): 0.1523 loss_val (RMSE): 0.3607\n",
      "Epoch: 0040 loss_train (RMSE): 0.1495 loss_val (RMSE): 0.3578\n",
      "Epoch: 0041 loss_train (RMSE): 0.1468 loss_val (RMSE): 0.3549\n",
      "Epoch: 0042 loss_train (RMSE): 0.1442 loss_val (RMSE): 0.3522\n",
      "Epoch: 0043 loss_train (RMSE): 0.1417 loss_val (RMSE): 0.3495\n",
      "Epoch: 0044 loss_train (RMSE): 0.1392 loss_val (RMSE): 0.3469\n",
      "Epoch: 0045 loss_train (RMSE): 0.1369 loss_val (RMSE): 0.3444\n",
      "Epoch: 0046 loss_train (RMSE): 0.1346 loss_val (RMSE): 0.3419\n",
      "Epoch: 0047 loss_train (RMSE): 0.1324 loss_val (RMSE): 0.3395\n",
      "Epoch: 0048 loss_train (RMSE): 0.1303 loss_val (RMSE): 0.3372\n",
      "Epoch: 0049 loss_train (RMSE): 0.1283 loss_val (RMSE): 0.3350\n",
      "Epoch: 0050 loss_train (RMSE): 0.1264 loss_val (RMSE): 0.3328\n",
      "Epoch: 0051 loss_train (RMSE): 0.1245 loss_val (RMSE): 0.3307\n",
      "Epoch: 0052 loss_train (RMSE): 0.1227 loss_val (RMSE): 0.3287\n",
      "Epoch: 0053 loss_train (RMSE): 0.1210 loss_val (RMSE): 0.3267\n",
      "Epoch: 0054 loss_train (RMSE): 0.1193 loss_val (RMSE): 0.3249\n",
      "Epoch: 0055 loss_train (RMSE): 0.1178 loss_val (RMSE): 0.3231\n",
      "Epoch: 0056 loss_train (RMSE): 0.1163 loss_val (RMSE): 0.3215\n",
      "Epoch: 0057 loss_train (RMSE): 0.1148 loss_val (RMSE): 0.3199\n",
      "Epoch: 0058 loss_train (RMSE): 0.1134 loss_val (RMSE): 0.3184\n",
      "Epoch: 0059 loss_train (RMSE): 0.1121 loss_val (RMSE): 0.3170\n",
      "Epoch: 0060 loss_train (RMSE): 0.1108 loss_val (RMSE): 0.3157\n",
      "Epoch: 0061 loss_train (RMSE): 0.1096 loss_val (RMSE): 0.3145\n",
      "Epoch: 0062 loss_train (RMSE): 0.1085 loss_val (RMSE): 0.3134\n",
      "Epoch: 0063 loss_train (RMSE): 0.1074 loss_val (RMSE): 0.3123\n",
      "Epoch: 0064 loss_train (RMSE): 0.1063 loss_val (RMSE): 0.3113\n",
      "Epoch: 0065 loss_train (RMSE): 0.1053 loss_val (RMSE): 0.3104\n",
      "Epoch: 0066 loss_train (RMSE): 0.1044 loss_val (RMSE): 0.3096\n",
      "Epoch: 0067 loss_train (RMSE): 0.1035 loss_val (RMSE): 0.3088\n",
      "Epoch: 0068 loss_train (RMSE): 0.1026 loss_val (RMSE): 0.3080\n",
      "Epoch: 0069 loss_train (RMSE): 0.1018 loss_val (RMSE): 0.3073\n",
      "Epoch: 0070 loss_train (RMSE): 0.1010 loss_val (RMSE): 0.3067\n",
      "Epoch: 0071 loss_train (RMSE): 0.1003 loss_val (RMSE): 0.3061\n",
      "Epoch: 0072 loss_train (RMSE): 0.0996 loss_val (RMSE): 0.3055\n",
      "Epoch: 0073 loss_train (RMSE): 0.0989 loss_val (RMSE): 0.3050\n",
      "Epoch: 0074 loss_train (RMSE): 0.0983 loss_val (RMSE): 0.3045\n",
      "Epoch: 0075 loss_train (RMSE): 0.0977 loss_val (RMSE): 0.3040\n",
      "Epoch: 0076 loss_train (RMSE): 0.0972 loss_val (RMSE): 0.3036\n",
      "Epoch: 0077 loss_train (RMSE): 0.0966 loss_val (RMSE): 0.3032\n",
      "Epoch: 0078 loss_train (RMSE): 0.0961 loss_val (RMSE): 0.3028\n",
      "Epoch: 0079 loss_train (RMSE): 0.0957 loss_val (RMSE): 0.3025\n",
      "Epoch: 0080 loss_train (RMSE): 0.0952 loss_val (RMSE): 0.3022\n",
      "Epoch: 0081 loss_train (RMSE): 0.0948 loss_val (RMSE): 0.3019\n",
      "Epoch: 0082 loss_train (RMSE): 0.0944 loss_val (RMSE): 0.3017\n",
      "Epoch: 0083 loss_train (RMSE): 0.0940 loss_val (RMSE): 0.3015\n",
      "Epoch: 0084 loss_train (RMSE): 0.0937 loss_val (RMSE): 0.3013\n",
      "Epoch: 0085 loss_train (RMSE): 0.0934 loss_val (RMSE): 0.3011\n",
      "Epoch: 0086 loss_train (RMSE): 0.0931 loss_val (RMSE): 0.3010\n",
      "Epoch: 0087 loss_train (RMSE): 0.0928 loss_val (RMSE): 0.3009\n",
      "Epoch: 0088 loss_train (RMSE): 0.0925 loss_val (RMSE): 0.3008\n",
      "Epoch: 0089 loss_train (RMSE): 0.0922 loss_val (RMSE): 0.3008\n",
      "Epoch: 0090 loss_train (RMSE): 0.0920 loss_val (RMSE): 0.3007\n",
      "Epoch: 0091 loss_train (RMSE): 0.0918 loss_val (RMSE): 0.3007\n",
      "Epoch: 0092 loss_train (RMSE): 0.0916 loss_val (RMSE): 0.3007\n",
      "Epoch: 0093 loss_train (RMSE): 0.0914 loss_val (RMSE): 0.3007\n",
      "Epoch: 0094 loss_train (RMSE): 0.0912 loss_val (RMSE): 0.3007\n",
      "Epoch: 0095 loss_train (RMSE): 0.0910 loss_val (RMSE): 0.3008\n",
      "Epoch: 0096 loss_train (RMSE): 0.0909 loss_val (RMSE): 0.3008\n",
      "Epoch: 0097 loss_train (RMSE): 0.0907 loss_val (RMSE): 0.3008\n",
      "Epoch: 0098 loss_train (RMSE): 0.0906 loss_val (RMSE): 0.3009\n",
      "Epoch: 0099 loss_train (RMSE): 0.0905 loss_val (RMSE): 0.3009\n",
      "Epoch: 0100 loss_train (RMSE): 0.0903 loss_val (RMSE): 0.3010\n",
      "Epoch: 0101 loss_train (RMSE): 0.0902 loss_val (RMSE): 0.3010\n",
      "Epoch: 0102 loss_train (RMSE): 0.0901 loss_val (RMSE): 0.3011\n",
      "Epoch: 0103 loss_train (RMSE): 0.0900 loss_val (RMSE): 0.3011\n",
      "Epoch: 0104 loss_train (RMSE): 0.0899 loss_val (RMSE): 0.3012\n",
      "Epoch: 0105 loss_train (RMSE): 0.0899 loss_val (RMSE): 0.3013\n",
      "Epoch: 0106 loss_train (RMSE): 0.0898 loss_val (RMSE): 0.3014\n",
      "Epoch: 0107 loss_train (RMSE): 0.0897 loss_val (RMSE): 0.3014\n",
      "Epoch: 0108 loss_train (RMSE): 0.0897 loss_val (RMSE): 0.3015\n",
      "Epoch: 0109 loss_train (RMSE): 0.0896 loss_val (RMSE): 0.3016\n",
      "Epoch: 0110 loss_train (RMSE): 0.0895 loss_val (RMSE): 0.3017\n",
      "Epoch: 0111 loss_train (RMSE): 0.0895 loss_val (RMSE): 0.3018\n",
      "Epoch: 0112 loss_train (RMSE): 0.0894 loss_val (RMSE): 0.3019\n",
      "Epoch: 0113 loss_train (RMSE): 0.0894 loss_val (RMSE): 0.3020\n",
      "Epoch: 0114 loss_train (RMSE): 0.0894 loss_val (RMSE): 0.3020\n",
      "Epoch: 0115 loss_train (RMSE): 0.0893 loss_val (RMSE): 0.3021\n",
      "Epoch: 0116 loss_train (RMSE): 0.0893 loss_val (RMSE): 0.3022\n",
      "Epoch: 0117 loss_train (RMSE): 0.0893 loss_val (RMSE): 0.3023\n",
      "Epoch: 0118 loss_train (RMSE): 0.0892 loss_val (RMSE): 0.3024\n",
      "Epoch: 0119 loss_train (RMSE): 0.0892 loss_val (RMSE): 0.3025\n",
      "Epoch: 0120 loss_train (RMSE): 0.0892 loss_val (RMSE): 0.3026\n",
      "Epoch: 0121 loss_train (RMSE): 0.0892 loss_val (RMSE): 0.3026\n",
      "Epoch: 0122 loss_train (RMSE): 0.0892 loss_val (RMSE): 0.3027\n",
      "Epoch: 0123 loss_train (RMSE): 0.0891 loss_val (RMSE): 0.3028\n",
      "Epoch: 0124 loss_train (RMSE): 0.0891 loss_val (RMSE): 0.3029\n",
      "Epoch: 0125 loss_train (RMSE): 0.0891 loss_val (RMSE): 0.3029\n",
      "Epoch: 0126 loss_train (RMSE): 0.0891 loss_val (RMSE): 0.3030\n",
      "Epoch: 0127 loss_train (RMSE): 0.0891 loss_val (RMSE): 0.3031\n",
      "Epoch: 0128 loss_train (RMSE): 0.0891 loss_val (RMSE): 0.3031\n",
      "Epoch: 0129 loss_train (RMSE): 0.0891 loss_val (RMSE): 0.3032\n",
      "Epoch: 0130 loss_train (RMSE): 0.0891 loss_val (RMSE): 0.3032\n",
      "Epoch: 0131 loss_train (RMSE): 0.0891 loss_val (RMSE): 0.3033\n",
      "Epoch: 0132 loss_train (RMSE): 0.0890 loss_val (RMSE): 0.3034\n",
      "Epoch: 0133 loss_train (RMSE): 0.0890 loss_val (RMSE): 0.3034\n",
      "Epoch: 0134 loss_train (RMSE): 0.0890 loss_val (RMSE): 0.3035\n",
      "Epoch: 0135 loss_train (RMSE): 0.0890 loss_val (RMSE): 0.3035\n",
      "Epoch: 0136 loss_train (RMSE): 0.0890 loss_val (RMSE): 0.3036\n",
      "Epoch: 0137 loss_train (RMSE): 0.0890 loss_val (RMSE): 0.3036\n",
      "Epoch: 0138 loss_train (RMSE): 0.0890 loss_val (RMSE): 0.3037\n",
      "Epoch: 0139 loss_train (RMSE): 0.0890 loss_val (RMSE): 0.3037\n",
      "Epoch: 0140 loss_train (RMSE): 0.0890 loss_val (RMSE): 0.3037\n",
      "Epoch: 0141 loss_train (RMSE): 0.0890 loss_val (RMSE): 0.3038\n",
      "Epoch: 0142 loss_train (RMSE): 0.0890 loss_val (RMSE): 0.3038\n",
      "Epoch: 0143 loss_train (RMSE): 0.0890 loss_val (RMSE): 0.3039\n",
      "Epoch: 0144 loss_train (RMSE): 0.0890 loss_val (RMSE): 0.3039\n",
      "Epoch: 0145 loss_train (RMSE): 0.0890 loss_val (RMSE): 0.3039\n",
      "Epoch: 0146 loss_train (RMSE): 0.0890 loss_val (RMSE): 0.3040\n",
      "Epoch: 0147 loss_train (RMSE): 0.0890 loss_val (RMSE): 0.3040\n",
      "Epoch: 0148 loss_train (RMSE): 0.0890 loss_val (RMSE): 0.3040\n",
      "Epoch: 0149 loss_train (RMSE): 0.0890 loss_val (RMSE): 0.3040\n",
      "Epoch: 0150 loss_train (RMSE): 0.0890 loss_val (RMSE): 0.3041\n",
      "Epoch: 0151 loss_train (RMSE): 0.0890 loss_val (RMSE): 0.3041\n",
      "Epoch: 0152 loss_train (RMSE): 0.0890 loss_val (RMSE): 0.3041\n",
      "Epoch: 0153 loss_train (RMSE): 0.0890 loss_val (RMSE): 0.3041\n",
      "Epoch: 0154 loss_train (RMSE): 0.0890 loss_val (RMSE): 0.3041\n",
      "Epoch: 0155 loss_train (RMSE): 0.0890 loss_val (RMSE): 0.3042\n",
      "Epoch: 0156 loss_train (RMSE): 0.0890 loss_val (RMSE): 0.3042\n",
      "Epoch: 0157 loss_train (RMSE): 0.0890 loss_val (RMSE): 0.3042\n",
      "Epoch: 0158 loss_train (RMSE): 0.0890 loss_val (RMSE): 0.3042\n",
      "Epoch: 0159 loss_train (RMSE): 0.0890 loss_val (RMSE): 0.3042\n",
      "Epoch: 0160 loss_train (RMSE): 0.0890 loss_val (RMSE): 0.3042\n",
      "Epoch: 0161 loss_train (RMSE): 0.0890 loss_val (RMSE): 0.3042\n",
      "Epoch: 0162 loss_train (RMSE): 0.0890 loss_val (RMSE): 0.3042\n",
      "Epoch: 0163 loss_train (RMSE): 0.0890 loss_val (RMSE): 0.3043\n",
      "Epoch: 0164 loss_train (RMSE): 0.0890 loss_val (RMSE): 0.3043\n",
      "Epoch: 0165 loss_train (RMSE): 0.0890 loss_val (RMSE): 0.3043\n",
      "Epoch: 0166 loss_train (RMSE): 0.0890 loss_val (RMSE): 0.3043\n",
      "Epoch: 0167 loss_train (RMSE): 0.0890 loss_val (RMSE): 0.3043\n",
      "Epoch: 0168 loss_train (RMSE): 0.0890 loss_val (RMSE): 0.3043\n",
      "Epoch: 0169 loss_train (RMSE): 0.0890 loss_val (RMSE): 0.3043\n",
      "Epoch: 0170 loss_train (RMSE): 0.0890 loss_val (RMSE): 0.3043\n",
      "Epoch: 0171 loss_train (RMSE): 0.0890 loss_val (RMSE): 0.3043\n",
      "Epoch: 0172 loss_train (RMSE): 0.0890 loss_val (RMSE): 0.3043\n",
      "Epoch: 0173 loss_train (RMSE): 0.0890 loss_val (RMSE): 0.3043\n",
      "Epoch: 0174 loss_train (RMSE): 0.0890 loss_val (RMSE): 0.3043\n",
      "Epoch: 0175 loss_train (RMSE): 0.0890 loss_val (RMSE): 0.3043\n",
      "Epoch: 0176 loss_train (RMSE): 0.0890 loss_val (RMSE): 0.3043\n",
      "Epoch: 0177 loss_train (RMSE): 0.0890 loss_val (RMSE): 0.3043\n",
      "Epoch: 0178 loss_train (RMSE): 0.0890 loss_val (RMSE): 0.3043\n",
      "Epoch: 0179 loss_train (RMSE): 0.0890 loss_val (RMSE): 0.3043\n",
      "Epoch: 0180 loss_train (RMSE): 0.0890 loss_val (RMSE): 0.3043\n",
      "Epoch: 0181 loss_train (RMSE): 0.0890 loss_val (RMSE): 0.3043\n",
      "Epoch: 0182 loss_train (RMSE): 0.0890 loss_val (RMSE): 0.3043\n",
      "Epoch: 0183 loss_train (RMSE): 0.0890 loss_val (RMSE): 0.3043\n",
      "Epoch: 0184 loss_train (RMSE): 0.0890 loss_val (RMSE): 0.3043\n",
      "Epoch: 0185 loss_train (RMSE): 0.0890 loss_val (RMSE): 0.3043\n",
      "Epoch: 0186 loss_train (RMSE): 0.0890 loss_val (RMSE): 0.3043\n",
      "Epoch: 0187 loss_train (RMSE): 0.0890 loss_val (RMSE): 0.3043\n",
      "Epoch: 0188 loss_train (RMSE): 0.0890 loss_val (RMSE): 0.3043\n",
      "Epoch: 0189 loss_train (RMSE): 0.0890 loss_val (RMSE): 0.3043\n",
      "Epoch: 0190 loss_train (RMSE): 0.0890 loss_val (RMSE): 0.3043\n",
      "Epoch: 0191 loss_train (RMSE): 0.0890 loss_val (RMSE): 0.3043\n",
      "Epoch: 0192 loss_train (RMSE): 0.0890 loss_val (RMSE): 0.3043\n",
      "Epoch: 0193 loss_train (RMSE): 0.0890 loss_val (RMSE): 0.3043\n",
      "Epoch: 0194 loss_train (RMSE): 0.0890 loss_val (RMSE): 0.3043\n",
      "Epoch: 0195 loss_train (RMSE): 0.0890 loss_val (RMSE): 0.3043\n",
      "Epoch: 0196 loss_train (RMSE): 0.0890 loss_val (RMSE): 0.3043\n",
      "Epoch: 0197 loss_train (RMSE): 0.0890 loss_val (RMSE): 0.3043\n",
      "Epoch: 0198 loss_train (RMSE): 0.0890 loss_val (RMSE): 0.3043\n",
      "Epoch: 0199 loss_train (RMSE): 0.0890 loss_val (RMSE): 0.3043\n",
      "Epoch: 0200 loss_train (RMSE): 0.0890 loss_val (RMSE): 0.3043\n",
      "Optimization Finished!\n",
      "Total time elapsed: 0.4039s\n",
      "Test set results: loss= 0.2784 (RMSE)\n",
      "GCN(\n",
      "  (gc1): GraphConvolution (10 -> 2000)\n",
      "  (gc5): GraphConvolution (2000 -> 2)\n",
      ")\n",
      "Epoch: 0001 loss_train (RMSE): 0.0823 loss_val (RMSE): 0.3069\n",
      "Epoch: 0002 loss_train (RMSE): 0.0794 loss_val (RMSE): 0.3012\n",
      "Epoch: 0003 loss_train (RMSE): 0.0768 loss_val (RMSE): 0.2958\n",
      "Epoch: 0004 loss_train (RMSE): 0.0743 loss_val (RMSE): 0.2906\n",
      "Epoch: 0005 loss_train (RMSE): 0.0721 loss_val (RMSE): 0.2858\n",
      "Epoch: 0006 loss_train (RMSE): 0.0700 loss_val (RMSE): 0.2814\n",
      "Epoch: 0007 loss_train (RMSE): 0.0682 loss_val (RMSE): 0.2772\n",
      "Epoch: 0008 loss_train (RMSE): 0.0665 loss_val (RMSE): 0.2735\n",
      "Epoch: 0009 loss_train (RMSE): 0.0650 loss_val (RMSE): 0.2701\n",
      "Epoch: 0010 loss_train (RMSE): 0.0635 loss_val (RMSE): 0.2670\n",
      "Epoch: 0011 loss_train (RMSE): 0.0622 loss_val (RMSE): 0.2643\n",
      "Epoch: 0012 loss_train (RMSE): 0.0610 loss_val (RMSE): 0.2618\n",
      "Epoch: 0013 loss_train (RMSE): 0.0598 loss_val (RMSE): 0.2596\n",
      "Epoch: 0014 loss_train (RMSE): 0.0587 loss_val (RMSE): 0.2577\n",
      "Epoch: 0015 loss_train (RMSE): 0.0577 loss_val (RMSE): 0.2561\n",
      "Epoch: 0016 loss_train (RMSE): 0.0568 loss_val (RMSE): 0.2547\n",
      "Epoch: 0017 loss_train (RMSE): 0.0560 loss_val (RMSE): 0.2535\n",
      "Epoch: 0018 loss_train (RMSE): 0.0553 loss_val (RMSE): 0.2526\n",
      "Epoch: 0019 loss_train (RMSE): 0.0546 loss_val (RMSE): 0.2518\n",
      "Epoch: 0020 loss_train (RMSE): 0.0541 loss_val (RMSE): 0.2512\n",
      "Epoch: 0021 loss_train (RMSE): 0.0536 loss_val (RMSE): 0.2507\n",
      "Epoch: 0022 loss_train (RMSE): 0.0532 loss_val (RMSE): 0.2503\n",
      "Epoch: 0023 loss_train (RMSE): 0.0529 loss_val (RMSE): 0.2500\n",
      "Epoch: 0024 loss_train (RMSE): 0.0527 loss_val (RMSE): 0.2497\n",
      "Epoch: 0025 loss_train (RMSE): 0.0525 loss_val (RMSE): 0.2494\n",
      "Epoch: 0026 loss_train (RMSE): 0.0524 loss_val (RMSE): 0.2491\n",
      "Epoch: 0027 loss_train (RMSE): 0.0523 loss_val (RMSE): 0.2488\n",
      "Epoch: 0028 loss_train (RMSE): 0.0522 loss_val (RMSE): 0.2485\n",
      "Epoch: 0029 loss_train (RMSE): 0.0521 loss_val (RMSE): 0.2482\n",
      "Epoch: 0030 loss_train (RMSE): 0.0521 loss_val (RMSE): 0.2478\n",
      "Epoch: 0031 loss_train (RMSE): 0.0521 loss_val (RMSE): 0.2475\n",
      "Epoch: 0032 loss_train (RMSE): 0.0521 loss_val (RMSE): 0.2472\n",
      "Epoch: 0033 loss_train (RMSE): 0.0521 loss_val (RMSE): 0.2469\n",
      "Epoch: 0034 loss_train (RMSE): 0.0521 loss_val (RMSE): 0.2467\n",
      "Epoch: 0035 loss_train (RMSE): 0.0522 loss_val (RMSE): 0.2464\n",
      "Epoch: 0036 loss_train (RMSE): 0.0522 loss_val (RMSE): 0.2463\n",
      "Epoch: 0037 loss_train (RMSE): 0.0522 loss_val (RMSE): 0.2462\n",
      "Epoch: 0038 loss_train (RMSE): 0.0523 loss_val (RMSE): 0.2461\n",
      "Epoch: 0039 loss_train (RMSE): 0.0523 loss_val (RMSE): 0.2461\n",
      "Epoch: 0040 loss_train (RMSE): 0.0523 loss_val (RMSE): 0.2461\n",
      "Epoch: 0041 loss_train (RMSE): 0.0523 loss_val (RMSE): 0.2461\n",
      "Epoch: 0042 loss_train (RMSE): 0.0523 loss_val (RMSE): 0.2462\n",
      "Epoch: 0043 loss_train (RMSE): 0.0523 loss_val (RMSE): 0.2463\n",
      "Epoch: 0044 loss_train (RMSE): 0.0523 loss_val (RMSE): 0.2464\n",
      "Epoch: 0045 loss_train (RMSE): 0.0523 loss_val (RMSE): 0.2466\n",
      "Epoch: 0046 loss_train (RMSE): 0.0523 loss_val (RMSE): 0.2467\n",
      "Epoch: 0047 loss_train (RMSE): 0.0523 loss_val (RMSE): 0.2468\n",
      "Epoch: 0048 loss_train (RMSE): 0.0523 loss_val (RMSE): 0.2469\n",
      "Epoch: 0049 loss_train (RMSE): 0.0523 loss_val (RMSE): 0.2470\n",
      "Epoch: 0050 loss_train (RMSE): 0.0522 loss_val (RMSE): 0.2471\n",
      "Epoch: 0051 loss_train (RMSE): 0.0522 loss_val (RMSE): 0.2471\n",
      "Epoch: 0052 loss_train (RMSE): 0.0522 loss_val (RMSE): 0.2471\n",
      "Epoch: 0053 loss_train (RMSE): 0.0522 loss_val (RMSE): 0.2471\n",
      "Epoch: 0054 loss_train (RMSE): 0.0522 loss_val (RMSE): 0.2471\n",
      "Epoch: 0055 loss_train (RMSE): 0.0521 loss_val (RMSE): 0.2470\n",
      "Epoch: 0056 loss_train (RMSE): 0.0521 loss_val (RMSE): 0.2469\n",
      "Epoch: 0057 loss_train (RMSE): 0.0521 loss_val (RMSE): 0.2468\n",
      "Epoch: 0058 loss_train (RMSE): 0.0521 loss_val (RMSE): 0.2468\n",
      "Epoch: 0059 loss_train (RMSE): 0.0521 loss_val (RMSE): 0.2467\n",
      "Epoch: 0060 loss_train (RMSE): 0.0521 loss_val (RMSE): 0.2466\n",
      "Epoch: 0061 loss_train (RMSE): 0.0521 loss_val (RMSE): 0.2466\n",
      "Epoch: 0062 loss_train (RMSE): 0.0521 loss_val (RMSE): 0.2466\n",
      "Epoch: 0063 loss_train (RMSE): 0.0521 loss_val (RMSE): 0.2466\n",
      "Epoch: 0064 loss_train (RMSE): 0.0521 loss_val (RMSE): 0.2466\n",
      "Epoch: 0065 loss_train (RMSE): 0.0521 loss_val (RMSE): 0.2466\n",
      "Epoch: 0066 loss_train (RMSE): 0.0521 loss_val (RMSE): 0.2467\n",
      "Epoch: 0067 loss_train (RMSE): 0.0521 loss_val (RMSE): 0.2467\n",
      "Epoch: 0068 loss_train (RMSE): 0.0521 loss_val (RMSE): 0.2468\n",
      "Epoch: 0069 loss_train (RMSE): 0.0521 loss_val (RMSE): 0.2468\n",
      "Epoch: 0070 loss_train (RMSE): 0.0521 loss_val (RMSE): 0.2469\n",
      "Epoch: 0071 loss_train (RMSE): 0.0521 loss_val (RMSE): 0.2470\n",
      "Epoch: 0072 loss_train (RMSE): 0.0521 loss_val (RMSE): 0.2470\n",
      "Epoch: 0073 loss_train (RMSE): 0.0521 loss_val (RMSE): 0.2470\n",
      "Epoch: 0074 loss_train (RMSE): 0.0521 loss_val (RMSE): 0.2471\n",
      "Epoch: 0075 loss_train (RMSE): 0.0521 loss_val (RMSE): 0.2471\n",
      "Epoch: 0076 loss_train (RMSE): 0.0521 loss_val (RMSE): 0.2471\n",
      "Epoch: 0077 loss_train (RMSE): 0.0521 loss_val (RMSE): 0.2470\n",
      "Epoch: 0078 loss_train (RMSE): 0.0521 loss_val (RMSE): 0.2470\n",
      "Epoch: 0079 loss_train (RMSE): 0.0521 loss_val (RMSE): 0.2470\n",
      "Epoch: 0080 loss_train (RMSE): 0.0521 loss_val (RMSE): 0.2470\n",
      "Epoch: 0081 loss_train (RMSE): 0.0521 loss_val (RMSE): 0.2469\n",
      "Epoch: 0082 loss_train (RMSE): 0.0521 loss_val (RMSE): 0.2469\n",
      "Epoch: 0083 loss_train (RMSE): 0.0521 loss_val (RMSE): 0.2469\n",
      "Epoch: 0084 loss_train (RMSE): 0.0521 loss_val (RMSE): 0.2468\n",
      "Epoch: 0085 loss_train (RMSE): 0.0521 loss_val (RMSE): 0.2468\n",
      "Epoch: 0086 loss_train (RMSE): 0.0521 loss_val (RMSE): 0.2468\n",
      "Epoch: 0087 loss_train (RMSE): 0.0521 loss_val (RMSE): 0.2468\n",
      "Epoch: 0088 loss_train (RMSE): 0.0521 loss_val (RMSE): 0.2468\n",
      "Epoch: 0089 loss_train (RMSE): 0.0521 loss_val (RMSE): 0.2468\n",
      "Epoch: 0090 loss_train (RMSE): 0.0521 loss_val (RMSE): 0.2468\n",
      "Epoch: 0091 loss_train (RMSE): 0.0521 loss_val (RMSE): 0.2468\n",
      "Epoch: 0092 loss_train (RMSE): 0.0521 loss_val (RMSE): 0.2468\n",
      "Epoch: 0093 loss_train (RMSE): 0.0521 loss_val (RMSE): 0.2469\n",
      "Epoch: 0094 loss_train (RMSE): 0.0521 loss_val (RMSE): 0.2469\n",
      "Epoch: 0095 loss_train (RMSE): 0.0521 loss_val (RMSE): 0.2469\n",
      "Epoch: 0096 loss_train (RMSE): 0.0521 loss_val (RMSE): 0.2469\n",
      "Epoch: 0097 loss_train (RMSE): 0.0521 loss_val (RMSE): 0.2469\n",
      "Epoch: 0098 loss_train (RMSE): 0.0521 loss_val (RMSE): 0.2468\n",
      "Epoch: 0099 loss_train (RMSE): 0.0521 loss_val (RMSE): 0.2468\n",
      "Epoch: 0100 loss_train (RMSE): 0.0521 loss_val (RMSE): 0.2468\n",
      "Epoch: 0101 loss_train (RMSE): 0.0521 loss_val (RMSE): 0.2468\n",
      "Epoch: 0102 loss_train (RMSE): 0.0521 loss_val (RMSE): 0.2468\n",
      "Epoch: 0103 loss_train (RMSE): 0.0521 loss_val (RMSE): 0.2468\n",
      "Epoch: 0104 loss_train (RMSE): 0.0521 loss_val (RMSE): 0.2468\n",
      "Epoch: 0105 loss_train (RMSE): 0.0521 loss_val (RMSE): 0.2468\n",
      "Epoch: 0106 loss_train (RMSE): 0.0521 loss_val (RMSE): 0.2468\n",
      "Epoch: 0107 loss_train (RMSE): 0.0521 loss_val (RMSE): 0.2468\n",
      "Epoch: 0108 loss_train (RMSE): 0.0521 loss_val (RMSE): 0.2468\n",
      "Epoch: 0109 loss_train (RMSE): 0.0521 loss_val (RMSE): 0.2468\n",
      "Epoch: 0110 loss_train (RMSE): 0.0521 loss_val (RMSE): 0.2468\n",
      "Epoch: 0111 loss_train (RMSE): 0.0521 loss_val (RMSE): 0.2468\n",
      "Epoch: 0112 loss_train (RMSE): 0.0521 loss_val (RMSE): 0.2468\n",
      "Epoch: 0113 loss_train (RMSE): 0.0521 loss_val (RMSE): 0.2468\n",
      "Epoch: 0114 loss_train (RMSE): 0.0521 loss_val (RMSE): 0.2468\n",
      "Epoch: 0115 loss_train (RMSE): 0.0521 loss_val (RMSE): 0.2468\n",
      "Epoch: 0116 loss_train (RMSE): 0.0521 loss_val (RMSE): 0.2468\n",
      "Epoch: 0117 loss_train (RMSE): 0.0521 loss_val (RMSE): 0.2468\n",
      "Epoch: 0118 loss_train (RMSE): 0.0521 loss_val (RMSE): 0.2468\n",
      "Epoch: 0119 loss_train (RMSE): 0.0521 loss_val (RMSE): 0.2468\n",
      "Epoch: 0120 loss_train (RMSE): 0.0521 loss_val (RMSE): 0.2468\n",
      "Epoch: 0121 loss_train (RMSE): 0.0521 loss_val (RMSE): 0.2468\n",
      "Epoch: 0122 loss_train (RMSE): 0.0521 loss_val (RMSE): 0.2468\n",
      "Epoch: 0123 loss_train (RMSE): 0.0521 loss_val (RMSE): 0.2468\n",
      "Epoch: 0124 loss_train (RMSE): 0.0521 loss_val (RMSE): 0.2468\n",
      "Epoch: 0125 loss_train (RMSE): 0.0521 loss_val (RMSE): 0.2468\n",
      "Epoch: 0126 loss_train (RMSE): 0.0521 loss_val (RMSE): 0.2468\n",
      "Epoch: 0127 loss_train (RMSE): 0.0521 loss_val (RMSE): 0.2468\n",
      "Epoch: 0128 loss_train (RMSE): 0.0521 loss_val (RMSE): 0.2468\n",
      "Epoch: 0129 loss_train (RMSE): 0.0521 loss_val (RMSE): 0.2468\n",
      "Epoch: 0130 loss_train (RMSE): 0.0521 loss_val (RMSE): 0.2468\n",
      "Epoch: 0131 loss_train (RMSE): 0.0521 loss_val (RMSE): 0.2468\n",
      "Epoch: 0132 loss_train (RMSE): 0.0521 loss_val (RMSE): 0.2468\n",
      "Epoch: 0133 loss_train (RMSE): 0.0521 loss_val (RMSE): 0.2468\n",
      "Epoch: 0134 loss_train (RMSE): 0.0521 loss_val (RMSE): 0.2468\n",
      "Epoch: 0135 loss_train (RMSE): 0.0521 loss_val (RMSE): 0.2468\n",
      "Epoch: 0136 loss_train (RMSE): 0.0521 loss_val (RMSE): 0.2468\n",
      "Epoch: 0137 loss_train (RMSE): 0.0521 loss_val (RMSE): 0.2468\n",
      "Epoch: 0138 loss_train (RMSE): 0.0521 loss_val (RMSE): 0.2468\n",
      "Epoch: 0139 loss_train (RMSE): 0.0521 loss_val (RMSE): 0.2468\n",
      "Epoch: 0140 loss_train (RMSE): 0.0521 loss_val (RMSE): 0.2468\n",
      "Epoch: 0141 loss_train (RMSE): 0.0521 loss_val (RMSE): 0.2468\n",
      "Epoch: 0142 loss_train (RMSE): 0.0521 loss_val (RMSE): 0.2468\n",
      "Epoch: 0143 loss_train (RMSE): 0.0521 loss_val (RMSE): 0.2468\n",
      "Epoch: 0144 loss_train (RMSE): 0.0521 loss_val (RMSE): 0.2468\n",
      "Epoch: 0145 loss_train (RMSE): 0.0521 loss_val (RMSE): 0.2468\n",
      "Epoch: 0146 loss_train (RMSE): 0.0521 loss_val (RMSE): 0.2468\n",
      "Epoch: 0147 loss_train (RMSE): 0.0521 loss_val (RMSE): 0.2468\n",
      "Epoch: 0148 loss_train (RMSE): 0.0521 loss_val (RMSE): 0.2468\n",
      "Epoch: 0149 loss_train (RMSE): 0.0521 loss_val (RMSE): 0.2468\n",
      "Epoch: 0150 loss_train (RMSE): 0.0521 loss_val (RMSE): 0.2468\n",
      "Epoch: 0151 loss_train (RMSE): 0.0521 loss_val (RMSE): 0.2468\n",
      "Epoch: 0152 loss_train (RMSE): 0.0521 loss_val (RMSE): 0.2468\n",
      "Epoch: 0153 loss_train (RMSE): 0.0521 loss_val (RMSE): 0.2468\n",
      "Epoch: 0154 loss_train (RMSE): 0.0521 loss_val (RMSE): 0.2468\n",
      "Epoch: 0155 loss_train (RMSE): 0.0521 loss_val (RMSE): 0.2468\n",
      "Epoch: 0156 loss_train (RMSE): 0.0521 loss_val (RMSE): 0.2468\n",
      "Epoch: 0157 loss_train (RMSE): 0.0521 loss_val (RMSE): 0.2468\n",
      "Epoch: 0158 loss_train (RMSE): 0.0521 loss_val (RMSE): 0.2468\n",
      "Epoch: 0159 loss_train (RMSE): 0.0521 loss_val (RMSE): 0.2468\n",
      "Epoch: 0160 loss_train (RMSE): 0.0521 loss_val (RMSE): 0.2468\n",
      "Epoch: 0161 loss_train (RMSE): 0.0521 loss_val (RMSE): 0.2468\n",
      "Epoch: 0162 loss_train (RMSE): 0.0521 loss_val (RMSE): 0.2468\n",
      "Epoch: 0163 loss_train (RMSE): 0.0521 loss_val (RMSE): 0.2468\n",
      "Epoch: 0164 loss_train (RMSE): 0.0521 loss_val (RMSE): 0.2468\n",
      "Epoch: 0165 loss_train (RMSE): 0.0521 loss_val (RMSE): 0.2468\n",
      "Epoch: 0166 loss_train (RMSE): 0.0521 loss_val (RMSE): 0.2468\n",
      "Epoch: 0167 loss_train (RMSE): 0.0521 loss_val (RMSE): 0.2468\n",
      "Epoch: 0168 loss_train (RMSE): 0.0521 loss_val (RMSE): 0.2468\n",
      "Epoch: 0169 loss_train (RMSE): 0.0521 loss_val (RMSE): 0.2468\n",
      "Epoch: 0170 loss_train (RMSE): 0.0521 loss_val (RMSE): 0.2468\n",
      "Epoch: 0171 loss_train (RMSE): 0.0521 loss_val (RMSE): 0.2468\n",
      "Epoch: 0172 loss_train (RMSE): 0.0521 loss_val (RMSE): 0.2468\n",
      "Epoch: 0173 loss_train (RMSE): 0.0521 loss_val (RMSE): 0.2468\n",
      "Epoch: 0174 loss_train (RMSE): 0.0521 loss_val (RMSE): 0.2468\n",
      "Epoch: 0175 loss_train (RMSE): 0.0521 loss_val (RMSE): 0.2468\n",
      "Epoch: 0176 loss_train (RMSE): 0.0521 loss_val (RMSE): 0.2468\n",
      "Epoch: 0177 loss_train (RMSE): 0.0521 loss_val (RMSE): 0.2468\n",
      "Epoch: 0178 loss_train (RMSE): 0.0521 loss_val (RMSE): 0.2468\n",
      "Epoch: 0179 loss_train (RMSE): 0.0521 loss_val (RMSE): 0.2468\n",
      "Epoch: 0180 loss_train (RMSE): 0.0521 loss_val (RMSE): 0.2468\n",
      "Epoch: 0181 loss_train (RMSE): 0.0521 loss_val (RMSE): 0.2468\n",
      "Epoch: 0182 loss_train (RMSE): 0.0521 loss_val (RMSE): 0.2468\n",
      "Epoch: 0183 loss_train (RMSE): 0.0521 loss_val (RMSE): 0.2468\n",
      "Epoch: 0184 loss_train (RMSE): 0.0521 loss_val (RMSE): 0.2468\n",
      "Epoch: 0185 loss_train (RMSE): 0.0521 loss_val (RMSE): 0.2468\n",
      "Epoch: 0186 loss_train (RMSE): 0.0521 loss_val (RMSE): 0.2468\n",
      "Epoch: 0187 loss_train (RMSE): 0.0521 loss_val (RMSE): 0.2468\n",
      "Epoch: 0188 loss_train (RMSE): 0.0521 loss_val (RMSE): 0.2468\n",
      "Epoch: 0189 loss_train (RMSE): 0.0521 loss_val (RMSE): 0.2468\n",
      "Epoch: 0190 loss_train (RMSE): 0.0521 loss_val (RMSE): 0.2468\n",
      "Epoch: 0191 loss_train (RMSE): 0.0521 loss_val (RMSE): 0.2468\n",
      "Epoch: 0192 loss_train (RMSE): 0.0521 loss_val (RMSE): 0.2468\n",
      "Epoch: 0193 loss_train (RMSE): 0.0521 loss_val (RMSE): 0.2468\n",
      "Epoch: 0194 loss_train (RMSE): 0.0521 loss_val (RMSE): 0.2468\n",
      "Epoch: 0195 loss_train (RMSE): 0.0521 loss_val (RMSE): 0.2468\n",
      "Epoch: 0196 loss_train (RMSE): 0.0521 loss_val (RMSE): 0.2468\n",
      "Epoch: 0197 loss_train (RMSE): 0.0521 loss_val (RMSE): 0.2468\n",
      "Epoch: 0198 loss_train (RMSE): 0.0521 loss_val (RMSE): 0.2468\n",
      "Epoch: 0199 loss_train (RMSE): 0.0521 loss_val (RMSE): 0.2468\n",
      "Epoch: 0200 loss_train (RMSE): 0.0521 loss_val (RMSE): 0.2468\n",
      "Optimization Finished!\n",
      "Total time elapsed: 0.4249s\n",
      "Test set results: loss= 0.2637 (RMSE)\n",
      "GCN(\n",
      "  (gc1): GraphConvolution (10 -> 2000)\n",
      "  (gc5): GraphConvolution (2000 -> 2)\n",
      ")\n",
      "Epoch: 0001 loss_train (RMSE): 0.7682 loss_val (RMSE): 0.8643\n",
      "Epoch: 0002 loss_train (RMSE): 0.7533 loss_val (RMSE): 0.8558\n",
      "Epoch: 0003 loss_train (RMSE): 0.7386 loss_val (RMSE): 0.8473\n",
      "Epoch: 0004 loss_train (RMSE): 0.7241 loss_val (RMSE): 0.8388\n",
      "Epoch: 0005 loss_train (RMSE): 0.7098 loss_val (RMSE): 0.8304\n",
      "Epoch: 0006 loss_train (RMSE): 0.6957 loss_val (RMSE): 0.8220\n",
      "Epoch: 0007 loss_train (RMSE): 0.6818 loss_val (RMSE): 0.8137\n",
      "Epoch: 0008 loss_train (RMSE): 0.6681 loss_val (RMSE): 0.8055\n",
      "Epoch: 0009 loss_train (RMSE): 0.6547 loss_val (RMSE): 0.7972\n",
      "Epoch: 0010 loss_train (RMSE): 0.6415 loss_val (RMSE): 0.7891\n",
      "Epoch: 0011 loss_train (RMSE): 0.6285 loss_val (RMSE): 0.7810\n",
      "Epoch: 0012 loss_train (RMSE): 0.6158 loss_val (RMSE): 0.7730\n",
      "Epoch: 0013 loss_train (RMSE): 0.6033 loss_val (RMSE): 0.7650\n",
      "Epoch: 0014 loss_train (RMSE): 0.5910 loss_val (RMSE): 0.7571\n",
      "Epoch: 0015 loss_train (RMSE): 0.5789 loss_val (RMSE): 0.7493\n",
      "Epoch: 0016 loss_train (RMSE): 0.5671 loss_val (RMSE): 0.7415\n",
      "Epoch: 0017 loss_train (RMSE): 0.5555 loss_val (RMSE): 0.7338\n",
      "Epoch: 0018 loss_train (RMSE): 0.5441 loss_val (RMSE): 0.7262\n",
      "Epoch: 0019 loss_train (RMSE): 0.5330 loss_val (RMSE): 0.7187\n",
      "Epoch: 0020 loss_train (RMSE): 0.5221 loss_val (RMSE): 0.7113\n",
      "Epoch: 0021 loss_train (RMSE): 0.5114 loss_val (RMSE): 0.7039\n",
      "Epoch: 0022 loss_train (RMSE): 0.5010 loss_val (RMSE): 0.6966\n",
      "Epoch: 0023 loss_train (RMSE): 0.4907 loss_val (RMSE): 0.6894\n",
      "Epoch: 0024 loss_train (RMSE): 0.4807 loss_val (RMSE): 0.6823\n",
      "Epoch: 0025 loss_train (RMSE): 0.4709 loss_val (RMSE): 0.6753\n",
      "Epoch: 0026 loss_train (RMSE): 0.4614 loss_val (RMSE): 0.6683\n",
      "Epoch: 0027 loss_train (RMSE): 0.4520 loss_val (RMSE): 0.6615\n",
      "Epoch: 0028 loss_train (RMSE): 0.4429 loss_val (RMSE): 0.6547\n",
      "Epoch: 0029 loss_train (RMSE): 0.4340 loss_val (RMSE): 0.6480\n",
      "Epoch: 0030 loss_train (RMSE): 0.4253 loss_val (RMSE): 0.6415\n",
      "Epoch: 0031 loss_train (RMSE): 0.4167 loss_val (RMSE): 0.6350\n",
      "Epoch: 0032 loss_train (RMSE): 0.4084 loss_val (RMSE): 0.6285\n",
      "Epoch: 0033 loss_train (RMSE): 0.4003 loss_val (RMSE): 0.6222\n",
      "Epoch: 0034 loss_train (RMSE): 0.3924 loss_val (RMSE): 0.6160\n",
      "Epoch: 0035 loss_train (RMSE): 0.3846 loss_val (RMSE): 0.6098\n",
      "Epoch: 0036 loss_train (RMSE): 0.3770 loss_val (RMSE): 0.6037\n",
      "Epoch: 0037 loss_train (RMSE): 0.3697 loss_val (RMSE): 0.5977\n",
      "Epoch: 0038 loss_train (RMSE): 0.3625 loss_val (RMSE): 0.5918\n",
      "Epoch: 0039 loss_train (RMSE): 0.3554 loss_val (RMSE): 0.5860\n",
      "Epoch: 0040 loss_train (RMSE): 0.3485 loss_val (RMSE): 0.5802\n",
      "Epoch: 0041 loss_train (RMSE): 0.3418 loss_val (RMSE): 0.5746\n",
      "Epoch: 0042 loss_train (RMSE): 0.3353 loss_val (RMSE): 0.5690\n",
      "Epoch: 0043 loss_train (RMSE): 0.3289 loss_val (RMSE): 0.5634\n",
      "Epoch: 0044 loss_train (RMSE): 0.3226 loss_val (RMSE): 0.5580\n",
      "Epoch: 0045 loss_train (RMSE): 0.3165 loss_val (RMSE): 0.5526\n",
      "Epoch: 0046 loss_train (RMSE): 0.3105 loss_val (RMSE): 0.5473\n",
      "Epoch: 0047 loss_train (RMSE): 0.3047 loss_val (RMSE): 0.5421\n",
      "Epoch: 0048 loss_train (RMSE): 0.2990 loss_val (RMSE): 0.5369\n",
      "Epoch: 0049 loss_train (RMSE): 0.2935 loss_val (RMSE): 0.5318\n",
      "Epoch: 0050 loss_train (RMSE): 0.2880 loss_val (RMSE): 0.5268\n",
      "Epoch: 0051 loss_train (RMSE): 0.2827 loss_val (RMSE): 0.5218\n",
      "Epoch: 0052 loss_train (RMSE): 0.2775 loss_val (RMSE): 0.5169\n",
      "Epoch: 0053 loss_train (RMSE): 0.2724 loss_val (RMSE): 0.5120\n",
      "Epoch: 0054 loss_train (RMSE): 0.2675 loss_val (RMSE): 0.5073\n",
      "Epoch: 0055 loss_train (RMSE): 0.2626 loss_val (RMSE): 0.5025\n",
      "Epoch: 0056 loss_train (RMSE): 0.2579 loss_val (RMSE): 0.4979\n",
      "Epoch: 0057 loss_train (RMSE): 0.2532 loss_val (RMSE): 0.4933\n",
      "Epoch: 0058 loss_train (RMSE): 0.2487 loss_val (RMSE): 0.4887\n",
      "Epoch: 0059 loss_train (RMSE): 0.2443 loss_val (RMSE): 0.4843\n",
      "Epoch: 0060 loss_train (RMSE): 0.2399 loss_val (RMSE): 0.4798\n",
      "Epoch: 0061 loss_train (RMSE): 0.2357 loss_val (RMSE): 0.4755\n",
      "Epoch: 0062 loss_train (RMSE): 0.2316 loss_val (RMSE): 0.4712\n",
      "Epoch: 0063 loss_train (RMSE): 0.2275 loss_val (RMSE): 0.4669\n",
      "Epoch: 0064 loss_train (RMSE): 0.2235 loss_val (RMSE): 0.4627\n",
      "Epoch: 0065 loss_train (RMSE): 0.2197 loss_val (RMSE): 0.4586\n",
      "Epoch: 0066 loss_train (RMSE): 0.2159 loss_val (RMSE): 0.4545\n",
      "Epoch: 0067 loss_train (RMSE): 0.2122 loss_val (RMSE): 0.4504\n",
      "Epoch: 0068 loss_train (RMSE): 0.2086 loss_val (RMSE): 0.4465\n",
      "Epoch: 0069 loss_train (RMSE): 0.2050 loss_val (RMSE): 0.4425\n",
      "Epoch: 0070 loss_train (RMSE): 0.2016 loss_val (RMSE): 0.4387\n",
      "Epoch: 0071 loss_train (RMSE): 0.1982 loss_val (RMSE): 0.4348\n",
      "Epoch: 0072 loss_train (RMSE): 0.1949 loss_val (RMSE): 0.4311\n",
      "Epoch: 0073 loss_train (RMSE): 0.1917 loss_val (RMSE): 0.4274\n",
      "Epoch: 0074 loss_train (RMSE): 0.1886 loss_val (RMSE): 0.4237\n",
      "Epoch: 0075 loss_train (RMSE): 0.1855 loss_val (RMSE): 0.4201\n",
      "Epoch: 0076 loss_train (RMSE): 0.1825 loss_val (RMSE): 0.4166\n",
      "Epoch: 0077 loss_train (RMSE): 0.1796 loss_val (RMSE): 0.4131\n",
      "Epoch: 0078 loss_train (RMSE): 0.1767 loss_val (RMSE): 0.4097\n",
      "Epoch: 0079 loss_train (RMSE): 0.1739 loss_val (RMSE): 0.4063\n",
      "Epoch: 0080 loss_train (RMSE): 0.1712 loss_val (RMSE): 0.4030\n",
      "Epoch: 0081 loss_train (RMSE): 0.1686 loss_val (RMSE): 0.3997\n",
      "Epoch: 0082 loss_train (RMSE): 0.1660 loss_val (RMSE): 0.3965\n",
      "Epoch: 0083 loss_train (RMSE): 0.1635 loss_val (RMSE): 0.3933\n",
      "Epoch: 0084 loss_train (RMSE): 0.1610 loss_val (RMSE): 0.3902\n",
      "Epoch: 0085 loss_train (RMSE): 0.1586 loss_val (RMSE): 0.3872\n",
      "Epoch: 0086 loss_train (RMSE): 0.1563 loss_val (RMSE): 0.3842\n",
      "Epoch: 0087 loss_train (RMSE): 0.1540 loss_val (RMSE): 0.3813\n",
      "Epoch: 0088 loss_train (RMSE): 0.1518 loss_val (RMSE): 0.3784\n",
      "Epoch: 0089 loss_train (RMSE): 0.1497 loss_val (RMSE): 0.3755\n",
      "Epoch: 0090 loss_train (RMSE): 0.1476 loss_val (RMSE): 0.3728\n",
      "Epoch: 0091 loss_train (RMSE): 0.1455 loss_val (RMSE): 0.3700\n",
      "Epoch: 0092 loss_train (RMSE): 0.1435 loss_val (RMSE): 0.3674\n",
      "Epoch: 0093 loss_train (RMSE): 0.1416 loss_val (RMSE): 0.3647\n",
      "Epoch: 0094 loss_train (RMSE): 0.1397 loss_val (RMSE): 0.3622\n",
      "Epoch: 0095 loss_train (RMSE): 0.1379 loss_val (RMSE): 0.3597\n",
      "Epoch: 0096 loss_train (RMSE): 0.1361 loss_val (RMSE): 0.3572\n",
      "Epoch: 0097 loss_train (RMSE): 0.1344 loss_val (RMSE): 0.3548\n",
      "Epoch: 0098 loss_train (RMSE): 0.1327 loss_val (RMSE): 0.3524\n",
      "Epoch: 0099 loss_train (RMSE): 0.1311 loss_val (RMSE): 0.3501\n",
      "Epoch: 0100 loss_train (RMSE): 0.1295 loss_val (RMSE): 0.3479\n",
      "Epoch: 0101 loss_train (RMSE): 0.1279 loss_val (RMSE): 0.3457\n",
      "Epoch: 0102 loss_train (RMSE): 0.1264 loss_val (RMSE): 0.3435\n",
      "Epoch: 0103 loss_train (RMSE): 0.1250 loss_val (RMSE): 0.3414\n",
      "Epoch: 0104 loss_train (RMSE): 0.1236 loss_val (RMSE): 0.3393\n",
      "Epoch: 0105 loss_train (RMSE): 0.1222 loss_val (RMSE): 0.3373\n",
      "Epoch: 0106 loss_train (RMSE): 0.1209 loss_val (RMSE): 0.3354\n",
      "Epoch: 0107 loss_train (RMSE): 0.1196 loss_val (RMSE): 0.3334\n",
      "Epoch: 0108 loss_train (RMSE): 0.1183 loss_val (RMSE): 0.3316\n",
      "Epoch: 0109 loss_train (RMSE): 0.1171 loss_val (RMSE): 0.3297\n",
      "Epoch: 0110 loss_train (RMSE): 0.1159 loss_val (RMSE): 0.3280\n",
      "Epoch: 0111 loss_train (RMSE): 0.1148 loss_val (RMSE): 0.3262\n",
      "Epoch: 0112 loss_train (RMSE): 0.1137 loss_val (RMSE): 0.3245\n",
      "Epoch: 0113 loss_train (RMSE): 0.1126 loss_val (RMSE): 0.3229\n",
      "Epoch: 0114 loss_train (RMSE): 0.1116 loss_val (RMSE): 0.3213\n",
      "Epoch: 0115 loss_train (RMSE): 0.1106 loss_val (RMSE): 0.3197\n",
      "Epoch: 0116 loss_train (RMSE): 0.1096 loss_val (RMSE): 0.3182\n",
      "Epoch: 0117 loss_train (RMSE): 0.1086 loss_val (RMSE): 0.3167\n",
      "Epoch: 0118 loss_train (RMSE): 0.1077 loss_val (RMSE): 0.3152\n",
      "Epoch: 0119 loss_train (RMSE): 0.1068 loss_val (RMSE): 0.3138\n",
      "Epoch: 0120 loss_train (RMSE): 0.1060 loss_val (RMSE): 0.3125\n",
      "Epoch: 0121 loss_train (RMSE): 0.1051 loss_val (RMSE): 0.3111\n",
      "Epoch: 0122 loss_train (RMSE): 0.1043 loss_val (RMSE): 0.3098\n",
      "Epoch: 0123 loss_train (RMSE): 0.1036 loss_val (RMSE): 0.3086\n",
      "Epoch: 0124 loss_train (RMSE): 0.1028 loss_val (RMSE): 0.3074\n",
      "Epoch: 0125 loss_train (RMSE): 0.1021 loss_val (RMSE): 0.3062\n",
      "Epoch: 0126 loss_train (RMSE): 0.1014 loss_val (RMSE): 0.3050\n",
      "Epoch: 0127 loss_train (RMSE): 0.1007 loss_val (RMSE): 0.3039\n",
      "Epoch: 0128 loss_train (RMSE): 0.1000 loss_val (RMSE): 0.3028\n",
      "Epoch: 0129 loss_train (RMSE): 0.0994 loss_val (RMSE): 0.3018\n",
      "Epoch: 0130 loss_train (RMSE): 0.0988 loss_val (RMSE): 0.3008\n",
      "Epoch: 0131 loss_train (RMSE): 0.0982 loss_val (RMSE): 0.2998\n",
      "Epoch: 0132 loss_train (RMSE): 0.0976 loss_val (RMSE): 0.2988\n",
      "Epoch: 0133 loss_train (RMSE): 0.0971 loss_val (RMSE): 0.2979\n",
      "Epoch: 0134 loss_train (RMSE): 0.0965 loss_val (RMSE): 0.2970\n",
      "Epoch: 0135 loss_train (RMSE): 0.0960 loss_val (RMSE): 0.2961\n",
      "Epoch: 0136 loss_train (RMSE): 0.0955 loss_val (RMSE): 0.2953\n",
      "Epoch: 0137 loss_train (RMSE): 0.0950 loss_val (RMSE): 0.2945\n",
      "Epoch: 0138 loss_train (RMSE): 0.0946 loss_val (RMSE): 0.2937\n",
      "Epoch: 0139 loss_train (RMSE): 0.0941 loss_val (RMSE): 0.2929\n",
      "Epoch: 0140 loss_train (RMSE): 0.0937 loss_val (RMSE): 0.2922\n",
      "Epoch: 0141 loss_train (RMSE): 0.0933 loss_val (RMSE): 0.2915\n",
      "Epoch: 0142 loss_train (RMSE): 0.0929 loss_val (RMSE): 0.2908\n",
      "Epoch: 0143 loss_train (RMSE): 0.0925 loss_val (RMSE): 0.2901\n",
      "Epoch: 0144 loss_train (RMSE): 0.0921 loss_val (RMSE): 0.2895\n",
      "Epoch: 0145 loss_train (RMSE): 0.0918 loss_val (RMSE): 0.2889\n",
      "Epoch: 0146 loss_train (RMSE): 0.0914 loss_val (RMSE): 0.2883\n",
      "Epoch: 0147 loss_train (RMSE): 0.0911 loss_val (RMSE): 0.2877\n",
      "Epoch: 0148 loss_train (RMSE): 0.0908 loss_val (RMSE): 0.2871\n",
      "Epoch: 0149 loss_train (RMSE): 0.0905 loss_val (RMSE): 0.2866\n",
      "Epoch: 0150 loss_train (RMSE): 0.0902 loss_val (RMSE): 0.2861\n",
      "Epoch: 0151 loss_train (RMSE): 0.0899 loss_val (RMSE): 0.2856\n",
      "Epoch: 0152 loss_train (RMSE): 0.0896 loss_val (RMSE): 0.2851\n",
      "Epoch: 0153 loss_train (RMSE): 0.0894 loss_val (RMSE): 0.2846\n",
      "Epoch: 0154 loss_train (RMSE): 0.0891 loss_val (RMSE): 0.2842\n",
      "Epoch: 0155 loss_train (RMSE): 0.0889 loss_val (RMSE): 0.2837\n",
      "Epoch: 0156 loss_train (RMSE): 0.0886 loss_val (RMSE): 0.2833\n",
      "Epoch: 0157 loss_train (RMSE): 0.0884 loss_val (RMSE): 0.2829\n",
      "Epoch: 0158 loss_train (RMSE): 0.0882 loss_val (RMSE): 0.2825\n",
      "Epoch: 0159 loss_train (RMSE): 0.0880 loss_val (RMSE): 0.2821\n",
      "Epoch: 0160 loss_train (RMSE): 0.0878 loss_val (RMSE): 0.2818\n",
      "Epoch: 0161 loss_train (RMSE): 0.0876 loss_val (RMSE): 0.2814\n",
      "Epoch: 0162 loss_train (RMSE): 0.0874 loss_val (RMSE): 0.2811\n",
      "Epoch: 0163 loss_train (RMSE): 0.0872 loss_val (RMSE): 0.2808\n",
      "Epoch: 0164 loss_train (RMSE): 0.0871 loss_val (RMSE): 0.2805\n",
      "Epoch: 0165 loss_train (RMSE): 0.0869 loss_val (RMSE): 0.2802\n",
      "Epoch: 0166 loss_train (RMSE): 0.0868 loss_val (RMSE): 0.2799\n",
      "Epoch: 0167 loss_train (RMSE): 0.0866 loss_val (RMSE): 0.2796\n",
      "Epoch: 0168 loss_train (RMSE): 0.0865 loss_val (RMSE): 0.2794\n",
      "Epoch: 0169 loss_train (RMSE): 0.0863 loss_val (RMSE): 0.2791\n",
      "Epoch: 0170 loss_train (RMSE): 0.0862 loss_val (RMSE): 0.2789\n",
      "Epoch: 0171 loss_train (RMSE): 0.0861 loss_val (RMSE): 0.2786\n",
      "Epoch: 0172 loss_train (RMSE): 0.0860 loss_val (RMSE): 0.2784\n",
      "Epoch: 0173 loss_train (RMSE): 0.0858 loss_val (RMSE): 0.2782\n",
      "Epoch: 0174 loss_train (RMSE): 0.0857 loss_val (RMSE): 0.2780\n",
      "Epoch: 0175 loss_train (RMSE): 0.0856 loss_val (RMSE): 0.2778\n",
      "Epoch: 0176 loss_train (RMSE): 0.0855 loss_val (RMSE): 0.2776\n",
      "Epoch: 0177 loss_train (RMSE): 0.0854 loss_val (RMSE): 0.2774\n",
      "Epoch: 0178 loss_train (RMSE): 0.0853 loss_val (RMSE): 0.2773\n",
      "Epoch: 0179 loss_train (RMSE): 0.0853 loss_val (RMSE): 0.2771\n",
      "Epoch: 0180 loss_train (RMSE): 0.0852 loss_val (RMSE): 0.2769\n",
      "Epoch: 0181 loss_train (RMSE): 0.0851 loss_val (RMSE): 0.2768\n",
      "Epoch: 0182 loss_train (RMSE): 0.0850 loss_val (RMSE): 0.2766\n",
      "Epoch: 0183 loss_train (RMSE): 0.0849 loss_val (RMSE): 0.2765\n",
      "Epoch: 0184 loss_train (RMSE): 0.0849 loss_val (RMSE): 0.2764\n",
      "Epoch: 0185 loss_train (RMSE): 0.0848 loss_val (RMSE): 0.2762\n",
      "Epoch: 0186 loss_train (RMSE): 0.0847 loss_val (RMSE): 0.2761\n",
      "Epoch: 0187 loss_train (RMSE): 0.0847 loss_val (RMSE): 0.2760\n",
      "Epoch: 0188 loss_train (RMSE): 0.0846 loss_val (RMSE): 0.2759\n",
      "Epoch: 0189 loss_train (RMSE): 0.0846 loss_val (RMSE): 0.2758\n",
      "Epoch: 0190 loss_train (RMSE): 0.0845 loss_val (RMSE): 0.2757\n",
      "Epoch: 0191 loss_train (RMSE): 0.0845 loss_val (RMSE): 0.2756\n",
      "Epoch: 0192 loss_train (RMSE): 0.0844 loss_val (RMSE): 0.2755\n",
      "Epoch: 0193 loss_train (RMSE): 0.0844 loss_val (RMSE): 0.2754\n",
      "Epoch: 0194 loss_train (RMSE): 0.0843 loss_val (RMSE): 0.2753\n",
      "Epoch: 0195 loss_train (RMSE): 0.0843 loss_val (RMSE): 0.2752\n",
      "Epoch: 0196 loss_train (RMSE): 0.0843 loss_val (RMSE): 0.2751\n",
      "Epoch: 0197 loss_train (RMSE): 0.0842 loss_val (RMSE): 0.2751\n",
      "Epoch: 0198 loss_train (RMSE): 0.0842 loss_val (RMSE): 0.2750\n",
      "Epoch: 0199 loss_train (RMSE): 0.0841 loss_val (RMSE): 0.2749\n",
      "Epoch: 0200 loss_train (RMSE): 0.0841 loss_val (RMSE): 0.2749\n",
      "Optimization Finished!\n",
      "Total time elapsed: 0.3926s\n",
      "Test set results: loss= 0.3189 (RMSE)\n",
      "GCN(\n",
      "  (gc1): GraphConvolution (10 -> 2000)\n",
      "  (gc5): GraphConvolution (2000 -> 2)\n",
      ")\n",
      "Epoch: 0001 loss_train (RMSE): 0.3496 loss_val (RMSE): 0.6609\n",
      "Epoch: 0002 loss_train (RMSE): 0.3409 loss_val (RMSE): 0.6530\n",
      "Epoch: 0003 loss_train (RMSE): 0.3324 loss_val (RMSE): 0.6451\n",
      "Epoch: 0004 loss_train (RMSE): 0.3241 loss_val (RMSE): 0.6372\n",
      "Epoch: 0005 loss_train (RMSE): 0.3160 loss_val (RMSE): 0.6295\n",
      "Epoch: 0006 loss_train (RMSE): 0.3081 loss_val (RMSE): 0.6218\n",
      "Epoch: 0007 loss_train (RMSE): 0.3005 loss_val (RMSE): 0.6143\n",
      "Epoch: 0008 loss_train (RMSE): 0.2930 loss_val (RMSE): 0.6068\n",
      "Epoch: 0009 loss_train (RMSE): 0.2858 loss_val (RMSE): 0.5994\n",
      "Epoch: 0010 loss_train (RMSE): 0.2788 loss_val (RMSE): 0.5922\n",
      "Epoch: 0011 loss_train (RMSE): 0.2720 loss_val (RMSE): 0.5850\n",
      "Epoch: 0012 loss_train (RMSE): 0.2655 loss_val (RMSE): 0.5779\n",
      "Epoch: 0013 loss_train (RMSE): 0.2591 loss_val (RMSE): 0.5710\n",
      "Epoch: 0014 loss_train (RMSE): 0.2529 loss_val (RMSE): 0.5641\n",
      "Epoch: 0015 loss_train (RMSE): 0.2470 loss_val (RMSE): 0.5574\n",
      "Epoch: 0016 loss_train (RMSE): 0.2413 loss_val (RMSE): 0.5508\n",
      "Epoch: 0017 loss_train (RMSE): 0.2357 loss_val (RMSE): 0.5443\n",
      "Epoch: 0018 loss_train (RMSE): 0.2303 loss_val (RMSE): 0.5380\n",
      "Epoch: 0019 loss_train (RMSE): 0.2251 loss_val (RMSE): 0.5317\n",
      "Epoch: 0020 loss_train (RMSE): 0.2201 loss_val (RMSE): 0.5255\n",
      "Epoch: 0021 loss_train (RMSE): 0.2153 loss_val (RMSE): 0.5195\n",
      "Epoch: 0022 loss_train (RMSE): 0.2106 loss_val (RMSE): 0.5136\n",
      "Epoch: 0023 loss_train (RMSE): 0.2060 loss_val (RMSE): 0.5077\n",
      "Epoch: 0024 loss_train (RMSE): 0.2016 loss_val (RMSE): 0.5020\n",
      "Epoch: 0025 loss_train (RMSE): 0.1973 loss_val (RMSE): 0.4964\n",
      "Epoch: 0026 loss_train (RMSE): 0.1931 loss_val (RMSE): 0.4908\n",
      "Epoch: 0027 loss_train (RMSE): 0.1891 loss_val (RMSE): 0.4854\n",
      "Epoch: 0028 loss_train (RMSE): 0.1852 loss_val (RMSE): 0.4800\n",
      "Epoch: 0029 loss_train (RMSE): 0.1813 loss_val (RMSE): 0.4748\n",
      "Epoch: 0030 loss_train (RMSE): 0.1776 loss_val (RMSE): 0.4696\n",
      "Epoch: 0031 loss_train (RMSE): 0.1740 loss_val (RMSE): 0.4645\n",
      "Epoch: 0032 loss_train (RMSE): 0.1705 loss_val (RMSE): 0.4595\n",
      "Epoch: 0033 loss_train (RMSE): 0.1671 loss_val (RMSE): 0.4546\n",
      "Epoch: 0034 loss_train (RMSE): 0.1638 loss_val (RMSE): 0.4498\n",
      "Epoch: 0035 loss_train (RMSE): 0.1605 loss_val (RMSE): 0.4450\n",
      "Epoch: 0036 loss_train (RMSE): 0.1574 loss_val (RMSE): 0.4404\n",
      "Epoch: 0037 loss_train (RMSE): 0.1544 loss_val (RMSE): 0.4358\n",
      "Epoch: 0038 loss_train (RMSE): 0.1514 loss_val (RMSE): 0.4314\n",
      "Epoch: 0039 loss_train (RMSE): 0.1486 loss_val (RMSE): 0.4270\n",
      "Epoch: 0040 loss_train (RMSE): 0.1458 loss_val (RMSE): 0.4227\n",
      "Epoch: 0041 loss_train (RMSE): 0.1432 loss_val (RMSE): 0.4185\n",
      "Epoch: 0042 loss_train (RMSE): 0.1406 loss_val (RMSE): 0.4144\n",
      "Epoch: 0043 loss_train (RMSE): 0.1381 loss_val (RMSE): 0.4104\n",
      "Epoch: 0044 loss_train (RMSE): 0.1358 loss_val (RMSE): 0.4065\n",
      "Epoch: 0045 loss_train (RMSE): 0.1335 loss_val (RMSE): 0.4026\n",
      "Epoch: 0046 loss_train (RMSE): 0.1313 loss_val (RMSE): 0.3989\n",
      "Epoch: 0047 loss_train (RMSE): 0.1292 loss_val (RMSE): 0.3952\n",
      "Epoch: 0048 loss_train (RMSE): 0.1271 loss_val (RMSE): 0.3917\n",
      "Epoch: 0049 loss_train (RMSE): 0.1252 loss_val (RMSE): 0.3882\n",
      "Epoch: 0050 loss_train (RMSE): 0.1233 loss_val (RMSE): 0.3848\n",
      "Epoch: 0051 loss_train (RMSE): 0.1216 loss_val (RMSE): 0.3814\n",
      "Epoch: 0052 loss_train (RMSE): 0.1199 loss_val (RMSE): 0.3782\n",
      "Epoch: 0053 loss_train (RMSE): 0.1182 loss_val (RMSE): 0.3750\n",
      "Epoch: 0054 loss_train (RMSE): 0.1167 loss_val (RMSE): 0.3719\n",
      "Epoch: 0055 loss_train (RMSE): 0.1152 loss_val (RMSE): 0.3689\n",
      "Epoch: 0056 loss_train (RMSE): 0.1137 loss_val (RMSE): 0.3659\n",
      "Epoch: 0057 loss_train (RMSE): 0.1124 loss_val (RMSE): 0.3630\n",
      "Epoch: 0058 loss_train (RMSE): 0.1110 loss_val (RMSE): 0.3602\n",
      "Epoch: 0059 loss_train (RMSE): 0.1098 loss_val (RMSE): 0.3574\n",
      "Epoch: 0060 loss_train (RMSE): 0.1086 loss_val (RMSE): 0.3547\n",
      "Epoch: 0061 loss_train (RMSE): 0.1075 loss_val (RMSE): 0.3521\n",
      "Epoch: 0062 loss_train (RMSE): 0.1064 loss_val (RMSE): 0.3495\n",
      "Epoch: 0063 loss_train (RMSE): 0.1054 loss_val (RMSE): 0.3470\n",
      "Epoch: 0064 loss_train (RMSE): 0.1044 loss_val (RMSE): 0.3445\n",
      "Epoch: 0065 loss_train (RMSE): 0.1034 loss_val (RMSE): 0.3422\n",
      "Epoch: 0066 loss_train (RMSE): 0.1025 loss_val (RMSE): 0.3398\n",
      "Epoch: 0067 loss_train (RMSE): 0.1017 loss_val (RMSE): 0.3376\n",
      "Epoch: 0068 loss_train (RMSE): 0.1009 loss_val (RMSE): 0.3354\n",
      "Epoch: 0069 loss_train (RMSE): 0.1001 loss_val (RMSE): 0.3333\n",
      "Epoch: 0070 loss_train (RMSE): 0.0994 loss_val (RMSE): 0.3312\n",
      "Epoch: 0071 loss_train (RMSE): 0.0987 loss_val (RMSE): 0.3293\n",
      "Epoch: 0072 loss_train (RMSE): 0.0981 loss_val (RMSE): 0.3273\n",
      "Epoch: 0073 loss_train (RMSE): 0.0975 loss_val (RMSE): 0.3255\n",
      "Epoch: 0074 loss_train (RMSE): 0.0969 loss_val (RMSE): 0.3237\n",
      "Epoch: 0075 loss_train (RMSE): 0.0963 loss_val (RMSE): 0.3219\n",
      "Epoch: 0076 loss_train (RMSE): 0.0958 loss_val (RMSE): 0.3202\n",
      "Epoch: 0077 loss_train (RMSE): 0.0953 loss_val (RMSE): 0.3186\n",
      "Epoch: 0078 loss_train (RMSE): 0.0949 loss_val (RMSE): 0.3171\n",
      "Epoch: 0079 loss_train (RMSE): 0.0944 loss_val (RMSE): 0.3156\n",
      "Epoch: 0080 loss_train (RMSE): 0.0940 loss_val (RMSE): 0.3141\n",
      "Epoch: 0081 loss_train (RMSE): 0.0937 loss_val (RMSE): 0.3127\n",
      "Epoch: 0082 loss_train (RMSE): 0.0933 loss_val (RMSE): 0.3114\n",
      "Epoch: 0083 loss_train (RMSE): 0.0930 loss_val (RMSE): 0.3101\n",
      "Epoch: 0084 loss_train (RMSE): 0.0927 loss_val (RMSE): 0.3089\n",
      "Epoch: 0085 loss_train (RMSE): 0.0924 loss_val (RMSE): 0.3077\n",
      "Epoch: 0086 loss_train (RMSE): 0.0921 loss_val (RMSE): 0.3066\n",
      "Epoch: 0087 loss_train (RMSE): 0.0918 loss_val (RMSE): 0.3055\n",
      "Epoch: 0088 loss_train (RMSE): 0.0916 loss_val (RMSE): 0.3045\n",
      "Epoch: 0089 loss_train (RMSE): 0.0913 loss_val (RMSE): 0.3035\n",
      "Epoch: 0090 loss_train (RMSE): 0.0911 loss_val (RMSE): 0.3025\n",
      "Epoch: 0091 loss_train (RMSE): 0.0909 loss_val (RMSE): 0.3016\n",
      "Epoch: 0092 loss_train (RMSE): 0.0908 loss_val (RMSE): 0.3007\n",
      "Epoch: 0093 loss_train (RMSE): 0.0906 loss_val (RMSE): 0.2999\n",
      "Epoch: 0094 loss_train (RMSE): 0.0904 loss_val (RMSE): 0.2991\n",
      "Epoch: 0095 loss_train (RMSE): 0.0903 loss_val (RMSE): 0.2983\n",
      "Epoch: 0096 loss_train (RMSE): 0.0901 loss_val (RMSE): 0.2976\n",
      "Epoch: 0097 loss_train (RMSE): 0.0900 loss_val (RMSE): 0.2969\n",
      "Epoch: 0098 loss_train (RMSE): 0.0899 loss_val (RMSE): 0.2962\n",
      "Epoch: 0099 loss_train (RMSE): 0.0898 loss_val (RMSE): 0.2955\n",
      "Epoch: 0100 loss_train (RMSE): 0.0897 loss_val (RMSE): 0.2949\n",
      "Epoch: 0101 loss_train (RMSE): 0.0896 loss_val (RMSE): 0.2943\n",
      "Epoch: 0102 loss_train (RMSE): 0.0895 loss_val (RMSE): 0.2937\n",
      "Epoch: 0103 loss_train (RMSE): 0.0894 loss_val (RMSE): 0.2932\n",
      "Epoch: 0104 loss_train (RMSE): 0.0893 loss_val (RMSE): 0.2926\n",
      "Epoch: 0105 loss_train (RMSE): 0.0893 loss_val (RMSE): 0.2921\n",
      "Epoch: 0106 loss_train (RMSE): 0.0892 loss_val (RMSE): 0.2916\n",
      "Epoch: 0107 loss_train (RMSE): 0.0891 loss_val (RMSE): 0.2912\n",
      "Epoch: 0108 loss_train (RMSE): 0.0891 loss_val (RMSE): 0.2907\n",
      "Epoch: 0109 loss_train (RMSE): 0.0890 loss_val (RMSE): 0.2903\n",
      "Epoch: 0110 loss_train (RMSE): 0.0890 loss_val (RMSE): 0.2899\n",
      "Epoch: 0111 loss_train (RMSE): 0.0890 loss_val (RMSE): 0.2895\n",
      "Epoch: 0112 loss_train (RMSE): 0.0889 loss_val (RMSE): 0.2891\n",
      "Epoch: 0113 loss_train (RMSE): 0.0889 loss_val (RMSE): 0.2887\n",
      "Epoch: 0114 loss_train (RMSE): 0.0889 loss_val (RMSE): 0.2884\n",
      "Epoch: 0115 loss_train (RMSE): 0.0888 loss_val (RMSE): 0.2880\n",
      "Epoch: 0116 loss_train (RMSE): 0.0888 loss_val (RMSE): 0.2877\n",
      "Epoch: 0117 loss_train (RMSE): 0.0888 loss_val (RMSE): 0.2874\n",
      "Epoch: 0118 loss_train (RMSE): 0.0888 loss_val (RMSE): 0.2871\n",
      "Epoch: 0119 loss_train (RMSE): 0.0887 loss_val (RMSE): 0.2869\n",
      "Epoch: 0120 loss_train (RMSE): 0.0887 loss_val (RMSE): 0.2866\n",
      "Epoch: 0121 loss_train (RMSE): 0.0887 loss_val (RMSE): 0.2864\n",
      "Epoch: 0122 loss_train (RMSE): 0.0887 loss_val (RMSE): 0.2861\n",
      "Epoch: 0123 loss_train (RMSE): 0.0887 loss_val (RMSE): 0.2859\n",
      "Epoch: 0124 loss_train (RMSE): 0.0887 loss_val (RMSE): 0.2857\n",
      "Epoch: 0125 loss_train (RMSE): 0.0886 loss_val (RMSE): 0.2855\n",
      "Epoch: 0126 loss_train (RMSE): 0.0886 loss_val (RMSE): 0.2853\n",
      "Epoch: 0127 loss_train (RMSE): 0.0886 loss_val (RMSE): 0.2851\n",
      "Epoch: 0128 loss_train (RMSE): 0.0886 loss_val (RMSE): 0.2849\n",
      "Epoch: 0129 loss_train (RMSE): 0.0886 loss_val (RMSE): 0.2848\n",
      "Epoch: 0130 loss_train (RMSE): 0.0886 loss_val (RMSE): 0.2846\n",
      "Epoch: 0131 loss_train (RMSE): 0.0886 loss_val (RMSE): 0.2845\n",
      "Epoch: 0132 loss_train (RMSE): 0.0886 loss_val (RMSE): 0.2843\n",
      "Epoch: 0133 loss_train (RMSE): 0.0886 loss_val (RMSE): 0.2842\n",
      "Epoch: 0134 loss_train (RMSE): 0.0886 loss_val (RMSE): 0.2841\n",
      "Epoch: 0135 loss_train (RMSE): 0.0886 loss_val (RMSE): 0.2839\n",
      "Epoch: 0136 loss_train (RMSE): 0.0886 loss_val (RMSE): 0.2838\n",
      "Epoch: 0137 loss_train (RMSE): 0.0886 loss_val (RMSE): 0.2837\n",
      "Epoch: 0138 loss_train (RMSE): 0.0886 loss_val (RMSE): 0.2836\n",
      "Epoch: 0139 loss_train (RMSE): 0.0886 loss_val (RMSE): 0.2835\n",
      "Epoch: 0140 loss_train (RMSE): 0.0886 loss_val (RMSE): 0.2835\n",
      "Epoch: 0141 loss_train (RMSE): 0.0886 loss_val (RMSE): 0.2834\n",
      "Epoch: 0142 loss_train (RMSE): 0.0886 loss_val (RMSE): 0.2833\n",
      "Epoch: 0143 loss_train (RMSE): 0.0886 loss_val (RMSE): 0.2832\n",
      "Epoch: 0144 loss_train (RMSE): 0.0886 loss_val (RMSE): 0.2832\n",
      "Epoch: 0145 loss_train (RMSE): 0.0886 loss_val (RMSE): 0.2831\n",
      "Epoch: 0146 loss_train (RMSE): 0.0886 loss_val (RMSE): 0.2830\n",
      "Epoch: 0147 loss_train (RMSE): 0.0886 loss_val (RMSE): 0.2830\n",
      "Epoch: 0148 loss_train (RMSE): 0.0886 loss_val (RMSE): 0.2829\n",
      "Epoch: 0149 loss_train (RMSE): 0.0886 loss_val (RMSE): 0.2829\n",
      "Epoch: 0150 loss_train (RMSE): 0.0886 loss_val (RMSE): 0.2828\n",
      "Epoch: 0151 loss_train (RMSE): 0.0886 loss_val (RMSE): 0.2828\n",
      "Epoch: 0152 loss_train (RMSE): 0.0886 loss_val (RMSE): 0.2827\n",
      "Epoch: 0153 loss_train (RMSE): 0.0886 loss_val (RMSE): 0.2827\n",
      "Epoch: 0154 loss_train (RMSE): 0.0886 loss_val (RMSE): 0.2827\n",
      "Epoch: 0155 loss_train (RMSE): 0.0886 loss_val (RMSE): 0.2826\n",
      "Epoch: 0156 loss_train (RMSE): 0.0886 loss_val (RMSE): 0.2826\n",
      "Epoch: 0157 loss_train (RMSE): 0.0886 loss_val (RMSE): 0.2826\n",
      "Epoch: 0158 loss_train (RMSE): 0.0886 loss_val (RMSE): 0.2826\n",
      "Epoch: 0159 loss_train (RMSE): 0.0886 loss_val (RMSE): 0.2825\n",
      "Epoch: 0160 loss_train (RMSE): 0.0886 loss_val (RMSE): 0.2825\n",
      "Epoch: 0161 loss_train (RMSE): 0.0886 loss_val (RMSE): 0.2825\n",
      "Epoch: 0162 loss_train (RMSE): 0.0886 loss_val (RMSE): 0.2825\n",
      "Epoch: 0163 loss_train (RMSE): 0.0886 loss_val (RMSE): 0.2824\n",
      "Epoch: 0164 loss_train (RMSE): 0.0886 loss_val (RMSE): 0.2824\n",
      "Epoch: 0165 loss_train (RMSE): 0.0886 loss_val (RMSE): 0.2824\n",
      "Epoch: 0166 loss_train (RMSE): 0.0886 loss_val (RMSE): 0.2824\n",
      "Epoch: 0167 loss_train (RMSE): 0.0886 loss_val (RMSE): 0.2824\n",
      "Epoch: 0168 loss_train (RMSE): 0.0886 loss_val (RMSE): 0.2824\n",
      "Epoch: 0169 loss_train (RMSE): 0.0886 loss_val (RMSE): 0.2824\n",
      "Epoch: 0170 loss_train (RMSE): 0.0886 loss_val (RMSE): 0.2824\n",
      "Epoch: 0171 loss_train (RMSE): 0.0886 loss_val (RMSE): 0.2824\n",
      "Epoch: 0172 loss_train (RMSE): 0.0886 loss_val (RMSE): 0.2824\n",
      "Epoch: 0173 loss_train (RMSE): 0.0886 loss_val (RMSE): 0.2823\n",
      "Epoch: 0174 loss_train (RMSE): 0.0886 loss_val (RMSE): 0.2823\n",
      "Epoch: 0175 loss_train (RMSE): 0.0886 loss_val (RMSE): 0.2823\n",
      "Epoch: 0176 loss_train (RMSE): 0.0886 loss_val (RMSE): 0.2823\n",
      "Epoch: 0177 loss_train (RMSE): 0.0886 loss_val (RMSE): 0.2823\n",
      "Epoch: 0178 loss_train (RMSE): 0.0886 loss_val (RMSE): 0.2823\n",
      "Epoch: 0179 loss_train (RMSE): 0.0886 loss_val (RMSE): 0.2823\n",
      "Epoch: 0180 loss_train (RMSE): 0.0886 loss_val (RMSE): 0.2823\n",
      "Epoch: 0181 loss_train (RMSE): 0.0886 loss_val (RMSE): 0.2823\n",
      "Epoch: 0182 loss_train (RMSE): 0.0886 loss_val (RMSE): 0.2823\n",
      "Epoch: 0183 loss_train (RMSE): 0.0886 loss_val (RMSE): 0.2823\n",
      "Epoch: 0184 loss_train (RMSE): 0.0886 loss_val (RMSE): 0.2823\n",
      "Epoch: 0185 loss_train (RMSE): 0.0886 loss_val (RMSE): 0.2823\n",
      "Epoch: 0186 loss_train (RMSE): 0.0886 loss_val (RMSE): 0.2823\n",
      "Epoch: 0187 loss_train (RMSE): 0.0886 loss_val (RMSE): 0.2823\n",
      "Epoch: 0188 loss_train (RMSE): 0.0886 loss_val (RMSE): 0.2823\n",
      "Epoch: 0189 loss_train (RMSE): 0.0886 loss_val (RMSE): 0.2823\n",
      "Epoch: 0190 loss_train (RMSE): 0.0886 loss_val (RMSE): 0.2823\n",
      "Epoch: 0191 loss_train (RMSE): 0.0886 loss_val (RMSE): 0.2823\n",
      "Epoch: 0192 loss_train (RMSE): 0.0886 loss_val (RMSE): 0.2823\n",
      "Epoch: 0193 loss_train (RMSE): 0.0886 loss_val (RMSE): 0.2823\n",
      "Epoch: 0194 loss_train (RMSE): 0.0886 loss_val (RMSE): 0.2823\n",
      "Epoch: 0195 loss_train (RMSE): 0.0886 loss_val (RMSE): 0.2824\n",
      "Epoch: 0196 loss_train (RMSE): 0.0886 loss_val (RMSE): 0.2824\n",
      "Epoch: 0197 loss_train (RMSE): 0.0886 loss_val (RMSE): 0.2824\n",
      "Epoch: 0198 loss_train (RMSE): 0.0886 loss_val (RMSE): 0.2824\n",
      "Epoch: 0199 loss_train (RMSE): 0.0886 loss_val (RMSE): 0.2824\n",
      "Epoch: 0200 loss_train (RMSE): 0.0886 loss_val (RMSE): 0.2824\n",
      "Optimization Finished!\n",
      "Total time elapsed: 0.4064s\n",
      "Test set results: loss= 0.2772 (RMSE)\n",
      "GCN(\n",
      "  (gc1): GraphConvolution (10 -> 2000)\n",
      "  (gc5): GraphConvolution (2000 -> 2)\n",
      ")\n",
      "Epoch: 0001 loss_train (RMSE): 0.2286 loss_val (RMSE): 0.5533\n",
      "Epoch: 0002 loss_train (RMSE): 0.2212 loss_val (RMSE): 0.5449\n",
      "Epoch: 0003 loss_train (RMSE): 0.2141 loss_val (RMSE): 0.5365\n",
      "Epoch: 0004 loss_train (RMSE): 0.2071 loss_val (RMSE): 0.5282\n",
      "Epoch: 0005 loss_train (RMSE): 0.2004 loss_val (RMSE): 0.5200\n",
      "Epoch: 0006 loss_train (RMSE): 0.1938 loss_val (RMSE): 0.5119\n",
      "Epoch: 0007 loss_train (RMSE): 0.1875 loss_val (RMSE): 0.5038\n",
      "Epoch: 0008 loss_train (RMSE): 0.1814 loss_val (RMSE): 0.4959\n",
      "Epoch: 0009 loss_train (RMSE): 0.1756 loss_val (RMSE): 0.4881\n",
      "Epoch: 0010 loss_train (RMSE): 0.1699 loss_val (RMSE): 0.4804\n",
      "Epoch: 0011 loss_train (RMSE): 0.1645 loss_val (RMSE): 0.4728\n",
      "Epoch: 0012 loss_train (RMSE): 0.1593 loss_val (RMSE): 0.4654\n",
      "Epoch: 0013 loss_train (RMSE): 0.1543 loss_val (RMSE): 0.4581\n",
      "Epoch: 0014 loss_train (RMSE): 0.1496 loss_val (RMSE): 0.4509\n",
      "Epoch: 0015 loss_train (RMSE): 0.1450 loss_val (RMSE): 0.4439\n",
      "Epoch: 0016 loss_train (RMSE): 0.1407 loss_val (RMSE): 0.4371\n",
      "Epoch: 0017 loss_train (RMSE): 0.1366 loss_val (RMSE): 0.4304\n",
      "Epoch: 0018 loss_train (RMSE): 0.1328 loss_val (RMSE): 0.4239\n",
      "Epoch: 0019 loss_train (RMSE): 0.1291 loss_val (RMSE): 0.4175\n",
      "Epoch: 0020 loss_train (RMSE): 0.1256 loss_val (RMSE): 0.4114\n",
      "Epoch: 0021 loss_train (RMSE): 0.1224 loss_val (RMSE): 0.4055\n",
      "Epoch: 0022 loss_train (RMSE): 0.1193 loss_val (RMSE): 0.3997\n",
      "Epoch: 0023 loss_train (RMSE): 0.1165 loss_val (RMSE): 0.3941\n",
      "Epoch: 0024 loss_train (RMSE): 0.1138 loss_val (RMSE): 0.3888\n",
      "Epoch: 0025 loss_train (RMSE): 0.1113 loss_val (RMSE): 0.3836\n",
      "Epoch: 0026 loss_train (RMSE): 0.1090 loss_val (RMSE): 0.3786\n",
      "Epoch: 0027 loss_train (RMSE): 0.1069 loss_val (RMSE): 0.3739\n",
      "Epoch: 0028 loss_train (RMSE): 0.1049 loss_val (RMSE): 0.3694\n",
      "Epoch: 0029 loss_train (RMSE): 0.1031 loss_val (RMSE): 0.3650\n",
      "Epoch: 0030 loss_train (RMSE): 0.1014 loss_val (RMSE): 0.3609\n",
      "Epoch: 0031 loss_train (RMSE): 0.0999 loss_val (RMSE): 0.3569\n",
      "Epoch: 0032 loss_train (RMSE): 0.0985 loss_val (RMSE): 0.3532\n",
      "Epoch: 0033 loss_train (RMSE): 0.0973 loss_val (RMSE): 0.3497\n",
      "Epoch: 0034 loss_train (RMSE): 0.0962 loss_val (RMSE): 0.3463\n",
      "Epoch: 0035 loss_train (RMSE): 0.0952 loss_val (RMSE): 0.3431\n",
      "Epoch: 0036 loss_train (RMSE): 0.0943 loss_val (RMSE): 0.3402\n",
      "Epoch: 0037 loss_train (RMSE): 0.0935 loss_val (RMSE): 0.3374\n",
      "Epoch: 0038 loss_train (RMSE): 0.0927 loss_val (RMSE): 0.3347\n",
      "Epoch: 0039 loss_train (RMSE): 0.0921 loss_val (RMSE): 0.3323\n",
      "Epoch: 0040 loss_train (RMSE): 0.0916 loss_val (RMSE): 0.3300\n",
      "Epoch: 0041 loss_train (RMSE): 0.0911 loss_val (RMSE): 0.3278\n",
      "Epoch: 0042 loss_train (RMSE): 0.0907 loss_val (RMSE): 0.3258\n",
      "Epoch: 0043 loss_train (RMSE): 0.0903 loss_val (RMSE): 0.3240\n",
      "Epoch: 0044 loss_train (RMSE): 0.0901 loss_val (RMSE): 0.3223\n",
      "Epoch: 0045 loss_train (RMSE): 0.0898 loss_val (RMSE): 0.3207\n",
      "Epoch: 0046 loss_train (RMSE): 0.0896 loss_val (RMSE): 0.3192\n",
      "Epoch: 0047 loss_train (RMSE): 0.0894 loss_val (RMSE): 0.3178\n",
      "Epoch: 0048 loss_train (RMSE): 0.0893 loss_val (RMSE): 0.3166\n",
      "Epoch: 0049 loss_train (RMSE): 0.0892 loss_val (RMSE): 0.3154\n",
      "Epoch: 0050 loss_train (RMSE): 0.0891 loss_val (RMSE): 0.3143\n",
      "Epoch: 0051 loss_train (RMSE): 0.0890 loss_val (RMSE): 0.3134\n",
      "Epoch: 0052 loss_train (RMSE): 0.0890 loss_val (RMSE): 0.3125\n",
      "Epoch: 0053 loss_train (RMSE): 0.0889 loss_val (RMSE): 0.3117\n",
      "Epoch: 0054 loss_train (RMSE): 0.0889 loss_val (RMSE): 0.3109\n",
      "Epoch: 0055 loss_train (RMSE): 0.0889 loss_val (RMSE): 0.3103\n",
      "Epoch: 0056 loss_train (RMSE): 0.0889 loss_val (RMSE): 0.3097\n",
      "Epoch: 0057 loss_train (RMSE): 0.0889 loss_val (RMSE): 0.3091\n",
      "Epoch: 0058 loss_train (RMSE): 0.0889 loss_val (RMSE): 0.3086\n",
      "Epoch: 0059 loss_train (RMSE): 0.0889 loss_val (RMSE): 0.3082\n",
      "Epoch: 0060 loss_train (RMSE): 0.0889 loss_val (RMSE): 0.3078\n",
      "Epoch: 0061 loss_train (RMSE): 0.0889 loss_val (RMSE): 0.3075\n",
      "Epoch: 0062 loss_train (RMSE): 0.0889 loss_val (RMSE): 0.3072\n",
      "Epoch: 0063 loss_train (RMSE): 0.0890 loss_val (RMSE): 0.3070\n",
      "Epoch: 0064 loss_train (RMSE): 0.0890 loss_val (RMSE): 0.3068\n",
      "Epoch: 0065 loss_train (RMSE): 0.0890 loss_val (RMSE): 0.3066\n",
      "Epoch: 0066 loss_train (RMSE): 0.0890 loss_val (RMSE): 0.3064\n",
      "Epoch: 0067 loss_train (RMSE): 0.0890 loss_val (RMSE): 0.3063\n",
      "Epoch: 0068 loss_train (RMSE): 0.0890 loss_val (RMSE): 0.3062\n",
      "Epoch: 0069 loss_train (RMSE): 0.0890 loss_val (RMSE): 0.3062\n",
      "Epoch: 0070 loss_train (RMSE): 0.0890 loss_val (RMSE): 0.3062\n",
      "Epoch: 0071 loss_train (RMSE): 0.0890 loss_val (RMSE): 0.3061\n",
      "Epoch: 0072 loss_train (RMSE): 0.0890 loss_val (RMSE): 0.3061\n",
      "Epoch: 0073 loss_train (RMSE): 0.0890 loss_val (RMSE): 0.3062\n",
      "Epoch: 0074 loss_train (RMSE): 0.0890 loss_val (RMSE): 0.3062\n",
      "Epoch: 0075 loss_train (RMSE): 0.0890 loss_val (RMSE): 0.3063\n",
      "Epoch: 0076 loss_train (RMSE): 0.0890 loss_val (RMSE): 0.3064\n",
      "Epoch: 0077 loss_train (RMSE): 0.0890 loss_val (RMSE): 0.3064\n",
      "Epoch: 0078 loss_train (RMSE): 0.0889 loss_val (RMSE): 0.3065\n",
      "Epoch: 0079 loss_train (RMSE): 0.0889 loss_val (RMSE): 0.3066\n",
      "Epoch: 0080 loss_train (RMSE): 0.0889 loss_val (RMSE): 0.3068\n",
      "Epoch: 0081 loss_train (RMSE): 0.0889 loss_val (RMSE): 0.3069\n",
      "Epoch: 0082 loss_train (RMSE): 0.0889 loss_val (RMSE): 0.3070\n",
      "Epoch: 0083 loss_train (RMSE): 0.0889 loss_val (RMSE): 0.3071\n",
      "Epoch: 0084 loss_train (RMSE): 0.0889 loss_val (RMSE): 0.3073\n",
      "Epoch: 0085 loss_train (RMSE): 0.0889 loss_val (RMSE): 0.3074\n",
      "Epoch: 0086 loss_train (RMSE): 0.0889 loss_val (RMSE): 0.3075\n",
      "Epoch: 0087 loss_train (RMSE): 0.0889 loss_val (RMSE): 0.3077\n",
      "Epoch: 0088 loss_train (RMSE): 0.0889 loss_val (RMSE): 0.3078\n",
      "Epoch: 0089 loss_train (RMSE): 0.0889 loss_val (RMSE): 0.3079\n",
      "Epoch: 0090 loss_train (RMSE): 0.0889 loss_val (RMSE): 0.3081\n",
      "Epoch: 0091 loss_train (RMSE): 0.0889 loss_val (RMSE): 0.3082\n",
      "Epoch: 0092 loss_train (RMSE): 0.0889 loss_val (RMSE): 0.3083\n",
      "Epoch: 0093 loss_train (RMSE): 0.0889 loss_val (RMSE): 0.3084\n",
      "Epoch: 0094 loss_train (RMSE): 0.0889 loss_val (RMSE): 0.3086\n",
      "Epoch: 0095 loss_train (RMSE): 0.0889 loss_val (RMSE): 0.3087\n",
      "Epoch: 0096 loss_train (RMSE): 0.0889 loss_val (RMSE): 0.3088\n",
      "Epoch: 0097 loss_train (RMSE): 0.0889 loss_val (RMSE): 0.3089\n",
      "Epoch: 0098 loss_train (RMSE): 0.0889 loss_val (RMSE): 0.3090\n",
      "Epoch: 0099 loss_train (RMSE): 0.0889 loss_val (RMSE): 0.3091\n",
      "Epoch: 0100 loss_train (RMSE): 0.0889 loss_val (RMSE): 0.3092\n",
      "Epoch: 0101 loss_train (RMSE): 0.0889 loss_val (RMSE): 0.3092\n",
      "Epoch: 0102 loss_train (RMSE): 0.0889 loss_val (RMSE): 0.3093\n",
      "Epoch: 0103 loss_train (RMSE): 0.0889 loss_val (RMSE): 0.3094\n",
      "Epoch: 0104 loss_train (RMSE): 0.0889 loss_val (RMSE): 0.3094\n",
      "Epoch: 0105 loss_train (RMSE): 0.0889 loss_val (RMSE): 0.3095\n",
      "Epoch: 0106 loss_train (RMSE): 0.0889 loss_val (RMSE): 0.3095\n",
      "Epoch: 0107 loss_train (RMSE): 0.0889 loss_val (RMSE): 0.3096\n",
      "Epoch: 0108 loss_train (RMSE): 0.0889 loss_val (RMSE): 0.3096\n",
      "Epoch: 0109 loss_train (RMSE): 0.0889 loss_val (RMSE): 0.3097\n",
      "Epoch: 0110 loss_train (RMSE): 0.0889 loss_val (RMSE): 0.3097\n",
      "Epoch: 0111 loss_train (RMSE): 0.0889 loss_val (RMSE): 0.3097\n",
      "Epoch: 0112 loss_train (RMSE): 0.0889 loss_val (RMSE): 0.3097\n",
      "Epoch: 0113 loss_train (RMSE): 0.0889 loss_val (RMSE): 0.3098\n",
      "Epoch: 0114 loss_train (RMSE): 0.0889 loss_val (RMSE): 0.3098\n",
      "Epoch: 0115 loss_train (RMSE): 0.0889 loss_val (RMSE): 0.3098\n",
      "Epoch: 0116 loss_train (RMSE): 0.0889 loss_val (RMSE): 0.3098\n",
      "Epoch: 0117 loss_train (RMSE): 0.0889 loss_val (RMSE): 0.3098\n",
      "Epoch: 0118 loss_train (RMSE): 0.0889 loss_val (RMSE): 0.3098\n",
      "Epoch: 0119 loss_train (RMSE): 0.0889 loss_val (RMSE): 0.3098\n",
      "Epoch: 0120 loss_train (RMSE): 0.0889 loss_val (RMSE): 0.3098\n",
      "Epoch: 0121 loss_train (RMSE): 0.0889 loss_val (RMSE): 0.3098\n",
      "Epoch: 0122 loss_train (RMSE): 0.0889 loss_val (RMSE): 0.3098\n",
      "Epoch: 0123 loss_train (RMSE): 0.0889 loss_val (RMSE): 0.3098\n",
      "Epoch: 0124 loss_train (RMSE): 0.0889 loss_val (RMSE): 0.3098\n",
      "Epoch: 0125 loss_train (RMSE): 0.0889 loss_val (RMSE): 0.3097\n",
      "Epoch: 0126 loss_train (RMSE): 0.0889 loss_val (RMSE): 0.3097\n",
      "Epoch: 0127 loss_train (RMSE): 0.0889 loss_val (RMSE): 0.3097\n",
      "Epoch: 0128 loss_train (RMSE): 0.0889 loss_val (RMSE): 0.3097\n",
      "Epoch: 0129 loss_train (RMSE): 0.0889 loss_val (RMSE): 0.3097\n",
      "Epoch: 0130 loss_train (RMSE): 0.0889 loss_val (RMSE): 0.3097\n",
      "Epoch: 0131 loss_train (RMSE): 0.0889 loss_val (RMSE): 0.3097\n",
      "Epoch: 0132 loss_train (RMSE): 0.0889 loss_val (RMSE): 0.3096\n",
      "Epoch: 0133 loss_train (RMSE): 0.0889 loss_val (RMSE): 0.3096\n",
      "Epoch: 0134 loss_train (RMSE): 0.0889 loss_val (RMSE): 0.3096\n",
      "Epoch: 0135 loss_train (RMSE): 0.0889 loss_val (RMSE): 0.3096\n",
      "Epoch: 0136 loss_train (RMSE): 0.0889 loss_val (RMSE): 0.3096\n",
      "Epoch: 0137 loss_train (RMSE): 0.0889 loss_val (RMSE): 0.3096\n",
      "Epoch: 0138 loss_train (RMSE): 0.0889 loss_val (RMSE): 0.3096\n",
      "Epoch: 0139 loss_train (RMSE): 0.0889 loss_val (RMSE): 0.3096\n",
      "Epoch: 0140 loss_train (RMSE): 0.0889 loss_val (RMSE): 0.3095\n",
      "Epoch: 0141 loss_train (RMSE): 0.0889 loss_val (RMSE): 0.3095\n",
      "Epoch: 0142 loss_train (RMSE): 0.0889 loss_val (RMSE): 0.3095\n",
      "Epoch: 0143 loss_train (RMSE): 0.0889 loss_val (RMSE): 0.3095\n",
      "Epoch: 0144 loss_train (RMSE): 0.0889 loss_val (RMSE): 0.3095\n",
      "Epoch: 0145 loss_train (RMSE): 0.0889 loss_val (RMSE): 0.3095\n",
      "Epoch: 0146 loss_train (RMSE): 0.0889 loss_val (RMSE): 0.3095\n",
      "Epoch: 0147 loss_train (RMSE): 0.0889 loss_val (RMSE): 0.3095\n",
      "Epoch: 0148 loss_train (RMSE): 0.0889 loss_val (RMSE): 0.3095\n",
      "Epoch: 0149 loss_train (RMSE): 0.0889 loss_val (RMSE): 0.3095\n",
      "Epoch: 0150 loss_train (RMSE): 0.0889 loss_val (RMSE): 0.3095\n",
      "Epoch: 0151 loss_train (RMSE): 0.0889 loss_val (RMSE): 0.3095\n",
      "Epoch: 0152 loss_train (RMSE): 0.0889 loss_val (RMSE): 0.3095\n",
      "Epoch: 0153 loss_train (RMSE): 0.0889 loss_val (RMSE): 0.3095\n",
      "Epoch: 0154 loss_train (RMSE): 0.0889 loss_val (RMSE): 0.3095\n",
      "Epoch: 0155 loss_train (RMSE): 0.0889 loss_val (RMSE): 0.3094\n",
      "Epoch: 0156 loss_train (RMSE): 0.0889 loss_val (RMSE): 0.3094\n",
      "Epoch: 0157 loss_train (RMSE): 0.0889 loss_val (RMSE): 0.3094\n",
      "Epoch: 0158 loss_train (RMSE): 0.0889 loss_val (RMSE): 0.3094\n",
      "Epoch: 0159 loss_train (RMSE): 0.0889 loss_val (RMSE): 0.3094\n",
      "Epoch: 0160 loss_train (RMSE): 0.0889 loss_val (RMSE): 0.3094\n",
      "Epoch: 0161 loss_train (RMSE): 0.0889 loss_val (RMSE): 0.3094\n",
      "Epoch: 0162 loss_train (RMSE): 0.0889 loss_val (RMSE): 0.3095\n",
      "Epoch: 0163 loss_train (RMSE): 0.0889 loss_val (RMSE): 0.3095\n",
      "Epoch: 0164 loss_train (RMSE): 0.0889 loss_val (RMSE): 0.3095\n",
      "Epoch: 0165 loss_train (RMSE): 0.0889 loss_val (RMSE): 0.3095\n",
      "Epoch: 0166 loss_train (RMSE): 0.0889 loss_val (RMSE): 0.3095\n",
      "Epoch: 0167 loss_train (RMSE): 0.0889 loss_val (RMSE): 0.3095\n",
      "Epoch: 0168 loss_train (RMSE): 0.0889 loss_val (RMSE): 0.3095\n",
      "Epoch: 0169 loss_train (RMSE): 0.0889 loss_val (RMSE): 0.3095\n",
      "Epoch: 0170 loss_train (RMSE): 0.0889 loss_val (RMSE): 0.3095\n",
      "Epoch: 0171 loss_train (RMSE): 0.0889 loss_val (RMSE): 0.3095\n",
      "Epoch: 0172 loss_train (RMSE): 0.0889 loss_val (RMSE): 0.3095\n",
      "Epoch: 0173 loss_train (RMSE): 0.0889 loss_val (RMSE): 0.3095\n",
      "Epoch: 0174 loss_train (RMSE): 0.0889 loss_val (RMSE): 0.3095\n",
      "Epoch: 0175 loss_train (RMSE): 0.0889 loss_val (RMSE): 0.3095\n",
      "Epoch: 0176 loss_train (RMSE): 0.0889 loss_val (RMSE): 0.3095\n",
      "Epoch: 0177 loss_train (RMSE): 0.0889 loss_val (RMSE): 0.3095\n",
      "Epoch: 0178 loss_train (RMSE): 0.0889 loss_val (RMSE): 0.3095\n",
      "Epoch: 0179 loss_train (RMSE): 0.0889 loss_val (RMSE): 0.3095\n",
      "Epoch: 0180 loss_train (RMSE): 0.0889 loss_val (RMSE): 0.3095\n",
      "Epoch: 0181 loss_train (RMSE): 0.0889 loss_val (RMSE): 0.3095\n",
      "Epoch: 0182 loss_train (RMSE): 0.0889 loss_val (RMSE): 0.3095\n",
      "Epoch: 0183 loss_train (RMSE): 0.0889 loss_val (RMSE): 0.3095\n",
      "Epoch: 0184 loss_train (RMSE): 0.0889 loss_val (RMSE): 0.3095\n",
      "Epoch: 0185 loss_train (RMSE): 0.0889 loss_val (RMSE): 0.3095\n",
      "Epoch: 0186 loss_train (RMSE): 0.0889 loss_val (RMSE): 0.3095\n",
      "Epoch: 0187 loss_train (RMSE): 0.0889 loss_val (RMSE): 0.3095\n",
      "Epoch: 0188 loss_train (RMSE): 0.0889 loss_val (RMSE): 0.3095\n",
      "Epoch: 0189 loss_train (RMSE): 0.0889 loss_val (RMSE): 0.3095\n",
      "Epoch: 0190 loss_train (RMSE): 0.0889 loss_val (RMSE): 0.3095\n",
      "Epoch: 0191 loss_train (RMSE): 0.0889 loss_val (RMSE): 0.3095\n",
      "Epoch: 0192 loss_train (RMSE): 0.0889 loss_val (RMSE): 0.3095\n",
      "Epoch: 0193 loss_train (RMSE): 0.0889 loss_val (RMSE): 0.3095\n",
      "Epoch: 0194 loss_train (RMSE): 0.0889 loss_val (RMSE): 0.3095\n",
      "Epoch: 0195 loss_train (RMSE): 0.0889 loss_val (RMSE): 0.3095\n",
      "Epoch: 0196 loss_train (RMSE): 0.0889 loss_val (RMSE): 0.3095\n",
      "Epoch: 0197 loss_train (RMSE): 0.0889 loss_val (RMSE): 0.3095\n",
      "Epoch: 0198 loss_train (RMSE): 0.0889 loss_val (RMSE): 0.3095\n",
      "Epoch: 0199 loss_train (RMSE): 0.0889 loss_val (RMSE): 0.3095\n",
      "Epoch: 0200 loss_train (RMSE): 0.0889 loss_val (RMSE): 0.3095\n",
      "Optimization Finished!\n",
      "Total time elapsed: 0.3985s\n",
      "Test set results: loss= 0.3115 (RMSE)\n",
      "GCN(\n",
      "  (gc1): GraphConvolution (10 -> 2000)\n",
      "  (gc5): GraphConvolution (2000 -> 2)\n",
      ")\n",
      "Epoch: 0001 loss_train (RMSE): 0.5992 loss_val (RMSE): 0.8015\n",
      "Epoch: 0002 loss_train (RMSE): 0.5872 loss_val (RMSE): 0.7932\n",
      "Epoch: 0003 loss_train (RMSE): 0.5754 loss_val (RMSE): 0.7849\n",
      "Epoch: 0004 loss_train (RMSE): 0.5638 loss_val (RMSE): 0.7766\n",
      "Epoch: 0005 loss_train (RMSE): 0.5524 loss_val (RMSE): 0.7684\n",
      "Epoch: 0006 loss_train (RMSE): 0.5413 loss_val (RMSE): 0.7603\n",
      "Epoch: 0007 loss_train (RMSE): 0.5303 loss_val (RMSE): 0.7523\n",
      "Epoch: 0008 loss_train (RMSE): 0.5196 loss_val (RMSE): 0.7443\n",
      "Epoch: 0009 loss_train (RMSE): 0.5091 loss_val (RMSE): 0.7363\n",
      "Epoch: 0010 loss_train (RMSE): 0.4988 loss_val (RMSE): 0.7285\n",
      "Epoch: 0011 loss_train (RMSE): 0.4888 loss_val (RMSE): 0.7207\n",
      "Epoch: 0012 loss_train (RMSE): 0.4789 loss_val (RMSE): 0.7130\n",
      "Epoch: 0013 loss_train (RMSE): 0.4693 loss_val (RMSE): 0.7054\n",
      "Epoch: 0014 loss_train (RMSE): 0.4599 loss_val (RMSE): 0.6979\n",
      "Epoch: 0015 loss_train (RMSE): 0.4507 loss_val (RMSE): 0.6905\n",
      "Epoch: 0016 loss_train (RMSE): 0.4417 loss_val (RMSE): 0.6832\n",
      "Epoch: 0017 loss_train (RMSE): 0.4330 loss_val (RMSE): 0.6759\n",
      "Epoch: 0018 loss_train (RMSE): 0.4244 loss_val (RMSE): 0.6688\n",
      "Epoch: 0019 loss_train (RMSE): 0.4160 loss_val (RMSE): 0.6617\n",
      "Epoch: 0020 loss_train (RMSE): 0.4079 loss_val (RMSE): 0.6548\n",
      "Epoch: 0021 loss_train (RMSE): 0.3999 loss_val (RMSE): 0.6479\n",
      "Epoch: 0022 loss_train (RMSE): 0.3921 loss_val (RMSE): 0.6412\n",
      "Epoch: 0023 loss_train (RMSE): 0.3845 loss_val (RMSE): 0.6345\n",
      "Epoch: 0024 loss_train (RMSE): 0.3770 loss_val (RMSE): 0.6280\n",
      "Epoch: 0025 loss_train (RMSE): 0.3697 loss_val (RMSE): 0.6215\n",
      "Epoch: 0026 loss_train (RMSE): 0.3626 loss_val (RMSE): 0.6151\n",
      "Epoch: 0027 loss_train (RMSE): 0.3556 loss_val (RMSE): 0.6088\n",
      "Epoch: 0028 loss_train (RMSE): 0.3488 loss_val (RMSE): 0.6027\n",
      "Epoch: 0029 loss_train (RMSE): 0.3421 loss_val (RMSE): 0.5965\n",
      "Epoch: 0030 loss_train (RMSE): 0.3356 loss_val (RMSE): 0.5905\n",
      "Epoch: 0031 loss_train (RMSE): 0.3291 loss_val (RMSE): 0.5846\n",
      "Epoch: 0032 loss_train (RMSE): 0.3228 loss_val (RMSE): 0.5787\n",
      "Epoch: 0033 loss_train (RMSE): 0.3167 loss_val (RMSE): 0.5730\n",
      "Epoch: 0034 loss_train (RMSE): 0.3106 loss_val (RMSE): 0.5673\n",
      "Epoch: 0035 loss_train (RMSE): 0.3047 loss_val (RMSE): 0.5617\n",
      "Epoch: 0036 loss_train (RMSE): 0.2988 loss_val (RMSE): 0.5562\n",
      "Epoch: 0037 loss_train (RMSE): 0.2931 loss_val (RMSE): 0.5507\n",
      "Epoch: 0038 loss_train (RMSE): 0.2875 loss_val (RMSE): 0.5453\n",
      "Epoch: 0039 loss_train (RMSE): 0.2820 loss_val (RMSE): 0.5400\n",
      "Epoch: 0040 loss_train (RMSE): 0.2766 loss_val (RMSE): 0.5348\n",
      "Epoch: 0041 loss_train (RMSE): 0.2713 loss_val (RMSE): 0.5297\n",
      "Epoch: 0042 loss_train (RMSE): 0.2661 loss_val (RMSE): 0.5246\n",
      "Epoch: 0043 loss_train (RMSE): 0.2610 loss_val (RMSE): 0.5196\n",
      "Epoch: 0044 loss_train (RMSE): 0.2560 loss_val (RMSE): 0.5147\n",
      "Epoch: 0045 loss_train (RMSE): 0.2511 loss_val (RMSE): 0.5098\n",
      "Epoch: 0046 loss_train (RMSE): 0.2464 loss_val (RMSE): 0.5051\n",
      "Epoch: 0047 loss_train (RMSE): 0.2417 loss_val (RMSE): 0.5004\n",
      "Epoch: 0048 loss_train (RMSE): 0.2371 loss_val (RMSE): 0.4957\n",
      "Epoch: 0049 loss_train (RMSE): 0.2327 loss_val (RMSE): 0.4912\n",
      "Epoch: 0050 loss_train (RMSE): 0.2283 loss_val (RMSE): 0.4867\n",
      "Epoch: 0051 loss_train (RMSE): 0.2240 loss_val (RMSE): 0.4822\n",
      "Epoch: 0052 loss_train (RMSE): 0.2199 loss_val (RMSE): 0.4779\n",
      "Epoch: 0053 loss_train (RMSE): 0.2158 loss_val (RMSE): 0.4736\n",
      "Epoch: 0054 loss_train (RMSE): 0.2119 loss_val (RMSE): 0.4693\n",
      "Epoch: 0055 loss_train (RMSE): 0.2080 loss_val (RMSE): 0.4651\n",
      "Epoch: 0056 loss_train (RMSE): 0.2042 loss_val (RMSE): 0.4610\n",
      "Epoch: 0057 loss_train (RMSE): 0.2005 loss_val (RMSE): 0.4570\n",
      "Epoch: 0058 loss_train (RMSE): 0.1970 loss_val (RMSE): 0.4530\n",
      "Epoch: 0059 loss_train (RMSE): 0.1935 loss_val (RMSE): 0.4490\n",
      "Epoch: 0060 loss_train (RMSE): 0.1901 loss_val (RMSE): 0.4451\n",
      "Epoch: 0061 loss_train (RMSE): 0.1867 loss_val (RMSE): 0.4413\n",
      "Epoch: 0062 loss_train (RMSE): 0.1835 loss_val (RMSE): 0.4375\n",
      "Epoch: 0063 loss_train (RMSE): 0.1804 loss_val (RMSE): 0.4338\n",
      "Epoch: 0064 loss_train (RMSE): 0.1773 loss_val (RMSE): 0.4301\n",
      "Epoch: 0065 loss_train (RMSE): 0.1743 loss_val (RMSE): 0.4265\n",
      "Epoch: 0066 loss_train (RMSE): 0.1714 loss_val (RMSE): 0.4229\n",
      "Epoch: 0067 loss_train (RMSE): 0.1686 loss_val (RMSE): 0.4193\n",
      "Epoch: 0068 loss_train (RMSE): 0.1658 loss_val (RMSE): 0.4159\n",
      "Epoch: 0069 loss_train (RMSE): 0.1631 loss_val (RMSE): 0.4124\n",
      "Epoch: 0070 loss_train (RMSE): 0.1605 loss_val (RMSE): 0.4090\n",
      "Epoch: 0071 loss_train (RMSE): 0.1580 loss_val (RMSE): 0.4057\n",
      "Epoch: 0072 loss_train (RMSE): 0.1555 loss_val (RMSE): 0.4024\n",
      "Epoch: 0073 loss_train (RMSE): 0.1531 loss_val (RMSE): 0.3992\n",
      "Epoch: 0074 loss_train (RMSE): 0.1508 loss_val (RMSE): 0.3960\n",
      "Epoch: 0075 loss_train (RMSE): 0.1485 loss_val (RMSE): 0.3929\n",
      "Epoch: 0076 loss_train (RMSE): 0.1463 loss_val (RMSE): 0.3898\n",
      "Epoch: 0077 loss_train (RMSE): 0.1442 loss_val (RMSE): 0.3868\n",
      "Epoch: 0078 loss_train (RMSE): 0.1421 loss_val (RMSE): 0.3838\n",
      "Epoch: 0079 loss_train (RMSE): 0.1401 loss_val (RMSE): 0.3809\n",
      "Epoch: 0080 loss_train (RMSE): 0.1381 loss_val (RMSE): 0.3781\n",
      "Epoch: 0081 loss_train (RMSE): 0.1362 loss_val (RMSE): 0.3753\n",
      "Epoch: 0082 loss_train (RMSE): 0.1344 loss_val (RMSE): 0.3725\n",
      "Epoch: 0083 loss_train (RMSE): 0.1326 loss_val (RMSE): 0.3699\n",
      "Epoch: 0084 loss_train (RMSE): 0.1308 loss_val (RMSE): 0.3672\n",
      "Epoch: 0085 loss_train (RMSE): 0.1292 loss_val (RMSE): 0.3647\n",
      "Epoch: 0086 loss_train (RMSE): 0.1275 loss_val (RMSE): 0.3622\n",
      "Epoch: 0087 loss_train (RMSE): 0.1260 loss_val (RMSE): 0.3597\n",
      "Epoch: 0088 loss_train (RMSE): 0.1244 loss_val (RMSE): 0.3573\n",
      "Epoch: 0089 loss_train (RMSE): 0.1229 loss_val (RMSE): 0.3550\n",
      "Epoch: 0090 loss_train (RMSE): 0.1215 loss_val (RMSE): 0.3527\n",
      "Epoch: 0091 loss_train (RMSE): 0.1201 loss_val (RMSE): 0.3505\n",
      "Epoch: 0092 loss_train (RMSE): 0.1188 loss_val (RMSE): 0.3483\n",
      "Epoch: 0093 loss_train (RMSE): 0.1175 loss_val (RMSE): 0.3462\n",
      "Epoch: 0094 loss_train (RMSE): 0.1162 loss_val (RMSE): 0.3442\n",
      "Epoch: 0095 loss_train (RMSE): 0.1150 loss_val (RMSE): 0.3422\n",
      "Epoch: 0096 loss_train (RMSE): 0.1138 loss_val (RMSE): 0.3402\n",
      "Epoch: 0097 loss_train (RMSE): 0.1127 loss_val (RMSE): 0.3383\n",
      "Epoch: 0098 loss_train (RMSE): 0.1116 loss_val (RMSE): 0.3365\n",
      "Epoch: 0099 loss_train (RMSE): 0.1106 loss_val (RMSE): 0.3347\n",
      "Epoch: 0100 loss_train (RMSE): 0.1095 loss_val (RMSE): 0.3330\n",
      "Epoch: 0101 loss_train (RMSE): 0.1085 loss_val (RMSE): 0.3313\n",
      "Epoch: 0102 loss_train (RMSE): 0.1076 loss_val (RMSE): 0.3297\n",
      "Epoch: 0103 loss_train (RMSE): 0.1067 loss_val (RMSE): 0.3281\n",
      "Epoch: 0104 loss_train (RMSE): 0.1058 loss_val (RMSE): 0.3265\n",
      "Epoch: 0105 loss_train (RMSE): 0.1049 loss_val (RMSE): 0.3250\n",
      "Epoch: 0106 loss_train (RMSE): 0.1041 loss_val (RMSE): 0.3236\n",
      "Epoch: 0107 loss_train (RMSE): 0.1033 loss_val (RMSE): 0.3222\n",
      "Epoch: 0108 loss_train (RMSE): 0.1025 loss_val (RMSE): 0.3208\n",
      "Epoch: 0109 loss_train (RMSE): 0.1018 loss_val (RMSE): 0.3195\n",
      "Epoch: 0110 loss_train (RMSE): 0.1011 loss_val (RMSE): 0.3182\n",
      "Epoch: 0111 loss_train (RMSE): 0.1004 loss_val (RMSE): 0.3169\n",
      "Epoch: 0112 loss_train (RMSE): 0.0997 loss_val (RMSE): 0.3157\n",
      "Epoch: 0113 loss_train (RMSE): 0.0991 loss_val (RMSE): 0.3145\n",
      "Epoch: 0114 loss_train (RMSE): 0.0985 loss_val (RMSE): 0.3134\n",
      "Epoch: 0115 loss_train (RMSE): 0.0979 loss_val (RMSE): 0.3123\n",
      "Epoch: 0116 loss_train (RMSE): 0.0973 loss_val (RMSE): 0.3112\n",
      "Epoch: 0117 loss_train (RMSE): 0.0968 loss_val (RMSE): 0.3102\n",
      "Epoch: 0118 loss_train (RMSE): 0.0963 loss_val (RMSE): 0.3091\n",
      "Epoch: 0119 loss_train (RMSE): 0.0958 loss_val (RMSE): 0.3082\n",
      "Epoch: 0120 loss_train (RMSE): 0.0953 loss_val (RMSE): 0.3072\n",
      "Epoch: 0121 loss_train (RMSE): 0.0948 loss_val (RMSE): 0.3063\n",
      "Epoch: 0122 loss_train (RMSE): 0.0944 loss_val (RMSE): 0.3054\n",
      "Epoch: 0123 loss_train (RMSE): 0.0939 loss_val (RMSE): 0.3045\n",
      "Epoch: 0124 loss_train (RMSE): 0.0935 loss_val (RMSE): 0.3037\n",
      "Epoch: 0125 loss_train (RMSE): 0.0931 loss_val (RMSE): 0.3029\n",
      "Epoch: 0126 loss_train (RMSE): 0.0928 loss_val (RMSE): 0.3021\n",
      "Epoch: 0127 loss_train (RMSE): 0.0924 loss_val (RMSE): 0.3013\n",
      "Epoch: 0128 loss_train (RMSE): 0.0920 loss_val (RMSE): 0.3006\n",
      "Epoch: 0129 loss_train (RMSE): 0.0917 loss_val (RMSE): 0.2999\n",
      "Epoch: 0130 loss_train (RMSE): 0.0914 loss_val (RMSE): 0.2992\n",
      "Epoch: 0131 loss_train (RMSE): 0.0911 loss_val (RMSE): 0.2985\n",
      "Epoch: 0132 loss_train (RMSE): 0.0908 loss_val (RMSE): 0.2979\n",
      "Epoch: 0133 loss_train (RMSE): 0.0905 loss_val (RMSE): 0.2973\n",
      "Epoch: 0134 loss_train (RMSE): 0.0902 loss_val (RMSE): 0.2967\n",
      "Epoch: 0135 loss_train (RMSE): 0.0900 loss_val (RMSE): 0.2961\n",
      "Epoch: 0136 loss_train (RMSE): 0.0897 loss_val (RMSE): 0.2955\n",
      "Epoch: 0137 loss_train (RMSE): 0.0895 loss_val (RMSE): 0.2950\n",
      "Epoch: 0138 loss_train (RMSE): 0.0893 loss_val (RMSE): 0.2945\n",
      "Epoch: 0139 loss_train (RMSE): 0.0891 loss_val (RMSE): 0.2940\n",
      "Epoch: 0140 loss_train (RMSE): 0.0889 loss_val (RMSE): 0.2935\n",
      "Epoch: 0141 loss_train (RMSE): 0.0887 loss_val (RMSE): 0.2930\n",
      "Epoch: 0142 loss_train (RMSE): 0.0885 loss_val (RMSE): 0.2926\n",
      "Epoch: 0143 loss_train (RMSE): 0.0883 loss_val (RMSE): 0.2921\n",
      "Epoch: 0144 loss_train (RMSE): 0.0881 loss_val (RMSE): 0.2917\n",
      "Epoch: 0145 loss_train (RMSE): 0.0880 loss_val (RMSE): 0.2913\n",
      "Epoch: 0146 loss_train (RMSE): 0.0878 loss_val (RMSE): 0.2909\n",
      "Epoch: 0147 loss_train (RMSE): 0.0877 loss_val (RMSE): 0.2906\n",
      "Epoch: 0148 loss_train (RMSE): 0.0875 loss_val (RMSE): 0.2902\n",
      "Epoch: 0149 loss_train (RMSE): 0.0874 loss_val (RMSE): 0.2899\n",
      "Epoch: 0150 loss_train (RMSE): 0.0873 loss_val (RMSE): 0.2895\n",
      "Epoch: 0151 loss_train (RMSE): 0.0871 loss_val (RMSE): 0.2892\n",
      "Epoch: 0152 loss_train (RMSE): 0.0870 loss_val (RMSE): 0.2889\n",
      "Epoch: 0153 loss_train (RMSE): 0.0869 loss_val (RMSE): 0.2886\n",
      "Epoch: 0154 loss_train (RMSE): 0.0868 loss_val (RMSE): 0.2883\n",
      "Epoch: 0155 loss_train (RMSE): 0.0867 loss_val (RMSE): 0.2881\n",
      "Epoch: 0156 loss_train (RMSE): 0.0866 loss_val (RMSE): 0.2878\n",
      "Epoch: 0157 loss_train (RMSE): 0.0865 loss_val (RMSE): 0.2876\n",
      "Epoch: 0158 loss_train (RMSE): 0.0864 loss_val (RMSE): 0.2873\n",
      "Epoch: 0159 loss_train (RMSE): 0.0864 loss_val (RMSE): 0.2871\n",
      "Epoch: 0160 loss_train (RMSE): 0.0863 loss_val (RMSE): 0.2869\n",
      "Epoch: 0161 loss_train (RMSE): 0.0862 loss_val (RMSE): 0.2866\n",
      "Epoch: 0162 loss_train (RMSE): 0.0861 loss_val (RMSE): 0.2864\n",
      "Epoch: 0163 loss_train (RMSE): 0.0861 loss_val (RMSE): 0.2862\n",
      "Epoch: 0164 loss_train (RMSE): 0.0860 loss_val (RMSE): 0.2861\n",
      "Epoch: 0165 loss_train (RMSE): 0.0860 loss_val (RMSE): 0.2859\n",
      "Epoch: 0166 loss_train (RMSE): 0.0859 loss_val (RMSE): 0.2857\n",
      "Epoch: 0167 loss_train (RMSE): 0.0858 loss_val (RMSE): 0.2855\n",
      "Epoch: 0168 loss_train (RMSE): 0.0858 loss_val (RMSE): 0.2854\n",
      "Epoch: 0169 loss_train (RMSE): 0.0857 loss_val (RMSE): 0.2852\n",
      "Epoch: 0170 loss_train (RMSE): 0.0857 loss_val (RMSE): 0.2850\n",
      "Epoch: 0171 loss_train (RMSE): 0.0857 loss_val (RMSE): 0.2849\n",
      "Epoch: 0172 loss_train (RMSE): 0.0856 loss_val (RMSE): 0.2848\n",
      "Epoch: 0173 loss_train (RMSE): 0.0856 loss_val (RMSE): 0.2846\n",
      "Epoch: 0174 loss_train (RMSE): 0.0855 loss_val (RMSE): 0.2845\n",
      "Epoch: 0175 loss_train (RMSE): 0.0855 loss_val (RMSE): 0.2844\n",
      "Epoch: 0176 loss_train (RMSE): 0.0855 loss_val (RMSE): 0.2842\n",
      "Epoch: 0177 loss_train (RMSE): 0.0854 loss_val (RMSE): 0.2841\n",
      "Epoch: 0178 loss_train (RMSE): 0.0854 loss_val (RMSE): 0.2840\n",
      "Epoch: 0179 loss_train (RMSE): 0.0854 loss_val (RMSE): 0.2839\n",
      "Epoch: 0180 loss_train (RMSE): 0.0854 loss_val (RMSE): 0.2838\n",
      "Epoch: 0181 loss_train (RMSE): 0.0853 loss_val (RMSE): 0.2837\n",
      "Epoch: 0182 loss_train (RMSE): 0.0853 loss_val (RMSE): 0.2836\n",
      "Epoch: 0183 loss_train (RMSE): 0.0853 loss_val (RMSE): 0.2835\n",
      "Epoch: 0184 loss_train (RMSE): 0.0853 loss_val (RMSE): 0.2834\n",
      "Epoch: 0185 loss_train (RMSE): 0.0852 loss_val (RMSE): 0.2834\n",
      "Epoch: 0186 loss_train (RMSE): 0.0852 loss_val (RMSE): 0.2833\n",
      "Epoch: 0187 loss_train (RMSE): 0.0852 loss_val (RMSE): 0.2832\n",
      "Epoch: 0188 loss_train (RMSE): 0.0852 loss_val (RMSE): 0.2831\n",
      "Epoch: 0189 loss_train (RMSE): 0.0852 loss_val (RMSE): 0.2830\n",
      "Epoch: 0190 loss_train (RMSE): 0.0852 loss_val (RMSE): 0.2830\n",
      "Epoch: 0191 loss_train (RMSE): 0.0852 loss_val (RMSE): 0.2829\n",
      "Epoch: 0192 loss_train (RMSE): 0.0851 loss_val (RMSE): 0.2828\n",
      "Epoch: 0193 loss_train (RMSE): 0.0851 loss_val (RMSE): 0.2828\n",
      "Epoch: 0194 loss_train (RMSE): 0.0851 loss_val (RMSE): 0.2827\n",
      "Epoch: 0195 loss_train (RMSE): 0.0851 loss_val (RMSE): 0.2827\n",
      "Epoch: 0196 loss_train (RMSE): 0.0851 loss_val (RMSE): 0.2826\n",
      "Epoch: 0197 loss_train (RMSE): 0.0851 loss_val (RMSE): 0.2826\n",
      "Epoch: 0198 loss_train (RMSE): 0.0851 loss_val (RMSE): 0.2825\n",
      "Epoch: 0199 loss_train (RMSE): 0.0851 loss_val (RMSE): 0.2825\n",
      "Epoch: 0200 loss_train (RMSE): 0.0851 loss_val (RMSE): 0.2824\n",
      "Optimization Finished!\n",
      "Total time elapsed: 0.3866s\n",
      "Test set results: loss= 0.2593 (RMSE)\n",
      "GCN(\n",
      "  (gc1): GraphConvolution (10 -> 2000)\n",
      "  (gc5): GraphConvolution (2000 -> 2)\n",
      ")\n",
      "Epoch: 0001 loss_train (RMSE): 0.6523 loss_val (RMSE): 0.7955\n",
      "Epoch: 0002 loss_train (RMSE): 0.6380 loss_val (RMSE): 0.7871\n",
      "Epoch: 0003 loss_train (RMSE): 0.6239 loss_val (RMSE): 0.7788\n",
      "Epoch: 0004 loss_train (RMSE): 0.6100 loss_val (RMSE): 0.7705\n",
      "Epoch: 0005 loss_train (RMSE): 0.5963 loss_val (RMSE): 0.7623\n",
      "Epoch: 0006 loss_train (RMSE): 0.5828 loss_val (RMSE): 0.7541\n",
      "Epoch: 0007 loss_train (RMSE): 0.5695 loss_val (RMSE): 0.7460\n",
      "Epoch: 0008 loss_train (RMSE): 0.5565 loss_val (RMSE): 0.7379\n",
      "Epoch: 0009 loss_train (RMSE): 0.5436 loss_val (RMSE): 0.7300\n",
      "Epoch: 0010 loss_train (RMSE): 0.5310 loss_val (RMSE): 0.7220\n",
      "Epoch: 0011 loss_train (RMSE): 0.5187 loss_val (RMSE): 0.7142\n",
      "Epoch: 0012 loss_train (RMSE): 0.5065 loss_val (RMSE): 0.7064\n",
      "Epoch: 0013 loss_train (RMSE): 0.4946 loss_val (RMSE): 0.6987\n",
      "Epoch: 0014 loss_train (RMSE): 0.4829 loss_val (RMSE): 0.6910\n",
      "Epoch: 0015 loss_train (RMSE): 0.4715 loss_val (RMSE): 0.6835\n",
      "Epoch: 0016 loss_train (RMSE): 0.4603 loss_val (RMSE): 0.6760\n",
      "Epoch: 0017 loss_train (RMSE): 0.4493 loss_val (RMSE): 0.6686\n",
      "Epoch: 0018 loss_train (RMSE): 0.4385 loss_val (RMSE): 0.6613\n",
      "Epoch: 0019 loss_train (RMSE): 0.4280 loss_val (RMSE): 0.6541\n",
      "Epoch: 0020 loss_train (RMSE): 0.4177 loss_val (RMSE): 0.6470\n",
      "Epoch: 0021 loss_train (RMSE): 0.4077 loss_val (RMSE): 0.6399\n",
      "Epoch: 0022 loss_train (RMSE): 0.3979 loss_val (RMSE): 0.6330\n",
      "Epoch: 0023 loss_train (RMSE): 0.3883 loss_val (RMSE): 0.6262\n",
      "Epoch: 0024 loss_train (RMSE): 0.3789 loss_val (RMSE): 0.6194\n",
      "Epoch: 0025 loss_train (RMSE): 0.3698 loss_val (RMSE): 0.6128\n",
      "Epoch: 0026 loss_train (RMSE): 0.3609 loss_val (RMSE): 0.6062\n",
      "Epoch: 0027 loss_train (RMSE): 0.3522 loss_val (RMSE): 0.5998\n",
      "Epoch: 0028 loss_train (RMSE): 0.3437 loss_val (RMSE): 0.5934\n",
      "Epoch: 0029 loss_train (RMSE): 0.3355 loss_val (RMSE): 0.5872\n",
      "Epoch: 0030 loss_train (RMSE): 0.3274 loss_val (RMSE): 0.5810\n",
      "Epoch: 0031 loss_train (RMSE): 0.3196 loss_val (RMSE): 0.5750\n",
      "Epoch: 0032 loss_train (RMSE): 0.3120 loss_val (RMSE): 0.5690\n",
      "Epoch: 0033 loss_train (RMSE): 0.3046 loss_val (RMSE): 0.5632\n",
      "Epoch: 0034 loss_train (RMSE): 0.2974 loss_val (RMSE): 0.5574\n",
      "Epoch: 0035 loss_train (RMSE): 0.2903 loss_val (RMSE): 0.5518\n",
      "Epoch: 0036 loss_train (RMSE): 0.2835 loss_val (RMSE): 0.5462\n",
      "Epoch: 0037 loss_train (RMSE): 0.2769 loss_val (RMSE): 0.5408\n",
      "Epoch: 0038 loss_train (RMSE): 0.2704 loss_val (RMSE): 0.5354\n",
      "Epoch: 0039 loss_train (RMSE): 0.2642 loss_val (RMSE): 0.5302\n",
      "Epoch: 0040 loss_train (RMSE): 0.2581 loss_val (RMSE): 0.5250\n",
      "Epoch: 0041 loss_train (RMSE): 0.2522 loss_val (RMSE): 0.5199\n",
      "Epoch: 0042 loss_train (RMSE): 0.2464 loss_val (RMSE): 0.5150\n",
      "Epoch: 0043 loss_train (RMSE): 0.2409 loss_val (RMSE): 0.5101\n",
      "Epoch: 0044 loss_train (RMSE): 0.2354 loss_val (RMSE): 0.5053\n",
      "Epoch: 0045 loss_train (RMSE): 0.2302 loss_val (RMSE): 0.5006\n",
      "Epoch: 0046 loss_train (RMSE): 0.2251 loss_val (RMSE): 0.4959\n",
      "Epoch: 0047 loss_train (RMSE): 0.2201 loss_val (RMSE): 0.4914\n",
      "Epoch: 0048 loss_train (RMSE): 0.2153 loss_val (RMSE): 0.4869\n",
      "Epoch: 0049 loss_train (RMSE): 0.2107 loss_val (RMSE): 0.4826\n",
      "Epoch: 0050 loss_train (RMSE): 0.2061 loss_val (RMSE): 0.4783\n",
      "Epoch: 0051 loss_train (RMSE): 0.2017 loss_val (RMSE): 0.4741\n",
      "Epoch: 0052 loss_train (RMSE): 0.1975 loss_val (RMSE): 0.4699\n",
      "Epoch: 0053 loss_train (RMSE): 0.1933 loss_val (RMSE): 0.4658\n",
      "Epoch: 0054 loss_train (RMSE): 0.1893 loss_val (RMSE): 0.4618\n",
      "Epoch: 0055 loss_train (RMSE): 0.1854 loss_val (RMSE): 0.4579\n",
      "Epoch: 0056 loss_train (RMSE): 0.1816 loss_val (RMSE): 0.4541\n",
      "Epoch: 0057 loss_train (RMSE): 0.1780 loss_val (RMSE): 0.4503\n",
      "Epoch: 0058 loss_train (RMSE): 0.1744 loss_val (RMSE): 0.4465\n",
      "Epoch: 0059 loss_train (RMSE): 0.1710 loss_val (RMSE): 0.4429\n",
      "Epoch: 0060 loss_train (RMSE): 0.1676 loss_val (RMSE): 0.4393\n",
      "Epoch: 0061 loss_train (RMSE): 0.1643 loss_val (RMSE): 0.4358\n",
      "Epoch: 0062 loss_train (RMSE): 0.1612 loss_val (RMSE): 0.4323\n",
      "Epoch: 0063 loss_train (RMSE): 0.1581 loss_val (RMSE): 0.4289\n",
      "Epoch: 0064 loss_train (RMSE): 0.1552 loss_val (RMSE): 0.4255\n",
      "Epoch: 0065 loss_train (RMSE): 0.1523 loss_val (RMSE): 0.4222\n",
      "Epoch: 0066 loss_train (RMSE): 0.1495 loss_val (RMSE): 0.4190\n",
      "Epoch: 0067 loss_train (RMSE): 0.1468 loss_val (RMSE): 0.4158\n",
      "Epoch: 0068 loss_train (RMSE): 0.1441 loss_val (RMSE): 0.4126\n",
      "Epoch: 0069 loss_train (RMSE): 0.1416 loss_val (RMSE): 0.4096\n",
      "Epoch: 0070 loss_train (RMSE): 0.1391 loss_val (RMSE): 0.4065\n",
      "Epoch: 0071 loss_train (RMSE): 0.1367 loss_val (RMSE): 0.4036\n",
      "Epoch: 0072 loss_train (RMSE): 0.1343 loss_val (RMSE): 0.4006\n",
      "Epoch: 0073 loss_train (RMSE): 0.1321 loss_val (RMSE): 0.3978\n",
      "Epoch: 0074 loss_train (RMSE): 0.1299 loss_val (RMSE): 0.3949\n",
      "Epoch: 0075 loss_train (RMSE): 0.1277 loss_val (RMSE): 0.3922\n",
      "Epoch: 0076 loss_train (RMSE): 0.1257 loss_val (RMSE): 0.3894\n",
      "Epoch: 0077 loss_train (RMSE): 0.1237 loss_val (RMSE): 0.3868\n",
      "Epoch: 0078 loss_train (RMSE): 0.1217 loss_val (RMSE): 0.3841\n",
      "Epoch: 0079 loss_train (RMSE): 0.1199 loss_val (RMSE): 0.3815\n",
      "Epoch: 0080 loss_train (RMSE): 0.1180 loss_val (RMSE): 0.3790\n",
      "Epoch: 0081 loss_train (RMSE): 0.1163 loss_val (RMSE): 0.3765\n",
      "Epoch: 0082 loss_train (RMSE): 0.1146 loss_val (RMSE): 0.3741\n",
      "Epoch: 0083 loss_train (RMSE): 0.1129 loss_val (RMSE): 0.3717\n",
      "Epoch: 0084 loss_train (RMSE): 0.1113 loss_val (RMSE): 0.3694\n",
      "Epoch: 0085 loss_train (RMSE): 0.1097 loss_val (RMSE): 0.3671\n",
      "Epoch: 0086 loss_train (RMSE): 0.1082 loss_val (RMSE): 0.3648\n",
      "Epoch: 0087 loss_train (RMSE): 0.1068 loss_val (RMSE): 0.3626\n",
      "Epoch: 0088 loss_train (RMSE): 0.1054 loss_val (RMSE): 0.3605\n",
      "Epoch: 0089 loss_train (RMSE): 0.1040 loss_val (RMSE): 0.3583\n",
      "Epoch: 0090 loss_train (RMSE): 0.1027 loss_val (RMSE): 0.3563\n",
      "Epoch: 0091 loss_train (RMSE): 0.1014 loss_val (RMSE): 0.3542\n",
      "Epoch: 0092 loss_train (RMSE): 0.1002 loss_val (RMSE): 0.3523\n",
      "Epoch: 0093 loss_train (RMSE): 0.0990 loss_val (RMSE): 0.3503\n",
      "Epoch: 0094 loss_train (RMSE): 0.0979 loss_val (RMSE): 0.3484\n",
      "Epoch: 0095 loss_train (RMSE): 0.0967 loss_val (RMSE): 0.3466\n",
      "Epoch: 0096 loss_train (RMSE): 0.0957 loss_val (RMSE): 0.3448\n",
      "Epoch: 0097 loss_train (RMSE): 0.0946 loss_val (RMSE): 0.3430\n",
      "Epoch: 0098 loss_train (RMSE): 0.0936 loss_val (RMSE): 0.3413\n",
      "Epoch: 0099 loss_train (RMSE): 0.0927 loss_val (RMSE): 0.3397\n",
      "Epoch: 0100 loss_train (RMSE): 0.0917 loss_val (RMSE): 0.3380\n",
      "Epoch: 0101 loss_train (RMSE): 0.0908 loss_val (RMSE): 0.3364\n",
      "Epoch: 0102 loss_train (RMSE): 0.0900 loss_val (RMSE): 0.3349\n",
      "Epoch: 0103 loss_train (RMSE): 0.0892 loss_val (RMSE): 0.3334\n",
      "Epoch: 0104 loss_train (RMSE): 0.0884 loss_val (RMSE): 0.3319\n",
      "Epoch: 0105 loss_train (RMSE): 0.0876 loss_val (RMSE): 0.3305\n",
      "Epoch: 0106 loss_train (RMSE): 0.0868 loss_val (RMSE): 0.3291\n",
      "Epoch: 0107 loss_train (RMSE): 0.0861 loss_val (RMSE): 0.3277\n",
      "Epoch: 0108 loss_train (RMSE): 0.0854 loss_val (RMSE): 0.3264\n",
      "Epoch: 0109 loss_train (RMSE): 0.0848 loss_val (RMSE): 0.3252\n",
      "Epoch: 0110 loss_train (RMSE): 0.0841 loss_val (RMSE): 0.3239\n",
      "Epoch: 0111 loss_train (RMSE): 0.0835 loss_val (RMSE): 0.3227\n",
      "Epoch: 0112 loss_train (RMSE): 0.0829 loss_val (RMSE): 0.3216\n",
      "Epoch: 0113 loss_train (RMSE): 0.0824 loss_val (RMSE): 0.3204\n",
      "Epoch: 0114 loss_train (RMSE): 0.0818 loss_val (RMSE): 0.3193\n",
      "Epoch: 0115 loss_train (RMSE): 0.0813 loss_val (RMSE): 0.3183\n",
      "Epoch: 0116 loss_train (RMSE): 0.0808 loss_val (RMSE): 0.3172\n",
      "Epoch: 0117 loss_train (RMSE): 0.0803 loss_val (RMSE): 0.3162\n",
      "Epoch: 0118 loss_train (RMSE): 0.0799 loss_val (RMSE): 0.3153\n",
      "Epoch: 0119 loss_train (RMSE): 0.0794 loss_val (RMSE): 0.3143\n",
      "Epoch: 0120 loss_train (RMSE): 0.0790 loss_val (RMSE): 0.3134\n",
      "Epoch: 0121 loss_train (RMSE): 0.0786 loss_val (RMSE): 0.3126\n",
      "Epoch: 0122 loss_train (RMSE): 0.0782 loss_val (RMSE): 0.3117\n",
      "Epoch: 0123 loss_train (RMSE): 0.0778 loss_val (RMSE): 0.3109\n",
      "Epoch: 0124 loss_train (RMSE): 0.0775 loss_val (RMSE): 0.3101\n",
      "Epoch: 0125 loss_train (RMSE): 0.0771 loss_val (RMSE): 0.3094\n",
      "Epoch: 0126 loss_train (RMSE): 0.0768 loss_val (RMSE): 0.3086\n",
      "Epoch: 0127 loss_train (RMSE): 0.0765 loss_val (RMSE): 0.3079\n",
      "Epoch: 0128 loss_train (RMSE): 0.0762 loss_val (RMSE): 0.3072\n",
      "Epoch: 0129 loss_train (RMSE): 0.0759 loss_val (RMSE): 0.3066\n",
      "Epoch: 0130 loss_train (RMSE): 0.0756 loss_val (RMSE): 0.3059\n",
      "Epoch: 0131 loss_train (RMSE): 0.0753 loss_val (RMSE): 0.3053\n",
      "Epoch: 0132 loss_train (RMSE): 0.0751 loss_val (RMSE): 0.3047\n",
      "Epoch: 0133 loss_train (RMSE): 0.0748 loss_val (RMSE): 0.3042\n",
      "Epoch: 0134 loss_train (RMSE): 0.0746 loss_val (RMSE): 0.3036\n",
      "Epoch: 0135 loss_train (RMSE): 0.0744 loss_val (RMSE): 0.3031\n",
      "Epoch: 0136 loss_train (RMSE): 0.0742 loss_val (RMSE): 0.3026\n",
      "Epoch: 0137 loss_train (RMSE): 0.0740 loss_val (RMSE): 0.3021\n",
      "Epoch: 0138 loss_train (RMSE): 0.0738 loss_val (RMSE): 0.3016\n",
      "Epoch: 0139 loss_train (RMSE): 0.0736 loss_val (RMSE): 0.3012\n",
      "Epoch: 0140 loss_train (RMSE): 0.0734 loss_val (RMSE): 0.3007\n",
      "Epoch: 0141 loss_train (RMSE): 0.0733 loss_val (RMSE): 0.3003\n",
      "Epoch: 0142 loss_train (RMSE): 0.0731 loss_val (RMSE): 0.2999\n",
      "Epoch: 0143 loss_train (RMSE): 0.0730 loss_val (RMSE): 0.2995\n",
      "Epoch: 0144 loss_train (RMSE): 0.0728 loss_val (RMSE): 0.2991\n",
      "Epoch: 0145 loss_train (RMSE): 0.0727 loss_val (RMSE): 0.2988\n",
      "Epoch: 0146 loss_train (RMSE): 0.0726 loss_val (RMSE): 0.2984\n",
      "Epoch: 0147 loss_train (RMSE): 0.0724 loss_val (RMSE): 0.2981\n",
      "Epoch: 0148 loss_train (RMSE): 0.0723 loss_val (RMSE): 0.2978\n",
      "Epoch: 0149 loss_train (RMSE): 0.0722 loss_val (RMSE): 0.2975\n",
      "Epoch: 0150 loss_train (RMSE): 0.0721 loss_val (RMSE): 0.2972\n",
      "Epoch: 0151 loss_train (RMSE): 0.0720 loss_val (RMSE): 0.2969\n",
      "Epoch: 0152 loss_train (RMSE): 0.0719 loss_val (RMSE): 0.2966\n",
      "Epoch: 0153 loss_train (RMSE): 0.0718 loss_val (RMSE): 0.2964\n",
      "Epoch: 0154 loss_train (RMSE): 0.0717 loss_val (RMSE): 0.2961\n",
      "Epoch: 0155 loss_train (RMSE): 0.0717 loss_val (RMSE): 0.2959\n",
      "Epoch: 0156 loss_train (RMSE): 0.0716 loss_val (RMSE): 0.2956\n",
      "Epoch: 0157 loss_train (RMSE): 0.0715 loss_val (RMSE): 0.2954\n",
      "Epoch: 0158 loss_train (RMSE): 0.0714 loss_val (RMSE): 0.2952\n",
      "Epoch: 0159 loss_train (RMSE): 0.0714 loss_val (RMSE): 0.2950\n",
      "Epoch: 0160 loss_train (RMSE): 0.0713 loss_val (RMSE): 0.2948\n",
      "Epoch: 0161 loss_train (RMSE): 0.0713 loss_val (RMSE): 0.2946\n",
      "Epoch: 0162 loss_train (RMSE): 0.0712 loss_val (RMSE): 0.2944\n",
      "Epoch: 0163 loss_train (RMSE): 0.0711 loss_val (RMSE): 0.2942\n",
      "Epoch: 0164 loss_train (RMSE): 0.0711 loss_val (RMSE): 0.2941\n",
      "Epoch: 0165 loss_train (RMSE): 0.0710 loss_val (RMSE): 0.2939\n",
      "Epoch: 0166 loss_train (RMSE): 0.0710 loss_val (RMSE): 0.2938\n",
      "Epoch: 0167 loss_train (RMSE): 0.0710 loss_val (RMSE): 0.2936\n",
      "Epoch: 0168 loss_train (RMSE): 0.0709 loss_val (RMSE): 0.2935\n",
      "Epoch: 0169 loss_train (RMSE): 0.0709 loss_val (RMSE): 0.2933\n",
      "Epoch: 0170 loss_train (RMSE): 0.0708 loss_val (RMSE): 0.2932\n",
      "Epoch: 0171 loss_train (RMSE): 0.0708 loss_val (RMSE): 0.2931\n",
      "Epoch: 0172 loss_train (RMSE): 0.0708 loss_val (RMSE): 0.2929\n",
      "Epoch: 0173 loss_train (RMSE): 0.0707 loss_val (RMSE): 0.2928\n",
      "Epoch: 0174 loss_train (RMSE): 0.0707 loss_val (RMSE): 0.2927\n",
      "Epoch: 0175 loss_train (RMSE): 0.0707 loss_val (RMSE): 0.2926\n",
      "Epoch: 0176 loss_train (RMSE): 0.0707 loss_val (RMSE): 0.2925\n",
      "Epoch: 0177 loss_train (RMSE): 0.0706 loss_val (RMSE): 0.2924\n",
      "Epoch: 0178 loss_train (RMSE): 0.0706 loss_val (RMSE): 0.2923\n",
      "Epoch: 0179 loss_train (RMSE): 0.0706 loss_val (RMSE): 0.2922\n",
      "Epoch: 0180 loss_train (RMSE): 0.0706 loss_val (RMSE): 0.2921\n",
      "Epoch: 0181 loss_train (RMSE): 0.0706 loss_val (RMSE): 0.2920\n",
      "Epoch: 0182 loss_train (RMSE): 0.0705 loss_val (RMSE): 0.2919\n",
      "Epoch: 0183 loss_train (RMSE): 0.0705 loss_val (RMSE): 0.2919\n",
      "Epoch: 0184 loss_train (RMSE): 0.0705 loss_val (RMSE): 0.2918\n",
      "Epoch: 0185 loss_train (RMSE): 0.0705 loss_val (RMSE): 0.2917\n",
      "Epoch: 0186 loss_train (RMSE): 0.0705 loss_val (RMSE): 0.2916\n",
      "Epoch: 0187 loss_train (RMSE): 0.0705 loss_val (RMSE): 0.2916\n",
      "Epoch: 0188 loss_train (RMSE): 0.0705 loss_val (RMSE): 0.2915\n",
      "Epoch: 0189 loss_train (RMSE): 0.0704 loss_val (RMSE): 0.2914\n",
      "Epoch: 0190 loss_train (RMSE): 0.0704 loss_val (RMSE): 0.2914\n",
      "Epoch: 0191 loss_train (RMSE): 0.0704 loss_val (RMSE): 0.2913\n",
      "Epoch: 0192 loss_train (RMSE): 0.0704 loss_val (RMSE): 0.2913\n",
      "Epoch: 0193 loss_train (RMSE): 0.0704 loss_val (RMSE): 0.2912\n",
      "Epoch: 0194 loss_train (RMSE): 0.0704 loss_val (RMSE): 0.2911\n",
      "Epoch: 0195 loss_train (RMSE): 0.0704 loss_val (RMSE): 0.2911\n",
      "Epoch: 0196 loss_train (RMSE): 0.0704 loss_val (RMSE): 0.2910\n",
      "Epoch: 0197 loss_train (RMSE): 0.0704 loss_val (RMSE): 0.2910\n",
      "Epoch: 0198 loss_train (RMSE): 0.0704 loss_val (RMSE): 0.2910\n",
      "Epoch: 0199 loss_train (RMSE): 0.0704 loss_val (RMSE): 0.2909\n",
      "Epoch: 0200 loss_train (RMSE): 0.0704 loss_val (RMSE): 0.2909\n",
      "Optimization Finished!\n",
      "Total time elapsed: 0.3957s\n",
      "Test set results: loss= 0.2855 (RMSE)\n",
      "GCN(\n",
      "  (gc1): GraphConvolution (10 -> 2000)\n",
      "  (gc5): GraphConvolution (2000 -> 2)\n",
      ")\n",
      "Epoch: 0001 loss_train (RMSE): 0.9256 loss_val (RMSE): 1.0475\n",
      "Epoch: 0002 loss_train (RMSE): 0.9076 loss_val (RMSE): 1.0382\n",
      "Epoch: 0003 loss_train (RMSE): 0.8898 loss_val (RMSE): 1.0289\n",
      "Epoch: 0004 loss_train (RMSE): 0.8722 loss_val (RMSE): 1.0196\n",
      "Epoch: 0005 loss_train (RMSE): 0.8548 loss_val (RMSE): 1.0103\n",
      "Epoch: 0006 loss_train (RMSE): 0.8376 loss_val (RMSE): 1.0010\n",
      "Epoch: 0007 loss_train (RMSE): 0.8207 loss_val (RMSE): 0.9918\n",
      "Epoch: 0008 loss_train (RMSE): 0.8039 loss_val (RMSE): 0.9826\n",
      "Epoch: 0009 loss_train (RMSE): 0.7874 loss_val (RMSE): 0.9734\n",
      "Epoch: 0010 loss_train (RMSE): 0.7712 loss_val (RMSE): 0.9643\n",
      "Epoch: 0011 loss_train (RMSE): 0.7551 loss_val (RMSE): 0.9552\n",
      "Epoch: 0012 loss_train (RMSE): 0.7393 loss_val (RMSE): 0.9461\n",
      "Epoch: 0013 loss_train (RMSE): 0.7237 loss_val (RMSE): 0.9371\n",
      "Epoch: 0014 loss_train (RMSE): 0.7083 loss_val (RMSE): 0.9281\n",
      "Epoch: 0015 loss_train (RMSE): 0.6932 loss_val (RMSE): 0.9192\n",
      "Epoch: 0016 loss_train (RMSE): 0.6783 loss_val (RMSE): 0.9103\n",
      "Epoch: 0017 loss_train (RMSE): 0.6637 loss_val (RMSE): 0.9014\n",
      "Epoch: 0018 loss_train (RMSE): 0.6493 loss_val (RMSE): 0.8926\n",
      "Epoch: 0019 loss_train (RMSE): 0.6351 loss_val (RMSE): 0.8839\n",
      "Epoch: 0020 loss_train (RMSE): 0.6212 loss_val (RMSE): 0.8752\n",
      "Epoch: 0021 loss_train (RMSE): 0.6075 loss_val (RMSE): 0.8666\n",
      "Epoch: 0022 loss_train (RMSE): 0.5941 loss_val (RMSE): 0.8580\n",
      "Epoch: 0023 loss_train (RMSE): 0.5809 loss_val (RMSE): 0.8495\n",
      "Epoch: 0024 loss_train (RMSE): 0.5680 loss_val (RMSE): 0.8410\n",
      "Epoch: 0025 loss_train (RMSE): 0.5552 loss_val (RMSE): 0.8326\n",
      "Epoch: 0026 loss_train (RMSE): 0.5428 loss_val (RMSE): 0.8243\n",
      "Epoch: 0027 loss_train (RMSE): 0.5305 loss_val (RMSE): 0.8160\n",
      "Epoch: 0028 loss_train (RMSE): 0.5185 loss_val (RMSE): 0.8078\n",
      "Epoch: 0029 loss_train (RMSE): 0.5068 loss_val (RMSE): 0.7997\n",
      "Epoch: 0030 loss_train (RMSE): 0.4952 loss_val (RMSE): 0.7916\n",
      "Epoch: 0031 loss_train (RMSE): 0.4840 loss_val (RMSE): 0.7836\n",
      "Epoch: 0032 loss_train (RMSE): 0.4729 loss_val (RMSE): 0.7757\n",
      "Epoch: 0033 loss_train (RMSE): 0.4621 loss_val (RMSE): 0.7679\n",
      "Epoch: 0034 loss_train (RMSE): 0.4515 loss_val (RMSE): 0.7601\n",
      "Epoch: 0035 loss_train (RMSE): 0.4411 loss_val (RMSE): 0.7524\n",
      "Epoch: 0036 loss_train (RMSE): 0.4309 loss_val (RMSE): 0.7448\n",
      "Epoch: 0037 loss_train (RMSE): 0.4210 loss_val (RMSE): 0.7372\n",
      "Epoch: 0038 loss_train (RMSE): 0.4113 loss_val (RMSE): 0.7297\n",
      "Epoch: 0039 loss_train (RMSE): 0.4018 loss_val (RMSE): 0.7224\n",
      "Epoch: 0040 loss_train (RMSE): 0.3925 loss_val (RMSE): 0.7150\n",
      "Epoch: 0041 loss_train (RMSE): 0.3835 loss_val (RMSE): 0.7078\n",
      "Epoch: 0042 loss_train (RMSE): 0.3746 loss_val (RMSE): 0.7007\n",
      "Epoch: 0043 loss_train (RMSE): 0.3660 loss_val (RMSE): 0.6936\n",
      "Epoch: 0044 loss_train (RMSE): 0.3575 loss_val (RMSE): 0.6866\n",
      "Epoch: 0045 loss_train (RMSE): 0.3493 loss_val (RMSE): 0.6797\n",
      "Epoch: 0046 loss_train (RMSE): 0.3413 loss_val (RMSE): 0.6728\n",
      "Epoch: 0047 loss_train (RMSE): 0.3334 loss_val (RMSE): 0.6661\n",
      "Epoch: 0048 loss_train (RMSE): 0.3257 loss_val (RMSE): 0.6594\n",
      "Epoch: 0049 loss_train (RMSE): 0.3183 loss_val (RMSE): 0.6529\n",
      "Epoch: 0050 loss_train (RMSE): 0.3110 loss_val (RMSE): 0.6464\n",
      "Epoch: 0051 loss_train (RMSE): 0.3039 loss_val (RMSE): 0.6399\n",
      "Epoch: 0052 loss_train (RMSE): 0.2970 loss_val (RMSE): 0.6336\n",
      "Epoch: 0053 loss_train (RMSE): 0.2902 loss_val (RMSE): 0.6274\n",
      "Epoch: 0054 loss_train (RMSE): 0.2837 loss_val (RMSE): 0.6212\n",
      "Epoch: 0055 loss_train (RMSE): 0.2773 loss_val (RMSE): 0.6151\n",
      "Epoch: 0056 loss_train (RMSE): 0.2710 loss_val (RMSE): 0.6091\n",
      "Epoch: 0057 loss_train (RMSE): 0.2650 loss_val (RMSE): 0.6032\n",
      "Epoch: 0058 loss_train (RMSE): 0.2590 loss_val (RMSE): 0.5973\n",
      "Epoch: 0059 loss_train (RMSE): 0.2533 loss_val (RMSE): 0.5916\n",
      "Epoch: 0060 loss_train (RMSE): 0.2477 loss_val (RMSE): 0.5859\n",
      "Epoch: 0061 loss_train (RMSE): 0.2422 loss_val (RMSE): 0.5803\n",
      "Epoch: 0062 loss_train (RMSE): 0.2369 loss_val (RMSE): 0.5748\n",
      "Epoch: 0063 loss_train (RMSE): 0.2318 loss_val (RMSE): 0.5693\n",
      "Epoch: 0064 loss_train (RMSE): 0.2268 loss_val (RMSE): 0.5640\n",
      "Epoch: 0065 loss_train (RMSE): 0.2219 loss_val (RMSE): 0.5587\n",
      "Epoch: 0066 loss_train (RMSE): 0.2172 loss_val (RMSE): 0.5535\n",
      "Epoch: 0067 loss_train (RMSE): 0.2125 loss_val (RMSE): 0.5484\n",
      "Epoch: 0068 loss_train (RMSE): 0.2081 loss_val (RMSE): 0.5434\n",
      "Epoch: 0069 loss_train (RMSE): 0.2037 loss_val (RMSE): 0.5384\n",
      "Epoch: 0070 loss_train (RMSE): 0.1995 loss_val (RMSE): 0.5335\n",
      "Epoch: 0071 loss_train (RMSE): 0.1954 loss_val (RMSE): 0.5287\n",
      "Epoch: 0072 loss_train (RMSE): 0.1914 loss_val (RMSE): 0.5240\n",
      "Epoch: 0073 loss_train (RMSE): 0.1875 loss_val (RMSE): 0.5193\n",
      "Epoch: 0074 loss_train (RMSE): 0.1837 loss_val (RMSE): 0.5148\n",
      "Epoch: 0075 loss_train (RMSE): 0.1801 loss_val (RMSE): 0.5103\n",
      "Epoch: 0076 loss_train (RMSE): 0.1765 loss_val (RMSE): 0.5058\n",
      "Epoch: 0077 loss_train (RMSE): 0.1731 loss_val (RMSE): 0.5015\n",
      "Epoch: 0078 loss_train (RMSE): 0.1697 loss_val (RMSE): 0.4972\n",
      "Epoch: 0079 loss_train (RMSE): 0.1665 loss_val (RMSE): 0.4930\n",
      "Epoch: 0080 loss_train (RMSE): 0.1633 loss_val (RMSE): 0.4888\n",
      "Epoch: 0081 loss_train (RMSE): 0.1603 loss_val (RMSE): 0.4848\n",
      "Epoch: 0082 loss_train (RMSE): 0.1573 loss_val (RMSE): 0.4808\n",
      "Epoch: 0083 loss_train (RMSE): 0.1545 loss_val (RMSE): 0.4768\n",
      "Epoch: 0084 loss_train (RMSE): 0.1517 loss_val (RMSE): 0.4730\n",
      "Epoch: 0085 loss_train (RMSE): 0.1490 loss_val (RMSE): 0.4692\n",
      "Epoch: 0086 loss_train (RMSE): 0.1463 loss_val (RMSE): 0.4654\n",
      "Epoch: 0087 loss_train (RMSE): 0.1438 loss_val (RMSE): 0.4618\n",
      "Epoch: 0088 loss_train (RMSE): 0.1413 loss_val (RMSE): 0.4582\n",
      "Epoch: 0089 loss_train (RMSE): 0.1389 loss_val (RMSE): 0.4546\n",
      "Epoch: 0090 loss_train (RMSE): 0.1366 loss_val (RMSE): 0.4512\n",
      "Epoch: 0091 loss_train (RMSE): 0.1344 loss_val (RMSE): 0.4478\n",
      "Epoch: 0092 loss_train (RMSE): 0.1322 loss_val (RMSE): 0.4444\n",
      "Epoch: 0093 loss_train (RMSE): 0.1301 loss_val (RMSE): 0.4411\n",
      "Epoch: 0094 loss_train (RMSE): 0.1281 loss_val (RMSE): 0.4379\n",
      "Epoch: 0095 loss_train (RMSE): 0.1261 loss_val (RMSE): 0.4347\n",
      "Epoch: 0096 loss_train (RMSE): 0.1242 loss_val (RMSE): 0.4316\n",
      "Epoch: 0097 loss_train (RMSE): 0.1223 loss_val (RMSE): 0.4286\n",
      "Epoch: 0098 loss_train (RMSE): 0.1205 loss_val (RMSE): 0.4256\n",
      "Epoch: 0099 loss_train (RMSE): 0.1188 loss_val (RMSE): 0.4227\n",
      "Epoch: 0100 loss_train (RMSE): 0.1171 loss_val (RMSE): 0.4198\n",
      "Epoch: 0101 loss_train (RMSE): 0.1155 loss_val (RMSE): 0.4170\n",
      "Epoch: 0102 loss_train (RMSE): 0.1139 loss_val (RMSE): 0.4142\n",
      "Epoch: 0103 loss_train (RMSE): 0.1124 loss_val (RMSE): 0.4115\n",
      "Epoch: 0104 loss_train (RMSE): 0.1109 loss_val (RMSE): 0.4088\n",
      "Epoch: 0105 loss_train (RMSE): 0.1095 loss_val (RMSE): 0.4062\n",
      "Epoch: 0106 loss_train (RMSE): 0.1081 loss_val (RMSE): 0.4037\n",
      "Epoch: 0107 loss_train (RMSE): 0.1067 loss_val (RMSE): 0.4012\n",
      "Epoch: 0108 loss_train (RMSE): 0.1054 loss_val (RMSE): 0.3987\n",
      "Epoch: 0109 loss_train (RMSE): 0.1042 loss_val (RMSE): 0.3963\n",
      "Epoch: 0110 loss_train (RMSE): 0.1030 loss_val (RMSE): 0.3939\n",
      "Epoch: 0111 loss_train (RMSE): 0.1018 loss_val (RMSE): 0.3916\n",
      "Epoch: 0112 loss_train (RMSE): 0.1007 loss_val (RMSE): 0.3894\n",
      "Epoch: 0113 loss_train (RMSE): 0.0996 loss_val (RMSE): 0.3871\n",
      "Epoch: 0114 loss_train (RMSE): 0.0985 loss_val (RMSE): 0.3850\n",
      "Epoch: 0115 loss_train (RMSE): 0.0975 loss_val (RMSE): 0.3828\n",
      "Epoch: 0116 loss_train (RMSE): 0.0965 loss_val (RMSE): 0.3808\n",
      "Epoch: 0117 loss_train (RMSE): 0.0956 loss_val (RMSE): 0.3787\n",
      "Epoch: 0118 loss_train (RMSE): 0.0946 loss_val (RMSE): 0.3767\n",
      "Epoch: 0119 loss_train (RMSE): 0.0938 loss_val (RMSE): 0.3748\n",
      "Epoch: 0120 loss_train (RMSE): 0.0929 loss_val (RMSE): 0.3729\n",
      "Epoch: 0121 loss_train (RMSE): 0.0921 loss_val (RMSE): 0.3710\n",
      "Epoch: 0122 loss_train (RMSE): 0.0913 loss_val (RMSE): 0.3692\n",
      "Epoch: 0123 loss_train (RMSE): 0.0905 loss_val (RMSE): 0.3674\n",
      "Epoch: 0124 loss_train (RMSE): 0.0897 loss_val (RMSE): 0.3656\n",
      "Epoch: 0125 loss_train (RMSE): 0.0890 loss_val (RMSE): 0.3639\n",
      "Epoch: 0126 loss_train (RMSE): 0.0883 loss_val (RMSE): 0.3622\n",
      "Epoch: 0127 loss_train (RMSE): 0.0877 loss_val (RMSE): 0.3606\n",
      "Epoch: 0128 loss_train (RMSE): 0.0870 loss_val (RMSE): 0.3590\n",
      "Epoch: 0129 loss_train (RMSE): 0.0864 loss_val (RMSE): 0.3574\n",
      "Epoch: 0130 loss_train (RMSE): 0.0858 loss_val (RMSE): 0.3559\n",
      "Epoch: 0131 loss_train (RMSE): 0.0852 loss_val (RMSE): 0.3544\n",
      "Epoch: 0132 loss_train (RMSE): 0.0846 loss_val (RMSE): 0.3529\n",
      "Epoch: 0133 loss_train (RMSE): 0.0841 loss_val (RMSE): 0.3515\n",
      "Epoch: 0134 loss_train (RMSE): 0.0836 loss_val (RMSE): 0.3501\n",
      "Epoch: 0135 loss_train (RMSE): 0.0831 loss_val (RMSE): 0.3487\n",
      "Epoch: 0136 loss_train (RMSE): 0.0826 loss_val (RMSE): 0.3474\n",
      "Epoch: 0137 loss_train (RMSE): 0.0821 loss_val (RMSE): 0.3460\n",
      "Epoch: 0138 loss_train (RMSE): 0.0817 loss_val (RMSE): 0.3448\n",
      "Epoch: 0139 loss_train (RMSE): 0.0812 loss_val (RMSE): 0.3435\n",
      "Epoch: 0140 loss_train (RMSE): 0.0808 loss_val (RMSE): 0.3423\n",
      "Epoch: 0141 loss_train (RMSE): 0.0804 loss_val (RMSE): 0.3411\n",
      "Epoch: 0142 loss_train (RMSE): 0.0800 loss_val (RMSE): 0.3399\n",
      "Epoch: 0143 loss_train (RMSE): 0.0797 loss_val (RMSE): 0.3388\n",
      "Epoch: 0144 loss_train (RMSE): 0.0793 loss_val (RMSE): 0.3377\n",
      "Epoch: 0145 loss_train (RMSE): 0.0790 loss_val (RMSE): 0.3366\n",
      "Epoch: 0146 loss_train (RMSE): 0.0786 loss_val (RMSE): 0.3356\n",
      "Epoch: 0147 loss_train (RMSE): 0.0783 loss_val (RMSE): 0.3345\n",
      "Epoch: 0148 loss_train (RMSE): 0.0780 loss_val (RMSE): 0.3335\n",
      "Epoch: 0149 loss_train (RMSE): 0.0777 loss_val (RMSE): 0.3325\n",
      "Epoch: 0150 loss_train (RMSE): 0.0774 loss_val (RMSE): 0.3316\n",
      "Epoch: 0151 loss_train (RMSE): 0.0772 loss_val (RMSE): 0.3307\n",
      "Epoch: 0152 loss_train (RMSE): 0.0769 loss_val (RMSE): 0.3297\n",
      "Epoch: 0153 loss_train (RMSE): 0.0766 loss_val (RMSE): 0.3288\n",
      "Epoch: 0154 loss_train (RMSE): 0.0764 loss_val (RMSE): 0.3280\n",
      "Epoch: 0155 loss_train (RMSE): 0.0762 loss_val (RMSE): 0.3271\n",
      "Epoch: 0156 loss_train (RMSE): 0.0759 loss_val (RMSE): 0.3263\n",
      "Epoch: 0157 loss_train (RMSE): 0.0757 loss_val (RMSE): 0.3255\n",
      "Epoch: 0158 loss_train (RMSE): 0.0755 loss_val (RMSE): 0.3247\n",
      "Epoch: 0159 loss_train (RMSE): 0.0753 loss_val (RMSE): 0.3239\n",
      "Epoch: 0160 loss_train (RMSE): 0.0751 loss_val (RMSE): 0.3232\n",
      "Epoch: 0161 loss_train (RMSE): 0.0750 loss_val (RMSE): 0.3225\n",
      "Epoch: 0162 loss_train (RMSE): 0.0748 loss_val (RMSE): 0.3218\n",
      "Epoch: 0163 loss_train (RMSE): 0.0746 loss_val (RMSE): 0.3211\n",
      "Epoch: 0164 loss_train (RMSE): 0.0745 loss_val (RMSE): 0.3204\n",
      "Epoch: 0165 loss_train (RMSE): 0.0743 loss_val (RMSE): 0.3197\n",
      "Epoch: 0166 loss_train (RMSE): 0.0742 loss_val (RMSE): 0.3191\n",
      "Epoch: 0167 loss_train (RMSE): 0.0740 loss_val (RMSE): 0.3185\n",
      "Epoch: 0168 loss_train (RMSE): 0.0739 loss_val (RMSE): 0.3179\n",
      "Epoch: 0169 loss_train (RMSE): 0.0738 loss_val (RMSE): 0.3173\n",
      "Epoch: 0170 loss_train (RMSE): 0.0736 loss_val (RMSE): 0.3167\n",
      "Epoch: 0171 loss_train (RMSE): 0.0735 loss_val (RMSE): 0.3161\n",
      "Epoch: 0172 loss_train (RMSE): 0.0734 loss_val (RMSE): 0.3156\n",
      "Epoch: 0173 loss_train (RMSE): 0.0733 loss_val (RMSE): 0.3150\n",
      "Epoch: 0174 loss_train (RMSE): 0.0732 loss_val (RMSE): 0.3145\n",
      "Epoch: 0175 loss_train (RMSE): 0.0731 loss_val (RMSE): 0.3140\n",
      "Epoch: 0176 loss_train (RMSE): 0.0730 loss_val (RMSE): 0.3135\n",
      "Epoch: 0177 loss_train (RMSE): 0.0729 loss_val (RMSE): 0.3130\n",
      "Epoch: 0178 loss_train (RMSE): 0.0728 loss_val (RMSE): 0.3126\n",
      "Epoch: 0179 loss_train (RMSE): 0.0728 loss_val (RMSE): 0.3121\n",
      "Epoch: 0180 loss_train (RMSE): 0.0727 loss_val (RMSE): 0.3117\n",
      "Epoch: 0181 loss_train (RMSE): 0.0726 loss_val (RMSE): 0.3112\n",
      "Epoch: 0182 loss_train (RMSE): 0.0725 loss_val (RMSE): 0.3108\n",
      "Epoch: 0183 loss_train (RMSE): 0.0725 loss_val (RMSE): 0.3104\n",
      "Epoch: 0184 loss_train (RMSE): 0.0724 loss_val (RMSE): 0.3100\n",
      "Epoch: 0185 loss_train (RMSE): 0.0723 loss_val (RMSE): 0.3096\n",
      "Epoch: 0186 loss_train (RMSE): 0.0723 loss_val (RMSE): 0.3092\n",
      "Epoch: 0187 loss_train (RMSE): 0.0722 loss_val (RMSE): 0.3089\n",
      "Epoch: 0188 loss_train (RMSE): 0.0722 loss_val (RMSE): 0.3085\n",
      "Epoch: 0189 loss_train (RMSE): 0.0721 loss_val (RMSE): 0.3082\n",
      "Epoch: 0190 loss_train (RMSE): 0.0721 loss_val (RMSE): 0.3078\n",
      "Epoch: 0191 loss_train (RMSE): 0.0720 loss_val (RMSE): 0.3075\n",
      "Epoch: 0192 loss_train (RMSE): 0.0720 loss_val (RMSE): 0.3072\n",
      "Epoch: 0193 loss_train (RMSE): 0.0719 loss_val (RMSE): 0.3068\n",
      "Epoch: 0194 loss_train (RMSE): 0.0719 loss_val (RMSE): 0.3065\n",
      "Epoch: 0195 loss_train (RMSE): 0.0718 loss_val (RMSE): 0.3062\n",
      "Epoch: 0196 loss_train (RMSE): 0.0718 loss_val (RMSE): 0.3060\n",
      "Epoch: 0197 loss_train (RMSE): 0.0718 loss_val (RMSE): 0.3057\n",
      "Epoch: 0198 loss_train (RMSE): 0.0717 loss_val (RMSE): 0.3054\n",
      "Epoch: 0199 loss_train (RMSE): 0.0717 loss_val (RMSE): 0.3051\n",
      "Epoch: 0200 loss_train (RMSE): 0.0717 loss_val (RMSE): 0.3049\n",
      "Optimization Finished!\n",
      "Total time elapsed: 0.4072s\n",
      "Test set results: loss= 0.2758 (RMSE)\n",
      "GCN(\n",
      "  (gc1): GraphConvolution (10 -> 2000)\n",
      "  (gc5): GraphConvolution (2000 -> 2)\n",
      ")\n",
      "Epoch: 0001 loss_train (RMSE): 0.4758 loss_val (RMSE): 0.7158\n",
      "Epoch: 0002 loss_train (RMSE): 0.4635 loss_val (RMSE): 0.7068\n",
      "Epoch: 0003 loss_train (RMSE): 0.4514 loss_val (RMSE): 0.6979\n",
      "Epoch: 0004 loss_train (RMSE): 0.4395 loss_val (RMSE): 0.6890\n",
      "Epoch: 0005 loss_train (RMSE): 0.4278 loss_val (RMSE): 0.6802\n",
      "Epoch: 0006 loss_train (RMSE): 0.4164 loss_val (RMSE): 0.6714\n",
      "Epoch: 0007 loss_train (RMSE): 0.4051 loss_val (RMSE): 0.6626\n",
      "Epoch: 0008 loss_train (RMSE): 0.3941 loss_val (RMSE): 0.6539\n",
      "Epoch: 0009 loss_train (RMSE): 0.3833 loss_val (RMSE): 0.6453\n",
      "Epoch: 0010 loss_train (RMSE): 0.3728 loss_val (RMSE): 0.6367\n",
      "Epoch: 0011 loss_train (RMSE): 0.3624 loss_val (RMSE): 0.6282\n",
      "Epoch: 0012 loss_train (RMSE): 0.3523 loss_val (RMSE): 0.6198\n",
      "Epoch: 0013 loss_train (RMSE): 0.3424 loss_val (RMSE): 0.6114\n",
      "Epoch: 0014 loss_train (RMSE): 0.3328 loss_val (RMSE): 0.6031\n",
      "Epoch: 0015 loss_train (RMSE): 0.3234 loss_val (RMSE): 0.5949\n",
      "Epoch: 0016 loss_train (RMSE): 0.3142 loss_val (RMSE): 0.5868\n",
      "Epoch: 0017 loss_train (RMSE): 0.3052 loss_val (RMSE): 0.5788\n",
      "Epoch: 0018 loss_train (RMSE): 0.2965 loss_val (RMSE): 0.5708\n",
      "Epoch: 0019 loss_train (RMSE): 0.2880 loss_val (RMSE): 0.5630\n",
      "Epoch: 0020 loss_train (RMSE): 0.2797 loss_val (RMSE): 0.5553\n",
      "Epoch: 0021 loss_train (RMSE): 0.2717 loss_val (RMSE): 0.5476\n",
      "Epoch: 0022 loss_train (RMSE): 0.2639 loss_val (RMSE): 0.5401\n",
      "Epoch: 0023 loss_train (RMSE): 0.2564 loss_val (RMSE): 0.5327\n",
      "Epoch: 0024 loss_train (RMSE): 0.2490 loss_val (RMSE): 0.5253\n",
      "Epoch: 0025 loss_train (RMSE): 0.2419 loss_val (RMSE): 0.5182\n",
      "Epoch: 0026 loss_train (RMSE): 0.2350 loss_val (RMSE): 0.5111\n",
      "Epoch: 0027 loss_train (RMSE): 0.2284 loss_val (RMSE): 0.5041\n",
      "Epoch: 0028 loss_train (RMSE): 0.2219 loss_val (RMSE): 0.4973\n",
      "Epoch: 0029 loss_train (RMSE): 0.2157 loss_val (RMSE): 0.4906\n",
      "Epoch: 0030 loss_train (RMSE): 0.2096 loss_val (RMSE): 0.4840\n",
      "Epoch: 0031 loss_train (RMSE): 0.2038 loss_val (RMSE): 0.4775\n",
      "Epoch: 0032 loss_train (RMSE): 0.1982 loss_val (RMSE): 0.4712\n",
      "Epoch: 0033 loss_train (RMSE): 0.1928 loss_val (RMSE): 0.4650\n",
      "Epoch: 0034 loss_train (RMSE): 0.1876 loss_val (RMSE): 0.4589\n",
      "Epoch: 0035 loss_train (RMSE): 0.1826 loss_val (RMSE): 0.4530\n",
      "Epoch: 0036 loss_train (RMSE): 0.1778 loss_val (RMSE): 0.4472\n",
      "Epoch: 0037 loss_train (RMSE): 0.1732 loss_val (RMSE): 0.4415\n",
      "Epoch: 0038 loss_train (RMSE): 0.1687 loss_val (RMSE): 0.4360\n",
      "Epoch: 0039 loss_train (RMSE): 0.1645 loss_val (RMSE): 0.4306\n",
      "Epoch: 0040 loss_train (RMSE): 0.1604 loss_val (RMSE): 0.4254\n",
      "Epoch: 0041 loss_train (RMSE): 0.1565 loss_val (RMSE): 0.4202\n",
      "Epoch: 0042 loss_train (RMSE): 0.1527 loss_val (RMSE): 0.4153\n",
      "Epoch: 0043 loss_train (RMSE): 0.1491 loss_val (RMSE): 0.4104\n",
      "Epoch: 0044 loss_train (RMSE): 0.1457 loss_val (RMSE): 0.4057\n",
      "Epoch: 0045 loss_train (RMSE): 0.1424 loss_val (RMSE): 0.4011\n",
      "Epoch: 0046 loss_train (RMSE): 0.1393 loss_val (RMSE): 0.3967\n",
      "Epoch: 0047 loss_train (RMSE): 0.1363 loss_val (RMSE): 0.3924\n",
      "Epoch: 0048 loss_train (RMSE): 0.1334 loss_val (RMSE): 0.3882\n",
      "Epoch: 0049 loss_train (RMSE): 0.1307 loss_val (RMSE): 0.3841\n",
      "Epoch: 0050 loss_train (RMSE): 0.1281 loss_val (RMSE): 0.3802\n",
      "Epoch: 0051 loss_train (RMSE): 0.1257 loss_val (RMSE): 0.3764\n",
      "Epoch: 0052 loss_train (RMSE): 0.1233 loss_val (RMSE): 0.3727\n",
      "Epoch: 0053 loss_train (RMSE): 0.1211 loss_val (RMSE): 0.3691\n",
      "Epoch: 0054 loss_train (RMSE): 0.1190 loss_val (RMSE): 0.3657\n",
      "Epoch: 0055 loss_train (RMSE): 0.1170 loss_val (RMSE): 0.3624\n",
      "Epoch: 0056 loss_train (RMSE): 0.1151 loss_val (RMSE): 0.3591\n",
      "Epoch: 0057 loss_train (RMSE): 0.1133 loss_val (RMSE): 0.3560\n",
      "Epoch: 0058 loss_train (RMSE): 0.1116 loss_val (RMSE): 0.3530\n",
      "Epoch: 0059 loss_train (RMSE): 0.1100 loss_val (RMSE): 0.3502\n",
      "Epoch: 0060 loss_train (RMSE): 0.1084 loss_val (RMSE): 0.3474\n",
      "Epoch: 0061 loss_train (RMSE): 0.1070 loss_val (RMSE): 0.3447\n",
      "Epoch: 0062 loss_train (RMSE): 0.1056 loss_val (RMSE): 0.3421\n",
      "Epoch: 0063 loss_train (RMSE): 0.1043 loss_val (RMSE): 0.3396\n",
      "Epoch: 0064 loss_train (RMSE): 0.1031 loss_val (RMSE): 0.3372\n",
      "Epoch: 0065 loss_train (RMSE): 0.1020 loss_val (RMSE): 0.3349\n",
      "Epoch: 0066 loss_train (RMSE): 0.1009 loss_val (RMSE): 0.3327\n",
      "Epoch: 0067 loss_train (RMSE): 0.0999 loss_val (RMSE): 0.3305\n",
      "Epoch: 0068 loss_train (RMSE): 0.0989 loss_val (RMSE): 0.3285\n",
      "Epoch: 0069 loss_train (RMSE): 0.0980 loss_val (RMSE): 0.3265\n",
      "Epoch: 0070 loss_train (RMSE): 0.0972 loss_val (RMSE): 0.3246\n",
      "Epoch: 0071 loss_train (RMSE): 0.0964 loss_val (RMSE): 0.3228\n",
      "Epoch: 0072 loss_train (RMSE): 0.0957 loss_val (RMSE): 0.3210\n",
      "Epoch: 0073 loss_train (RMSE): 0.0950 loss_val (RMSE): 0.3193\n",
      "Epoch: 0074 loss_train (RMSE): 0.0943 loss_val (RMSE): 0.3177\n",
      "Epoch: 0075 loss_train (RMSE): 0.0937 loss_val (RMSE): 0.3161\n",
      "Epoch: 0076 loss_train (RMSE): 0.0931 loss_val (RMSE): 0.3146\n",
      "Epoch: 0077 loss_train (RMSE): 0.0926 loss_val (RMSE): 0.3132\n",
      "Epoch: 0078 loss_train (RMSE): 0.0921 loss_val (RMSE): 0.3118\n",
      "Epoch: 0079 loss_train (RMSE): 0.0916 loss_val (RMSE): 0.3104\n",
      "Epoch: 0080 loss_train (RMSE): 0.0912 loss_val (RMSE): 0.3092\n",
      "Epoch: 0081 loss_train (RMSE): 0.0908 loss_val (RMSE): 0.3079\n",
      "Epoch: 0082 loss_train (RMSE): 0.0904 loss_val (RMSE): 0.3067\n",
      "Epoch: 0083 loss_train (RMSE): 0.0900 loss_val (RMSE): 0.3056\n",
      "Epoch: 0084 loss_train (RMSE): 0.0897 loss_val (RMSE): 0.3045\n",
      "Epoch: 0085 loss_train (RMSE): 0.0894 loss_val (RMSE): 0.3034\n",
      "Epoch: 0086 loss_train (RMSE): 0.0891 loss_val (RMSE): 0.3024\n",
      "Epoch: 0087 loss_train (RMSE): 0.0888 loss_val (RMSE): 0.3014\n",
      "Epoch: 0088 loss_train (RMSE): 0.0886 loss_val (RMSE): 0.3005\n",
      "Epoch: 0089 loss_train (RMSE): 0.0884 loss_val (RMSE): 0.2996\n",
      "Epoch: 0090 loss_train (RMSE): 0.0882 loss_val (RMSE): 0.2987\n",
      "Epoch: 0091 loss_train (RMSE): 0.0880 loss_val (RMSE): 0.2979\n",
      "Epoch: 0092 loss_train (RMSE): 0.0878 loss_val (RMSE): 0.2971\n",
      "Epoch: 0093 loss_train (RMSE): 0.0876 loss_val (RMSE): 0.2963\n",
      "Epoch: 0094 loss_train (RMSE): 0.0874 loss_val (RMSE): 0.2956\n",
      "Epoch: 0095 loss_train (RMSE): 0.0873 loss_val (RMSE): 0.2949\n",
      "Epoch: 0096 loss_train (RMSE): 0.0872 loss_val (RMSE): 0.2942\n",
      "Epoch: 0097 loss_train (RMSE): 0.0870 loss_val (RMSE): 0.2935\n",
      "Epoch: 0098 loss_train (RMSE): 0.0869 loss_val (RMSE): 0.2929\n",
      "Epoch: 0099 loss_train (RMSE): 0.0868 loss_val (RMSE): 0.2922\n",
      "Epoch: 0100 loss_train (RMSE): 0.0867 loss_val (RMSE): 0.2917\n",
      "Epoch: 0101 loss_train (RMSE): 0.0866 loss_val (RMSE): 0.2911\n",
      "Epoch: 0102 loss_train (RMSE): 0.0865 loss_val (RMSE): 0.2905\n",
      "Epoch: 0103 loss_train (RMSE): 0.0865 loss_val (RMSE): 0.2900\n",
      "Epoch: 0104 loss_train (RMSE): 0.0864 loss_val (RMSE): 0.2895\n",
      "Epoch: 0105 loss_train (RMSE): 0.0863 loss_val (RMSE): 0.2890\n",
      "Epoch: 0106 loss_train (RMSE): 0.0863 loss_val (RMSE): 0.2885\n",
      "Epoch: 0107 loss_train (RMSE): 0.0862 loss_val (RMSE): 0.2881\n",
      "Epoch: 0108 loss_train (RMSE): 0.0862 loss_val (RMSE): 0.2877\n",
      "Epoch: 0109 loss_train (RMSE): 0.0861 loss_val (RMSE): 0.2872\n",
      "Epoch: 0110 loss_train (RMSE): 0.0861 loss_val (RMSE): 0.2868\n",
      "Epoch: 0111 loss_train (RMSE): 0.0860 loss_val (RMSE): 0.2865\n",
      "Epoch: 0112 loss_train (RMSE): 0.0860 loss_val (RMSE): 0.2861\n",
      "Epoch: 0113 loss_train (RMSE): 0.0860 loss_val (RMSE): 0.2857\n",
      "Epoch: 0114 loss_train (RMSE): 0.0859 loss_val (RMSE): 0.2854\n",
      "Epoch: 0115 loss_train (RMSE): 0.0859 loss_val (RMSE): 0.2851\n",
      "Epoch: 0116 loss_train (RMSE): 0.0859 loss_val (RMSE): 0.2848\n",
      "Epoch: 0117 loss_train (RMSE): 0.0859 loss_val (RMSE): 0.2845\n",
      "Epoch: 0118 loss_train (RMSE): 0.0858 loss_val (RMSE): 0.2842\n",
      "Epoch: 0119 loss_train (RMSE): 0.0858 loss_val (RMSE): 0.2839\n",
      "Epoch: 0120 loss_train (RMSE): 0.0858 loss_val (RMSE): 0.2836\n",
      "Epoch: 0121 loss_train (RMSE): 0.0858 loss_val (RMSE): 0.2834\n",
      "Epoch: 0122 loss_train (RMSE): 0.0858 loss_val (RMSE): 0.2831\n",
      "Epoch: 0123 loss_train (RMSE): 0.0858 loss_val (RMSE): 0.2829\n",
      "Epoch: 0124 loss_train (RMSE): 0.0858 loss_val (RMSE): 0.2827\n",
      "Epoch: 0125 loss_train (RMSE): 0.0857 loss_val (RMSE): 0.2825\n",
      "Epoch: 0126 loss_train (RMSE): 0.0857 loss_val (RMSE): 0.2823\n",
      "Epoch: 0127 loss_train (RMSE): 0.0857 loss_val (RMSE): 0.2821\n",
      "Epoch: 0128 loss_train (RMSE): 0.0857 loss_val (RMSE): 0.2819\n",
      "Epoch: 0129 loss_train (RMSE): 0.0857 loss_val (RMSE): 0.2817\n",
      "Epoch: 0130 loss_train (RMSE): 0.0857 loss_val (RMSE): 0.2816\n",
      "Epoch: 0131 loss_train (RMSE): 0.0857 loss_val (RMSE): 0.2814\n",
      "Epoch: 0132 loss_train (RMSE): 0.0857 loss_val (RMSE): 0.2813\n",
      "Epoch: 0133 loss_train (RMSE): 0.0857 loss_val (RMSE): 0.2811\n",
      "Epoch: 0134 loss_train (RMSE): 0.0857 loss_val (RMSE): 0.2810\n",
      "Epoch: 0135 loss_train (RMSE): 0.0857 loss_val (RMSE): 0.2808\n",
      "Epoch: 0136 loss_train (RMSE): 0.0857 loss_val (RMSE): 0.2807\n",
      "Epoch: 0137 loss_train (RMSE): 0.0857 loss_val (RMSE): 0.2806\n",
      "Epoch: 0138 loss_train (RMSE): 0.0857 loss_val (RMSE): 0.2805\n",
      "Epoch: 0139 loss_train (RMSE): 0.0857 loss_val (RMSE): 0.2804\n",
      "Epoch: 0140 loss_train (RMSE): 0.0857 loss_val (RMSE): 0.2803\n",
      "Epoch: 0141 loss_train (RMSE): 0.0857 loss_val (RMSE): 0.2802\n",
      "Epoch: 0142 loss_train (RMSE): 0.0857 loss_val (RMSE): 0.2801\n",
      "Epoch: 0143 loss_train (RMSE): 0.0857 loss_val (RMSE): 0.2800\n",
      "Epoch: 0144 loss_train (RMSE): 0.0857 loss_val (RMSE): 0.2799\n",
      "Epoch: 0145 loss_train (RMSE): 0.0857 loss_val (RMSE): 0.2799\n",
      "Epoch: 0146 loss_train (RMSE): 0.0857 loss_val (RMSE): 0.2798\n",
      "Epoch: 0147 loss_train (RMSE): 0.0857 loss_val (RMSE): 0.2797\n",
      "Epoch: 0148 loss_train (RMSE): 0.0857 loss_val (RMSE): 0.2797\n",
      "Epoch: 0149 loss_train (RMSE): 0.0857 loss_val (RMSE): 0.2796\n",
      "Epoch: 0150 loss_train (RMSE): 0.0857 loss_val (RMSE): 0.2796\n",
      "Epoch: 0151 loss_train (RMSE): 0.0857 loss_val (RMSE): 0.2795\n",
      "Epoch: 0152 loss_train (RMSE): 0.0857 loss_val (RMSE): 0.2795\n",
      "Epoch: 0153 loss_train (RMSE): 0.0857 loss_val (RMSE): 0.2794\n",
      "Epoch: 0154 loss_train (RMSE): 0.0857 loss_val (RMSE): 0.2794\n",
      "Epoch: 0155 loss_train (RMSE): 0.0857 loss_val (RMSE): 0.2794\n",
      "Epoch: 0156 loss_train (RMSE): 0.0857 loss_val (RMSE): 0.2793\n",
      "Epoch: 0157 loss_train (RMSE): 0.0857 loss_val (RMSE): 0.2793\n",
      "Epoch: 0158 loss_train (RMSE): 0.0857 loss_val (RMSE): 0.2793\n",
      "Epoch: 0159 loss_train (RMSE): 0.0857 loss_val (RMSE): 0.2792\n",
      "Epoch: 0160 loss_train (RMSE): 0.0857 loss_val (RMSE): 0.2792\n",
      "Epoch: 0161 loss_train (RMSE): 0.0857 loss_val (RMSE): 0.2792\n",
      "Epoch: 0162 loss_train (RMSE): 0.0857 loss_val (RMSE): 0.2792\n",
      "Epoch: 0163 loss_train (RMSE): 0.0857 loss_val (RMSE): 0.2791\n",
      "Epoch: 0164 loss_train (RMSE): 0.0857 loss_val (RMSE): 0.2791\n",
      "Epoch: 0165 loss_train (RMSE): 0.0857 loss_val (RMSE): 0.2791\n",
      "Epoch: 0166 loss_train (RMSE): 0.0857 loss_val (RMSE): 0.2791\n",
      "Epoch: 0167 loss_train (RMSE): 0.0857 loss_val (RMSE): 0.2791\n",
      "Epoch: 0168 loss_train (RMSE): 0.0857 loss_val (RMSE): 0.2791\n",
      "Epoch: 0169 loss_train (RMSE): 0.0857 loss_val (RMSE): 0.2791\n",
      "Epoch: 0170 loss_train (RMSE): 0.0857 loss_val (RMSE): 0.2791\n",
      "Epoch: 0171 loss_train (RMSE): 0.0857 loss_val (RMSE): 0.2791\n",
      "Epoch: 0172 loss_train (RMSE): 0.0857 loss_val (RMSE): 0.2790\n",
      "Epoch: 0173 loss_train (RMSE): 0.0857 loss_val (RMSE): 0.2790\n",
      "Epoch: 0174 loss_train (RMSE): 0.0857 loss_val (RMSE): 0.2790\n",
      "Epoch: 0175 loss_train (RMSE): 0.0857 loss_val (RMSE): 0.2790\n",
      "Epoch: 0176 loss_train (RMSE): 0.0857 loss_val (RMSE): 0.2790\n",
      "Epoch: 0177 loss_train (RMSE): 0.0857 loss_val (RMSE): 0.2790\n",
      "Epoch: 0178 loss_train (RMSE): 0.0857 loss_val (RMSE): 0.2790\n",
      "Epoch: 0179 loss_train (RMSE): 0.0857 loss_val (RMSE): 0.2790\n",
      "Epoch: 0180 loss_train (RMSE): 0.0857 loss_val (RMSE): 0.2790\n",
      "Epoch: 0181 loss_train (RMSE): 0.0857 loss_val (RMSE): 0.2790\n",
      "Epoch: 0182 loss_train (RMSE): 0.0857 loss_val (RMSE): 0.2790\n",
      "Epoch: 0183 loss_train (RMSE): 0.0857 loss_val (RMSE): 0.2790\n",
      "Epoch: 0184 loss_train (RMSE): 0.0857 loss_val (RMSE): 0.2790\n",
      "Epoch: 0185 loss_train (RMSE): 0.0857 loss_val (RMSE): 0.2790\n",
      "Epoch: 0186 loss_train (RMSE): 0.0857 loss_val (RMSE): 0.2790\n",
      "Epoch: 0187 loss_train (RMSE): 0.0857 loss_val (RMSE): 0.2790\n",
      "Epoch: 0188 loss_train (RMSE): 0.0857 loss_val (RMSE): 0.2790\n",
      "Epoch: 0189 loss_train (RMSE): 0.0857 loss_val (RMSE): 0.2790\n",
      "Epoch: 0190 loss_train (RMSE): 0.0857 loss_val (RMSE): 0.2790\n",
      "Epoch: 0191 loss_train (RMSE): 0.0857 loss_val (RMSE): 0.2790\n",
      "Epoch: 0192 loss_train (RMSE): 0.0857 loss_val (RMSE): 0.2790\n",
      "Epoch: 0193 loss_train (RMSE): 0.0857 loss_val (RMSE): 0.2791\n",
      "Epoch: 0194 loss_train (RMSE): 0.0857 loss_val (RMSE): 0.2791\n",
      "Epoch: 0195 loss_train (RMSE): 0.0857 loss_val (RMSE): 0.2791\n",
      "Epoch: 0196 loss_train (RMSE): 0.0857 loss_val (RMSE): 0.2791\n",
      "Epoch: 0197 loss_train (RMSE): 0.0857 loss_val (RMSE): 0.2791\n",
      "Epoch: 0198 loss_train (RMSE): 0.0857 loss_val (RMSE): 0.2791\n",
      "Epoch: 0199 loss_train (RMSE): 0.0857 loss_val (RMSE): 0.2791\n",
      "Epoch: 0200 loss_train (RMSE): 0.0857 loss_val (RMSE): 0.2791\n",
      "Optimization Finished!\n",
      "Total time elapsed: 0.4034s\n",
      "Test set results: loss= 0.2818 (RMSE)\n",
      "GCN(\n",
      "  (gc1): GraphConvolution (10 -> 2000)\n",
      "  (gc5): GraphConvolution (2000 -> 2)\n",
      ")\n",
      "Epoch: 0001 loss_train (RMSE): 0.4014 loss_val (RMSE): 0.6148\n",
      "Epoch: 0002 loss_train (RMSE): 0.3921 loss_val (RMSE): 0.6079\n",
      "Epoch: 0003 loss_train (RMSE): 0.3829 loss_val (RMSE): 0.6010\n",
      "Epoch: 0004 loss_train (RMSE): 0.3739 loss_val (RMSE): 0.5943\n",
      "Epoch: 0005 loss_train (RMSE): 0.3652 loss_val (RMSE): 0.5876\n",
      "Epoch: 0006 loss_train (RMSE): 0.3567 loss_val (RMSE): 0.5811\n",
      "Epoch: 0007 loss_train (RMSE): 0.3483 loss_val (RMSE): 0.5746\n",
      "Epoch: 0008 loss_train (RMSE): 0.3402 loss_val (RMSE): 0.5683\n",
      "Epoch: 0009 loss_train (RMSE): 0.3324 loss_val (RMSE): 0.5621\n",
      "Epoch: 0010 loss_train (RMSE): 0.3247 loss_val (RMSE): 0.5560\n",
      "Epoch: 0011 loss_train (RMSE): 0.3172 loss_val (RMSE): 0.5499\n",
      "Epoch: 0012 loss_train (RMSE): 0.3100 loss_val (RMSE): 0.5440\n",
      "Epoch: 0013 loss_train (RMSE): 0.3029 loss_val (RMSE): 0.5382\n",
      "Epoch: 0014 loss_train (RMSE): 0.2961 loss_val (RMSE): 0.5325\n",
      "Epoch: 0015 loss_train (RMSE): 0.2894 loss_val (RMSE): 0.5269\n",
      "Epoch: 0016 loss_train (RMSE): 0.2830 loss_val (RMSE): 0.5214\n",
      "Epoch: 0017 loss_train (RMSE): 0.2767 loss_val (RMSE): 0.5160\n",
      "Epoch: 0018 loss_train (RMSE): 0.2706 loss_val (RMSE): 0.5106\n",
      "Epoch: 0019 loss_train (RMSE): 0.2646 loss_val (RMSE): 0.5053\n",
      "Epoch: 0020 loss_train (RMSE): 0.2588 loss_val (RMSE): 0.5000\n",
      "Epoch: 0021 loss_train (RMSE): 0.2532 loss_val (RMSE): 0.4948\n",
      "Epoch: 0022 loss_train (RMSE): 0.2477 loss_val (RMSE): 0.4896\n",
      "Epoch: 0023 loss_train (RMSE): 0.2423 loss_val (RMSE): 0.4845\n",
      "Epoch: 0024 loss_train (RMSE): 0.2370 loss_val (RMSE): 0.4794\n",
      "Epoch: 0025 loss_train (RMSE): 0.2318 loss_val (RMSE): 0.4743\n",
      "Epoch: 0026 loss_train (RMSE): 0.2268 loss_val (RMSE): 0.4692\n",
      "Epoch: 0027 loss_train (RMSE): 0.2219 loss_val (RMSE): 0.4641\n",
      "Epoch: 0028 loss_train (RMSE): 0.2171 loss_val (RMSE): 0.4591\n",
      "Epoch: 0029 loss_train (RMSE): 0.2123 loss_val (RMSE): 0.4541\n",
      "Epoch: 0030 loss_train (RMSE): 0.2077 loss_val (RMSE): 0.4492\n",
      "Epoch: 0031 loss_train (RMSE): 0.2032 loss_val (RMSE): 0.4442\n",
      "Epoch: 0032 loss_train (RMSE): 0.1988 loss_val (RMSE): 0.4393\n",
      "Epoch: 0033 loss_train (RMSE): 0.1945 loss_val (RMSE): 0.4345\n",
      "Epoch: 0034 loss_train (RMSE): 0.1903 loss_val (RMSE): 0.4297\n",
      "Epoch: 0035 loss_train (RMSE): 0.1862 loss_val (RMSE): 0.4249\n",
      "Epoch: 0036 loss_train (RMSE): 0.1822 loss_val (RMSE): 0.4202\n",
      "Epoch: 0037 loss_train (RMSE): 0.1783 loss_val (RMSE): 0.4156\n",
      "Epoch: 0038 loss_train (RMSE): 0.1745 loss_val (RMSE): 0.4111\n",
      "Epoch: 0039 loss_train (RMSE): 0.1708 loss_val (RMSE): 0.4066\n",
      "Epoch: 0040 loss_train (RMSE): 0.1673 loss_val (RMSE): 0.4022\n",
      "Epoch: 0041 loss_train (RMSE): 0.1638 loss_val (RMSE): 0.3979\n",
      "Epoch: 0042 loss_train (RMSE): 0.1605 loss_val (RMSE): 0.3937\n",
      "Epoch: 0043 loss_train (RMSE): 0.1572 loss_val (RMSE): 0.3896\n",
      "Epoch: 0044 loss_train (RMSE): 0.1541 loss_val (RMSE): 0.3856\n",
      "Epoch: 0045 loss_train (RMSE): 0.1510 loss_val (RMSE): 0.3816\n",
      "Epoch: 0046 loss_train (RMSE): 0.1481 loss_val (RMSE): 0.3778\n",
      "Epoch: 0047 loss_train (RMSE): 0.1453 loss_val (RMSE): 0.3741\n",
      "Epoch: 0048 loss_train (RMSE): 0.1425 loss_val (RMSE): 0.3704\n",
      "Epoch: 0049 loss_train (RMSE): 0.1398 loss_val (RMSE): 0.3669\n",
      "Epoch: 0050 loss_train (RMSE): 0.1372 loss_val (RMSE): 0.3635\n",
      "Epoch: 0051 loss_train (RMSE): 0.1347 loss_val (RMSE): 0.3601\n",
      "Epoch: 0052 loss_train (RMSE): 0.1323 loss_val (RMSE): 0.3569\n",
      "Epoch: 0053 loss_train (RMSE): 0.1300 loss_val (RMSE): 0.3537\n",
      "Epoch: 0054 loss_train (RMSE): 0.1277 loss_val (RMSE): 0.3507\n",
      "Epoch: 0055 loss_train (RMSE): 0.1256 loss_val (RMSE): 0.3477\n",
      "Epoch: 0056 loss_train (RMSE): 0.1235 loss_val (RMSE): 0.3449\n",
      "Epoch: 0057 loss_train (RMSE): 0.1214 loss_val (RMSE): 0.3421\n",
      "Epoch: 0058 loss_train (RMSE): 0.1195 loss_val (RMSE): 0.3394\n",
      "Epoch: 0059 loss_train (RMSE): 0.1176 loss_val (RMSE): 0.3368\n",
      "Epoch: 0060 loss_train (RMSE): 0.1158 loss_val (RMSE): 0.3343\n",
      "Epoch: 0061 loss_train (RMSE): 0.1140 loss_val (RMSE): 0.3319\n",
      "Epoch: 0062 loss_train (RMSE): 0.1123 loss_val (RMSE): 0.3296\n",
      "Epoch: 0063 loss_train (RMSE): 0.1107 loss_val (RMSE): 0.3273\n",
      "Epoch: 0064 loss_train (RMSE): 0.1091 loss_val (RMSE): 0.3251\n",
      "Epoch: 0065 loss_train (RMSE): 0.1076 loss_val (RMSE): 0.3230\n",
      "Epoch: 0066 loss_train (RMSE): 0.1062 loss_val (RMSE): 0.3210\n",
      "Epoch: 0067 loss_train (RMSE): 0.1048 loss_val (RMSE): 0.3190\n",
      "Epoch: 0068 loss_train (RMSE): 0.1035 loss_val (RMSE): 0.3172\n",
      "Epoch: 0069 loss_train (RMSE): 0.1022 loss_val (RMSE): 0.3153\n",
      "Epoch: 0070 loss_train (RMSE): 0.1010 loss_val (RMSE): 0.3136\n",
      "Epoch: 0071 loss_train (RMSE): 0.0998 loss_val (RMSE): 0.3119\n",
      "Epoch: 0072 loss_train (RMSE): 0.0987 loss_val (RMSE): 0.3102\n",
      "Epoch: 0073 loss_train (RMSE): 0.0976 loss_val (RMSE): 0.3086\n",
      "Epoch: 0074 loss_train (RMSE): 0.0966 loss_val (RMSE): 0.3071\n",
      "Epoch: 0075 loss_train (RMSE): 0.0956 loss_val (RMSE): 0.3056\n",
      "Epoch: 0076 loss_train (RMSE): 0.0947 loss_val (RMSE): 0.3042\n",
      "Epoch: 0077 loss_train (RMSE): 0.0938 loss_val (RMSE): 0.3029\n",
      "Epoch: 0078 loss_train (RMSE): 0.0929 loss_val (RMSE): 0.3015\n",
      "Epoch: 0079 loss_train (RMSE): 0.0921 loss_val (RMSE): 0.3003\n",
      "Epoch: 0080 loss_train (RMSE): 0.0913 loss_val (RMSE): 0.2990\n",
      "Epoch: 0081 loss_train (RMSE): 0.0905 loss_val (RMSE): 0.2979\n",
      "Epoch: 0082 loss_train (RMSE): 0.0898 loss_val (RMSE): 0.2967\n",
      "Epoch: 0083 loss_train (RMSE): 0.0891 loss_val (RMSE): 0.2957\n",
      "Epoch: 0084 loss_train (RMSE): 0.0885 loss_val (RMSE): 0.2946\n",
      "Epoch: 0085 loss_train (RMSE): 0.0879 loss_val (RMSE): 0.2936\n",
      "Epoch: 0086 loss_train (RMSE): 0.0873 loss_val (RMSE): 0.2927\n",
      "Epoch: 0087 loss_train (RMSE): 0.0867 loss_val (RMSE): 0.2918\n",
      "Epoch: 0088 loss_train (RMSE): 0.0862 loss_val (RMSE): 0.2909\n",
      "Epoch: 0089 loss_train (RMSE): 0.0857 loss_val (RMSE): 0.2901\n",
      "Epoch: 0090 loss_train (RMSE): 0.0852 loss_val (RMSE): 0.2893\n",
      "Epoch: 0091 loss_train (RMSE): 0.0847 loss_val (RMSE): 0.2886\n",
      "Epoch: 0092 loss_train (RMSE): 0.0843 loss_val (RMSE): 0.2879\n",
      "Epoch: 0093 loss_train (RMSE): 0.0839 loss_val (RMSE): 0.2872\n",
      "Epoch: 0094 loss_train (RMSE): 0.0835 loss_val (RMSE): 0.2866\n",
      "Epoch: 0095 loss_train (RMSE): 0.0831 loss_val (RMSE): 0.2860\n",
      "Epoch: 0096 loss_train (RMSE): 0.0828 loss_val (RMSE): 0.2855\n",
      "Epoch: 0097 loss_train (RMSE): 0.0825 loss_val (RMSE): 0.2850\n",
      "Epoch: 0098 loss_train (RMSE): 0.0821 loss_val (RMSE): 0.2845\n",
      "Epoch: 0099 loss_train (RMSE): 0.0818 loss_val (RMSE): 0.2840\n",
      "Epoch: 0100 loss_train (RMSE): 0.0816 loss_val (RMSE): 0.2836\n",
      "Epoch: 0101 loss_train (RMSE): 0.0813 loss_val (RMSE): 0.2832\n",
      "Epoch: 0102 loss_train (RMSE): 0.0810 loss_val (RMSE): 0.2828\n",
      "Epoch: 0103 loss_train (RMSE): 0.0808 loss_val (RMSE): 0.2824\n",
      "Epoch: 0104 loss_train (RMSE): 0.0806 loss_val (RMSE): 0.2821\n",
      "Epoch: 0105 loss_train (RMSE): 0.0804 loss_val (RMSE): 0.2818\n",
      "Epoch: 0106 loss_train (RMSE): 0.0802 loss_val (RMSE): 0.2815\n",
      "Epoch: 0107 loss_train (RMSE): 0.0800 loss_val (RMSE): 0.2812\n",
      "Epoch: 0108 loss_train (RMSE): 0.0798 loss_val (RMSE): 0.2809\n",
      "Epoch: 0109 loss_train (RMSE): 0.0796 loss_val (RMSE): 0.2807\n",
      "Epoch: 0110 loss_train (RMSE): 0.0795 loss_val (RMSE): 0.2805\n",
      "Epoch: 0111 loss_train (RMSE): 0.0793 loss_val (RMSE): 0.2802\n",
      "Epoch: 0112 loss_train (RMSE): 0.0792 loss_val (RMSE): 0.2800\n",
      "Epoch: 0113 loss_train (RMSE): 0.0791 loss_val (RMSE): 0.2799\n",
      "Epoch: 0114 loss_train (RMSE): 0.0790 loss_val (RMSE): 0.2797\n",
      "Epoch: 0115 loss_train (RMSE): 0.0788 loss_val (RMSE): 0.2795\n",
      "Epoch: 0116 loss_train (RMSE): 0.0787 loss_val (RMSE): 0.2794\n",
      "Epoch: 0117 loss_train (RMSE): 0.0786 loss_val (RMSE): 0.2792\n",
      "Epoch: 0118 loss_train (RMSE): 0.0786 loss_val (RMSE): 0.2791\n",
      "Epoch: 0119 loss_train (RMSE): 0.0785 loss_val (RMSE): 0.2790\n",
      "Epoch: 0120 loss_train (RMSE): 0.0784 loss_val (RMSE): 0.2788\n",
      "Epoch: 0121 loss_train (RMSE): 0.0783 loss_val (RMSE): 0.2787\n",
      "Epoch: 0122 loss_train (RMSE): 0.0782 loss_val (RMSE): 0.2786\n",
      "Epoch: 0123 loss_train (RMSE): 0.0782 loss_val (RMSE): 0.2785\n",
      "Epoch: 0124 loss_train (RMSE): 0.0781 loss_val (RMSE): 0.2784\n",
      "Epoch: 0125 loss_train (RMSE): 0.0781 loss_val (RMSE): 0.2783\n",
      "Epoch: 0126 loss_train (RMSE): 0.0780 loss_val (RMSE): 0.2783\n",
      "Epoch: 0127 loss_train (RMSE): 0.0780 loss_val (RMSE): 0.2782\n",
      "Epoch: 0128 loss_train (RMSE): 0.0779 loss_val (RMSE): 0.2781\n",
      "Epoch: 0129 loss_train (RMSE): 0.0779 loss_val (RMSE): 0.2781\n",
      "Epoch: 0130 loss_train (RMSE): 0.0778 loss_val (RMSE): 0.2780\n",
      "Epoch: 0131 loss_train (RMSE): 0.0778 loss_val (RMSE): 0.2780\n",
      "Epoch: 0132 loss_train (RMSE): 0.0778 loss_val (RMSE): 0.2779\n",
      "Epoch: 0133 loss_train (RMSE): 0.0777 loss_val (RMSE): 0.2779\n",
      "Epoch: 0134 loss_train (RMSE): 0.0777 loss_val (RMSE): 0.2778\n",
      "Epoch: 0135 loss_train (RMSE): 0.0777 loss_val (RMSE): 0.2778\n",
      "Epoch: 0136 loss_train (RMSE): 0.0776 loss_val (RMSE): 0.2778\n",
      "Epoch: 0137 loss_train (RMSE): 0.0776 loss_val (RMSE): 0.2777\n",
      "Epoch: 0138 loss_train (RMSE): 0.0776 loss_val (RMSE): 0.2777\n",
      "Epoch: 0139 loss_train (RMSE): 0.0776 loss_val (RMSE): 0.2777\n",
      "Epoch: 0140 loss_train (RMSE): 0.0776 loss_val (RMSE): 0.2777\n",
      "Epoch: 0141 loss_train (RMSE): 0.0775 loss_val (RMSE): 0.2776\n",
      "Epoch: 0142 loss_train (RMSE): 0.0775 loss_val (RMSE): 0.2776\n",
      "Epoch: 0143 loss_train (RMSE): 0.0775 loss_val (RMSE): 0.2776\n",
      "Epoch: 0144 loss_train (RMSE): 0.0775 loss_val (RMSE): 0.2776\n",
      "Epoch: 0145 loss_train (RMSE): 0.0775 loss_val (RMSE): 0.2776\n",
      "Epoch: 0146 loss_train (RMSE): 0.0775 loss_val (RMSE): 0.2776\n",
      "Epoch: 0147 loss_train (RMSE): 0.0775 loss_val (RMSE): 0.2776\n",
      "Epoch: 0148 loss_train (RMSE): 0.0775 loss_val (RMSE): 0.2776\n",
      "Epoch: 0149 loss_train (RMSE): 0.0774 loss_val (RMSE): 0.2776\n",
      "Epoch: 0150 loss_train (RMSE): 0.0774 loss_val (RMSE): 0.2776\n",
      "Epoch: 0151 loss_train (RMSE): 0.0774 loss_val (RMSE): 0.2775\n",
      "Epoch: 0152 loss_train (RMSE): 0.0774 loss_val (RMSE): 0.2775\n",
      "Epoch: 0153 loss_train (RMSE): 0.0774 loss_val (RMSE): 0.2775\n",
      "Epoch: 0154 loss_train (RMSE): 0.0774 loss_val (RMSE): 0.2775\n",
      "Epoch: 0155 loss_train (RMSE): 0.0774 loss_val (RMSE): 0.2775\n",
      "Epoch: 0156 loss_train (RMSE): 0.0774 loss_val (RMSE): 0.2775\n",
      "Epoch: 0157 loss_train (RMSE): 0.0774 loss_val (RMSE): 0.2775\n",
      "Epoch: 0158 loss_train (RMSE): 0.0774 loss_val (RMSE): 0.2775\n",
      "Epoch: 0159 loss_train (RMSE): 0.0774 loss_val (RMSE): 0.2775\n",
      "Epoch: 0160 loss_train (RMSE): 0.0774 loss_val (RMSE): 0.2775\n",
      "Epoch: 0161 loss_train (RMSE): 0.0774 loss_val (RMSE): 0.2775\n",
      "Epoch: 0162 loss_train (RMSE): 0.0774 loss_val (RMSE): 0.2775\n",
      "Epoch: 0163 loss_train (RMSE): 0.0774 loss_val (RMSE): 0.2775\n",
      "Epoch: 0164 loss_train (RMSE): 0.0774 loss_val (RMSE): 0.2775\n",
      "Epoch: 0165 loss_train (RMSE): 0.0774 loss_val (RMSE): 0.2775\n",
      "Epoch: 0166 loss_train (RMSE): 0.0774 loss_val (RMSE): 0.2775\n",
      "Epoch: 0167 loss_train (RMSE): 0.0774 loss_val (RMSE): 0.2775\n",
      "Epoch: 0168 loss_train (RMSE): 0.0774 loss_val (RMSE): 0.2775\n",
      "Epoch: 0169 loss_train (RMSE): 0.0774 loss_val (RMSE): 0.2775\n",
      "Epoch: 0170 loss_train (RMSE): 0.0774 loss_val (RMSE): 0.2775\n",
      "Epoch: 0171 loss_train (RMSE): 0.0774 loss_val (RMSE): 0.2775\n",
      "Epoch: 0172 loss_train (RMSE): 0.0774 loss_val (RMSE): 0.2775\n",
      "Epoch: 0173 loss_train (RMSE): 0.0774 loss_val (RMSE): 0.2775\n",
      "Epoch: 0174 loss_train (RMSE): 0.0774 loss_val (RMSE): 0.2775\n",
      "Epoch: 0175 loss_train (RMSE): 0.0774 loss_val (RMSE): 0.2775\n",
      "Epoch: 0176 loss_train (RMSE): 0.0774 loss_val (RMSE): 0.2775\n",
      "Epoch: 0177 loss_train (RMSE): 0.0774 loss_val (RMSE): 0.2775\n",
      "Epoch: 0178 loss_train (RMSE): 0.0774 loss_val (RMSE): 0.2775\n",
      "Epoch: 0179 loss_train (RMSE): 0.0774 loss_val (RMSE): 0.2775\n",
      "Epoch: 0180 loss_train (RMSE): 0.0774 loss_val (RMSE): 0.2775\n",
      "Epoch: 0181 loss_train (RMSE): 0.0774 loss_val (RMSE): 0.2775\n",
      "Epoch: 0182 loss_train (RMSE): 0.0774 loss_val (RMSE): 0.2775\n",
      "Epoch: 0183 loss_train (RMSE): 0.0774 loss_val (RMSE): 0.2775\n",
      "Epoch: 0184 loss_train (RMSE): 0.0774 loss_val (RMSE): 0.2775\n",
      "Epoch: 0185 loss_train (RMSE): 0.0774 loss_val (RMSE): 0.2775\n",
      "Epoch: 0186 loss_train (RMSE): 0.0774 loss_val (RMSE): 0.2775\n",
      "Epoch: 0187 loss_train (RMSE): 0.0774 loss_val (RMSE): 0.2775\n",
      "Epoch: 0188 loss_train (RMSE): 0.0774 loss_val (RMSE): 0.2775\n",
      "Epoch: 0189 loss_train (RMSE): 0.0774 loss_val (RMSE): 0.2775\n",
      "Epoch: 0190 loss_train (RMSE): 0.0774 loss_val (RMSE): 0.2775\n",
      "Epoch: 0191 loss_train (RMSE): 0.0774 loss_val (RMSE): 0.2775\n",
      "Epoch: 0192 loss_train (RMSE): 0.0774 loss_val (RMSE): 0.2775\n",
      "Epoch: 0193 loss_train (RMSE): 0.0774 loss_val (RMSE): 0.2775\n",
      "Epoch: 0194 loss_train (RMSE): 0.0774 loss_val (RMSE): 0.2775\n",
      "Epoch: 0195 loss_train (RMSE): 0.0774 loss_val (RMSE): 0.2775\n",
      "Epoch: 0196 loss_train (RMSE): 0.0774 loss_val (RMSE): 0.2775\n",
      "Epoch: 0197 loss_train (RMSE): 0.0774 loss_val (RMSE): 0.2775\n",
      "Epoch: 0198 loss_train (RMSE): 0.0774 loss_val (RMSE): 0.2775\n",
      "Epoch: 0199 loss_train (RMSE): 0.0774 loss_val (RMSE): 0.2775\n",
      "Epoch: 0200 loss_train (RMSE): 0.0774 loss_val (RMSE): 0.2775\n",
      "Optimization Finished!\n",
      "Total time elapsed: 0.3942s\n",
      "Test set results: loss= 0.3574 (RMSE)\n",
      "=====================================\n",
      "\n",
      "Averaged Test results: loss= 0.2989 (RMSE)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "from scipy import io\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from data_processing import load_data\n",
    "from models import GCN\n",
    "import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 训练设置\n",
    "# 移除 argparse 部分\n",
    "args = {\n",
    "    'no_cuda': False,\n",
    "    'fastmode': False,\n",
    "    'seed': 42,\n",
    "    'epochs': 200,\n",
    "    'lr': 0.01,\n",
    "    'weight_decay': 0,\n",
    "    'hidden': 2000,\n",
    "    'dropout': 0.5\n",
    "}\n",
    "\n",
    "args['cuda'] = not args['no_cuda'] and torch.cuda.is_available()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "threshold = 1\n",
    "seed = 42\n",
    "anchor = 50\n",
    "repeat = 20  # 确保定义了 repeat 变量\n",
    "loss_tem = np.zeros(repeat)\n",
    "\n",
    "# 更新 load_data 函数，确保路径指向正确的文件\n",
    "def load_data(threshold, num_anchor):\n",
    "    # 更改为新的 .mat 文件路径\n",
    "    mat_file_path = \"/kaggle/working/GNN-For-localization/Networks/8anchor_1000agent_10PercentNLOS_largeLOS.mat\"\n",
    "    m = io.loadmat(mat_file_path)\n",
    "\n",
    "    Range_Mat = m[\"Range_Mat\"]  # Range = Distance + noise\n",
    "    Dist_Mat = m[\"Dist_Mat\"]\n",
    "    \n",
    "    # 假设以下变量是从 .mat 文件中提取的\n",
    "    mode_fea = 1  # 例如\n",
    "    mode_adj = 1  # 例如\n",
    "    num_anchor = num_anchor\n",
    "    adj = np.zeros((num_anchor, num_anchor))  # 示例，需替换为实际数据\n",
    "    features = np.random.rand(num_anchor, 10)  # 示例特征数据\n",
    "    labels = np.random.rand(num_anchor, 2)  # 示例标签数据！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！\n",
    "    delta = None\n",
    "    degree = None\n",
    "    fea_original = None\n",
    "    fea_true = None\n",
    "    Range = None\n",
    "    Dist = None\n",
    "    truncated_noise = None\n",
    "    idx_train = np.arange(num_anchor // 2)\n",
    "    idx_val = np.arange(num_anchor // 2, num_anchor * 3 // 4)\n",
    "    idx_test = np.arange(num_anchor * 3 // 4, num_anchor)\n",
    "\n",
    "    return mode_fea, mode_adj, num_anchor, adj, features, labels, delta, degree, fea_original, fea_true, Range_Mat, Range, Dist_Mat, Dist, truncated_noise, idx_train, idx_val, idx_test\n",
    "\n",
    "for axis in range(repeat):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if args['cuda']:\n",
    "        torch.cuda.manual_seed(seed)\n",
    "\n",
    "    # 加载数据\n",
    "    mode_fea, mode_adj, num_anchor, adj, features, labels, delta, degree, fea_original, fea_true, Range_Mat, Range, Dist_Mat, Dist, truncated_noise, idx_train, idx_val, idx_test = load_data(threshold, anchor)\n",
    "\n",
    "    # 将 NumPy 数组转换为 PyTorch 张量并移动到 GPU（如果可用）\n",
    "    features = torch.from_numpy(features).float().to(device)\n",
    "    adj = torch.from_numpy(adj).float().to(device)\n",
    "    labels = torch.from_numpy(labels).float().to(device)\n",
    "    # 确保其他 NumPy 数组（如 delta, degree 等）也转换为张量\n",
    "    if delta is not None:\n",
    "        delta = torch.from_numpy(delta).float().to(device)\n",
    "    if degree is not None:\n",
    "        degree = torch.from_numpy(degree).float().to(device)\n",
    "\n",
    "    # 将 idx_train、idx_val、idx_test 转换为张量\n",
    "    idx_train = torch.from_numpy(idx_train).long().to(device)\n",
    "    idx_val = torch.from_numpy(idx_val).long().to(device)\n",
    "    idx_test = torch.from_numpy(idx_test).long().to(device)\n",
    "\n",
    "    # 模型和优化器\n",
    "    model = GCN(nfeat=features.shape[1],\n",
    "                nhid1=args['hidden'],\n",
    "                nhid2=2000,\n",
    "                #nout=labels.shape[1],\n",
    "                nout=2,\n",
    "                dropout=args['dropout'])\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(),\n",
    "                           lr=args['lr'], weight_decay=args['weight_decay'])\n",
    "\n",
    "    print(model)\n",
    "\n",
    "    loss_fun = torch.nn.MSELoss()\n",
    "\n",
    "    if args['cuda']:\n",
    "        model.cuda()\n",
    "\n",
    "    def train(epoch):\n",
    "        t = time.time()\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        output = model(features, adj)\n",
    "        loss_train = loss_fun(output[idx_train], labels[idx_train])\n",
    "\n",
    "        loss_train.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if not args['fastmode']:\n",
    "            model.eval()\n",
    "            output = model(features, adj)\n",
    "\n",
    "        loss_val = loss_fun(output[idx_val], labels[idx_val])\n",
    "        loss_val = torch.sqrt(loss_val)\n",
    "        print('Epoch: {:04d}'.format(epoch + 1),\n",
    "              'loss_train (RMSE): {:.4f}'.format(loss_train.item()),\n",
    "              'loss_val (RMSE): {:.4f}'.format(loss_val.item()))\n",
    "        return loss_train\n",
    "\n",
    "    def test():\n",
    "        model.eval()\n",
    "        output = model(features, adj)\n",
    "        loss_test = loss_fun(output[idx_test], labels[idx_test])\n",
    "        loss_test = torch.sqrt(loss_test)\n",
    "        print(\"Test set results:\",\n",
    "              \"loss= {:.4f} (RMSE)\".format(loss_test.item()))\n",
    "        return output, loss_test\n",
    "\n",
    "    # 训练模型\n",
    "    t_total = time.time()\n",
    "    for epoch in range(args['epochs']):\n",
    "        loss_train = train(epoch)\n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "    print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
    "\n",
    "    # 测试\n",
    "    predict, loss_test = test()\n",
    "    loss_tem[axis] = loss_test.item()\n",
    "    predict = predict.data.cpu().numpy()\n",
    "\n",
    "    seed += 1\n",
    "\n",
    "loss = sum(loss_tem) / repeat\n",
    "print(\"=====================================\\n\")\n",
    "print(\"Averaged Test results:\", \"loss= {:.4f} (RMSE)\".format(loss))\n",
    "\n",
    "nowTime = datetime.datetime.now().strftime('%Y-%m-%d-%H_%M_%S')  # 获取当前时间\n",
    "file_handle = open('result.txt', mode='a')\n",
    "file_handle.write('=====================================\\n')\n",
    "file_handle.write(nowTime + '\\n')\n",
    "file_handle.write('loss_train (RMSE):' + format(loss_train.item()) + '\\n')\n",
    "file_handle.write('loss_test (RMSE):' + format(loss) + '\\n')\n",
    "file_handle.close()\n",
    "\n",
    "labels = labels.data.cpu().numpy()\n",
    "\n",
    "plt.figure(1)\n",
    "plt.scatter(predict[:, 0], predict[:, 1], color='b')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "02c708b5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-28T17:37:17.037405Z",
     "iopub.status.busy": "2024-10-28T17:37:17.036443Z",
     "iopub.status.idle": "2024-10-28T17:37:17.041529Z",
     "shell.execute_reply": "2024-10-28T17:37:17.040652Z"
    },
    "papermill": {
     "duration": 0.023142,
     "end_time": "2024-10-28T17:37:17.043957",
     "exception": false,
     "start_time": "2024-10-28T17:37:17.020815",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict shape: (50, 2)\n",
      "Labels shape: (50, 2)\n"
     ]
    }
   ],
   "source": [
    "# 检查预测结果的形状\n",
    "print(f\"Predict shape: {predict.shape}\")\n",
    "print(f\"Labels shape: {labels.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a03e8ee",
   "metadata": {
    "papermill": {
     "duration": 0.014063,
     "end_time": "2024-10-28T17:37:17.072891",
     "exception": false,
     "start_time": "2024-10-28T17:37:17.058828",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 5966441,
     "sourceId": 9746155,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 33.027965,
   "end_time": "2024-10-28T17:37:18.410369",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-10-28T17:36:45.382404",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
